<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mark Fuge">
<meta name="dcterms.date" content="2025-10-22">

<title>7&nbsp; Introduction to Inference – Machine Learning for Mechanical Engineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part2/part2.html" rel="next">
<link href="../part1/distribution_distance.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-0dd2bd5de344125cf763a379ddc3eb04.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part1/part1.html">Foundational Skills</a></li><li class="breadcrumb-item"><a href="../part1/introduction_to_inference.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning for Mechanical Engineering</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part1/part1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foundational Skills</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/reviewing_supervised_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Reviewing Supervised Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/cross_validation_linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/supervised_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/linear_decompositions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Review of Linear Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/taking_derivatives.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Taking Derivatives with Automatic Differentiation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/distribution_distance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Measuring Distribution Distances</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/introduction_to_inference.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Inference</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part2/part2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model-Specific Approaches</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part2/review_neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Review of Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part2/gen_models/intro_to_GANS.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part2/gen_models/GAN_pitfalls.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">GAN Training Pitfalls</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part2/gen_models/OT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Optimal Transport for Generative Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part2/gen_models/VAEs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Variational Autoencoders (VAEs)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part2/gen_models/normalizing_flows.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Normalizing Flows</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../problems/problems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../problems/ps1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Problem Set 1</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../notebooks/notebooks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">In-Class Notebooks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/california_housing_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Housing Price Data Visualization In-Class Exercise</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/helpful_tooling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Helpful Tooling for Working with and Debugging Machine Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/course_progression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Course Lecture Progression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/review_of_singular_value_decomposition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Review of Matrices and the Singular Value Decomposition</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/review_of_math_and_computing_foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Reviewing Mathematical and Computational Foundations for Machine Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivation-and-toy-problem-definition" id="toc-motivation-and-toy-problem-definition" class="nav-link active" data-scroll-target="#motivation-and-toy-problem-definition"><span class="header-section-number">7.1</span> Motivation and Toy Problem Definition</a>
  <ul class="collapse">
  <li><a href="#the-dataset-nonlinear-sensor-measurements" id="toc-the-dataset-nonlinear-sensor-measurements" class="nav-link" data-scroll-target="#the-dataset-nonlinear-sensor-measurements"><span class="header-section-number">7.1.1</span> The Dataset: Nonlinear Sensor Measurements</a></li>
  </ul></li>
  <li><a href="#linear-bayesian-regression" id="toc-linear-bayesian-regression" class="nav-link" data-scroll-target="#linear-bayesian-regression"><span class="header-section-number">7.2</span> Linear Bayesian Regression</a>
  <ul class="collapse">
  <li><a href="#the-model" id="toc-the-model" class="nav-link" data-scroll-target="#the-model"><span class="header-section-number">7.2.1</span> The Model</a></li>
  <li><a href="#the-posterior-predictive-distribution" id="toc-the-posterior-predictive-distribution" class="nav-link" data-scroll-target="#the-posterior-predictive-distribution"><span class="header-section-number">7.2.2</span> The Posterior Predictive Distribution</a></li>
  </ul></li>
  <li><a href="#a-small-neural-network-nonlinear-model" id="toc-a-small-neural-network-nonlinear-model" class="nav-link" data-scroll-target="#a-small-neural-network-nonlinear-model"><span class="header-section-number">7.3</span> A Small Neural Network (Nonlinear Model)</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture"><span class="header-section-number">7.3.1</span> Architecture</a></li>
  <li><a href="#the-new-challenge-intractable-posterior" id="toc-the-new-challenge-intractable-posterior" class="nav-link" data-scroll-target="#the-new-challenge-intractable-posterior"><span class="header-section-number">7.3.2</span> The New Challenge: Intractable Posterior</a></li>
  </ul></li>
  <li><a href="#mcmc-approximation" id="toc-mcmc-approximation" class="nav-link" data-scroll-target="#mcmc-approximation"><span class="header-section-number">7.4</span> MCMC Approximation</a>
  <ul class="collapse">
  <li><a href="#why-cant-we-just-compute-the-posterior-using-regular-monte-carlo-integration" id="toc-why-cant-we-just-compute-the-posterior-using-regular-monte-carlo-integration" class="nav-link" data-scroll-target="#why-cant-we-just-compute-the-posterior-using-regular-monte-carlo-integration"><span class="header-section-number">7.4.1</span> Why can’t we just compute the posterior using regular Monte Carlo Integration?</a></li>
  <li><a href="#what-is-mcmc" id="toc-what-is-mcmc" class="nav-link" data-scroll-target="#what-is-mcmc"><span class="header-section-number">7.4.2</span> What is MCMC?</a></li>
  <li><a href="#note-on-implementation" id="toc-note-on-implementation" class="nav-link" data-scroll-target="#note-on-implementation"><span class="header-section-number">7.4.3</span> Note on Implementation</a></li>
  <li><a href="#toy-example-2d-posterior-sampling-from-the-banana-distribution" id="toc-toy-example-2d-posterior-sampling-from-the-banana-distribution" class="nav-link" data-scroll-target="#toy-example-2d-posterior-sampling-from-the-banana-distribution"><span class="header-section-number">7.4.4</span> Toy Example: 2D Posterior Sampling from the Banana Distribution</a></li>
  <li><a href="#why-is-struggles-and-mcmc-succeeds" id="toc-why-is-struggles-and-mcmc-succeeds" class="nav-link" data-scroll-target="#why-is-struggles-and-mcmc-succeeds"><span class="header-section-number">7.4.5</span> Why IS struggles and MCMC succeeds</a></li>
  <li><a href="#returning-to-neural-networks-now-with-mcmc-sampling" id="toc-returning-to-neural-networks-now-with-mcmc-sampling" class="nav-link" data-scroll-target="#returning-to-neural-networks-now-with-mcmc-sampling"><span class="header-section-number">7.4.6</span> Returning to Neural Networks (now with MCMC sampling)</a></li>
  <li><a href="#advanced-sampling-hamiltonian-monte-carlo-hmc" id="toc-advanced-sampling-hamiltonian-monte-carlo-hmc" class="nav-link" data-scroll-target="#advanced-sampling-hamiltonian-monte-carlo-hmc"><span class="header-section-number">7.4.7</span> Advanced Sampling: Hamiltonian Monte Carlo (HMC)</a></li>
  </ul></li>
  <li><a href="#variational-inference" id="toc-variational-inference" class="nav-link" data-scroll-target="#variational-inference"><span class="header-section-number">7.5</span> Variational Inference</a>
  <ul class="collapse">
  <li><a href="#chosing-a-tractable-family-q_phimathbfw" id="toc-chosing-a-tractable-family-q_phimathbfw" class="nav-link" data-scroll-target="#chosing-a-tractable-family-q_phimathbfw"><span class="header-section-number">7.5.1</span> Chosing a tractable family <span class="math inline">\(q_\phi(\mathbf{w})\)</span></a></li>
  <li><a href="#marginal-likelihood-evidence" id="toc-marginal-likelihood-evidence" class="nav-link" data-scroll-target="#marginal-likelihood-evidence"><span class="header-section-number">7.5.2</span> 1. Marginal Likelihood (Evidence)</a></li>
  <li><a href="#introducing-a-variational-distribution" id="toc-introducing-a-variational-distribution" class="nav-link" data-scroll-target="#introducing-a-variational-distribution"><span class="header-section-number">7.5.3</span> 2. Introducing a Variational Distribution</a></li>
  <li><a href="#applying-jensens-inequality" id="toc-applying-jensens-inequality" class="nav-link" data-scroll-target="#applying-jensens-inequality"><span class="header-section-number">7.5.4</span> 3. Applying Jensen’s Inequality</a></li>
  <li><a href="#the-elbo-expression" id="toc-the-elbo-expression" class="nav-link" data-scroll-target="#the-elbo-expression"><span class="header-section-number">7.5.5</span> 4. The ELBO Expression</a></li>
  <li><a href="#example-of-using-vi-for-the-banana-distribution" id="toc-example-of-using-vi-for-the-banana-distribution" class="nav-link" data-scroll-target="#example-of-using-vi-for-the-banana-distribution"><span class="header-section-number">7.5.6</span> Example of using VI for the Banana Distribution</a></li>
  <li><a href="#monte-carlo-variational-inference" id="toc-monte-carlo-variational-inference" class="nav-link" data-scroll-target="#monte-carlo-variational-inference"><span class="header-section-number">7.5.7</span> Monte Carlo Variational Inference</a></li>
  <li><a href="#the-reparameterization-trick" id="toc-the-reparameterization-trick" class="nav-link" data-scroll-target="#the-reparameterization-trick"><span class="header-section-number">7.5.8</span> The Reparameterization Trick</a></li>
  <li><a href="#black-box-variational-inference" id="toc-black-box-variational-inference" class="nav-link" data-scroll-target="#black-box-variational-inference"><span class="header-section-number">7.5.9</span> Black-Box Variational Inference</a></li>
  <li><a href="#example-of-using-vi-for-neural-networks" id="toc-example-of-using-vi-for-neural-networks" class="nav-link" data-scroll-target="#example-of-using-vi-for-neural-networks"><span class="header-section-number">7.5.10</span> Example of using VI for Neural Networks</a></li>
  </ul></li>
  <li><a href="#possible-additional-experiments" id="toc-possible-additional-experiments" class="nav-link" data-scroll-target="#possible-additional-experiments"><span class="header-section-number">7.6</span> Possible Additional Experiments</a></li>
  <li><a href="#summary-and-the-bridge-to-vaes" id="toc-summary-and-the-bridge-to-vaes" class="nav-link" data-scroll-target="#summary-and-the-bridge-to-vaes"><span class="header-section-number">7.7</span> Summary and the bridge to VAEs</a>
  <ul class="collapse">
  <li><a href="#looking-forward-to-vaes-variational-autoencoders" id="toc-looking-forward-to-vaes-variational-autoencoders" class="nav-link" data-scroll-target="#looking-forward-to-vaes-variational-autoencoders"><span class="header-section-number">7.7.1</span> Looking forward to VAEs (Variational Autoencoders)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part1/part1.html">Foundational Skills</a></li><li class="breadcrumb-item"><a href="../part1/introduction_to_inference.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Inference</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Inference</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mark Fuge </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 22, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><strong>Learning Objectives</strong></p>
<p>By the end of this notebook, you will:</p>
<ol type="1">
<li>Understand <strong>Bayesian inference for regression</strong> and how it quantifies model uncertainty</li>
<li>Visualize the <strong>posterior predictive distribution</strong> for linear models</li>
<li>Recognize the <strong>limitations of linear models</strong> on nonlinear data</li>
<li>Learn how <strong>neural networks</strong> provide flexible nonlinear models</li>
<li>Understand why exact Bayesian inference becomes <strong>intractable</strong> for neural networks</li>
<li>Implement <strong>Variational Inference (VI)</strong> as an approximate solution</li>
<li>Compare VI to MCMC and understand the <strong>speed vs.&nbsp;accuracy tradeoff</strong></li>
<li>Visualize <strong>predictive uncertainty</strong> in function space</li>
</ol>
<p><strong>The Big Picture</strong></p>
<p>Imagine you have sparse sensor measurements from a mechanical system. You want to:</p>
<ol type="1">
<li><strong>Fit a model</strong> that captures the underlying relationship</li>
<li><strong>Quantify uncertainty</strong> in predictions (critical for safety-critical systems)</li>
<li><strong>Make predictions</strong> with confidence intervals</li>
</ol>
<p>This notebook shows you how to do this using <strong>Bayesian inference</strong>, starting with simple linear models and progressing to flexible neural networks where <strong>variational inference</strong> becomes essential.</p>
<section id="motivation-and-toy-problem-definition" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="motivation-and-toy-problem-definition"><span class="header-section-number">7.1</span> Motivation and Toy Problem Definition</h2>
<p>Let’s start with a fundamental question: <strong>when you fit a model to data, how confident should you be in its predictions?</strong></p>
<p>In traditional machine learning, we find a single “best” set of parameters. But in engineering, we often need to know: - How uncertain are we about the model? - Where in the input space are predictions reliable? - What happens if we had slightly different data?</p>
<p><strong>Bayesian inference</strong> answers these questions by maintaining a <strong>distribution over models</strong> rather than picking a single one.</p>
<section id="the-dataset-nonlinear-sensor-measurements" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="the-dataset-nonlinear-sensor-measurements"><span class="header-section-number">7.1.1</span> The Dataset: Nonlinear Sensor Measurements</h3>
<p>We’ll use a simple nonlinear function to represent sensor data: <span class="math display">\[y = \sin(x) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, 0.1^2)\]</span></p>
<p>This represents, for example: - Cyclic behavior (vibrations, temperatures, etc.) - Noisy measurements - Sparse observations (we won’t measure everywhere)</p>
<p>We will be interested in understanding how the model can capture the uncertainty of its prediction.</p>
<div id="2fb1a989" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup and imports</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Tuple</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ipywidgets <span class="im">as</span> widgets</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Set style</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.dpi'</span>] <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'savefig.dpi'</span>] <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seeds for reproducibility</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>&lt;torch._C.Generator at 0x280584e30f0&gt;</code></pre>
</div>
</div>
<div id="502d8bcc" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate nonlinear dataset</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_nonlinear_data(n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">20</span>, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                           noise_std: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                           x_range: Tuple[<span class="bu">float</span>, <span class="bu">float</span>] <span class="op">=</span> (<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)) <span class="op">-&gt;</span> Tuple[np.ndarray, np.ndarray]:</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Generate sparse noisy samples from a sinusoidal function.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    This simulates sensor measurements from a cyclic process.</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">        n_samples: Number of observations</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">        noise_std: Standard deviation of measurement noise</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">        x_range: Input domain (min, max)</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">        x: Input locations (n_samples,)</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co">        y: Noisy measurements (n_samples,)</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> x_range</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample input locations (sparse, irregular spacing)</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.random.uniform(x_min, x_max, n_samples)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># True underlying function (unknown to the model)</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> np.sin(x)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add measurement noise</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y_true <span class="op">+</span> noise_std <span class="op">*</span> np.random.randn(n_samples)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort for easier visualization</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    sort_idx <span class="op">=</span> np.argsort(x)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x[sort_idx]</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y[sort_idx]</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x, y</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate training data</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>n_train <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>noise_std <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>x_train, y_train <span class="op">=</span> generate_nonlinear_data(n_train, noise_std)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dense grid for visualization (the "true" function)</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">200</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>y_test_true <span class="op">=</span> np.sin(x_test)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the dataset</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot true function</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True function: $y = </span><span class="er">\</span><span class="st">sin(x)$'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot noisy training observations</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">80</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="fl">1.5</span>, zorder<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="ss">f'Training data (n=</span><span class="sc">{</span>n_train<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Shade noise region</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>ax.fill_between(x_test, y_test_true <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>noise_std, y_test_true <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>noise_std,</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'gray'</span>, label<span class="op">=</span><span class="ss">f'±2sigma noise band (sigma=</span><span class="sc">{</span>noise_std<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Nonlinear Sensor Measurements'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="linear-bayesian-regression" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="linear-bayesian-regression"><span class="header-section-number">7.2</span> Linear Bayesian Regression</h2>
<p>Let’s start with the simplest approach: <strong>linear regression with Bayesian inference</strong>.</p>
<section id="the-model" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="the-model"><span class="header-section-number">7.2.1</span> The Model</h3>
<p>We assume a linear relationship: <span class="math display">\[y = w_0 + w_1 x + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)\]</span></p>
<p>In Bayesian inference, we place a <strong>prior</strong> over the weights: <span class="math display">\[p(\mathbf{w}) = \mathcal{N}(\mathbf{0}, \alpha^{-1} \mathbf{I})\]</span></p>
<p>After seeing data <span class="math inline">\((X, Y)\)</span>, the <strong>posterior</strong> over weights is: <span class="math display">\[p(\mathbf{w} | X, Y) = \mathcal{N}(\mathbf{m}_N, \mathbf{S}_N)\]</span></p>
<p>where (for linear-Gaussian models, we can derive it exactly): <span class="math display">\[\mathbf{S}_N = (\alpha \mathbf{I} + \beta \mathbf{\Phi}^T \mathbf{\Phi})^{-1}\]</span> <span class="math display">\[\mathbf{m}_N = \beta \mathbf{S}_N \mathbf{\Phi}^T \mathbf{y}\]</span></p>
<p>where <span class="math inline">\(\beta = 1/\sigma^2\)</span> is the noise precision and <span class="math inline">\(\mathbf{\Phi}\)</span> is the design matrix.</p>
</section>
<section id="the-posterior-predictive-distribution" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="the-posterior-predictive-distribution"><span class="header-section-number">7.2.2</span> The Posterior Predictive Distribution</h3>
<p>For a new input <span class="math inline">\(x_*\)</span>, the <strong>predictive distribution</strong> integrates over all possible weights: <span class="math display">\[p(y_* | x_*, X, Y) = \int p(y_* | x_*, \mathbf{w}) p(\mathbf{w} | X, Y) d\mathbf{w}\]</span></p>
<p>For linear-Gaussian models, this is also Gaussian: <span class="math display">\[p(y_* | x_*, X, Y) = \mathcal{N}(\mathbf{m}_N^T \phi(x_*), \sigma_N^2(x_*))\]</span></p>
<p>The variance <span class="math inline">\(\sigma_N^2(x_*)\)</span> tells us <strong>how uncertain</strong> we are about predictions at <span class="math inline">\(x_*\)</span>.</p>
<p>Let’s implement this and see what happens when applied to our non-linear data.</p>
<div id="79950a28" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BayesianLinearRegression:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Bayesian linear regression with Gaussian prior and likelihood.</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Closed-form posterior over weights for polynomial basis functions.</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, degree: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>, alpha: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>, beta: <span class="bu">float</span> <span class="op">=</span> <span class="fl">100.0</span>):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">            degree: Degree of polynomial basis (1 = linear)</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">            alpha: Prior precision (inverse variance)</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">            beta: Noise precision (1/sigma^2)</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.degree <span class="op">=</span> degree</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> beta</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.m_N <span class="op">=</span> <span class="va">None</span>  <span class="co"># Posterior mean</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.S_N <span class="op">=</span> <span class="va">None</span>  <span class="co"># Posterior covariance</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _design_matrix(<span class="va">self</span>, x: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute polynomial design matrix."""</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        Phi <span class="op">=</span> np.concatenate([x<span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.degree <span class="op">+</span> <span class="dv">1</span>)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Phi</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X: np.ndarray, y: np.ndarray):</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute posterior over weights given training data."""</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        Phi <span class="op">=</span> <span class="va">self</span>._design_matrix(X)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        n_features <span class="op">=</span> Phi.shape[<span class="dv">1</span>]</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Posterior covariance</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.S_N <span class="op">=</span> np.linalg.inv(<span class="va">self</span>.alpha <span class="op">*</span> np.eye(n_features) <span class="op">+</span> </span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>                                  <span class="va">self</span>.beta <span class="op">*</span> Phi.T <span class="op">@</span> Phi)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Posterior mean</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.m_N <span class="op">=</span> <span class="va">self</span>.beta <span class="op">*</span> <span class="va">self</span>.S_N <span class="op">@</span> Phi.T <span class="op">@</span> y</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X_test: np.ndarray, return_std: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="co">        Posterior predictive distribution.</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co">            mean: Predictive mean</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="co">            std: Predictive standard deviation (if return_std=True)</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        Phi_test <span class="op">=</span> <span class="va">self</span>._design_matrix(X_test)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predictive mean</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        y_mean <span class="op">=</span> Phi_test <span class="op">@</span> <span class="va">self</span>.m_N</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_std:</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Predictive variance (includes noise and weight uncertainty)</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>            y_var <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span><span class="va">self</span>.beta <span class="op">+</span> np.<span class="bu">sum</span>(Phi_test <span class="op">@</span> <span class="va">self</span>.S_N <span class="op">*</span> Phi_test, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>            y_std <span class="op">=</span> np.sqrt(y_var)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> y_mean, y_std</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y_mean</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_weights(<span class="va">self</span>, n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sample weight vectors from posterior."""</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.multivariate_normal(<span class="va">self</span>.m_N, <span class="va">self</span>.S_N, size<span class="op">=</span>n_samples)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit linear Bayesian regression</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>blr <span class="op">=</span> BayesianLinearRegression(degree<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">1.0</span><span class="op">/</span>noise_std<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>blr.fit(x_train, y_train)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior predictive distribution</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>y_pred_mean, y_pred_std <span class="op">=</span> blr.predict(x_test)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear Bayesian Regression Fitted!"</span>)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Posterior mean weights: </span><span class="sc">{</span>blr<span class="sc">.</span>m_N<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Posterior std of weights: </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(np.diag(blr.S_N))<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear Bayesian Regression Fitted!
Posterior mean weights: [-0.07553835  0.33272507]
Posterior std of weights: [0.0225762  0.01242588]</code></pre>
</div>
</div>
<div id="24f073d0" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize linear Bayesian regression predictions</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">6</span>))</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: Posterior predictive with uncertainty bands</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># True function</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True: $y = </span><span class="er">\</span><span class="st">sin(x)$'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Training data</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">80</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="fl">1.5</span>, zorder<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictive mean</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_pred_mean, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Linear prediction (posterior mean)'</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncertainty bands (±1sigma and ±2sigma)</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>ax.fill_between(x_test, y_pred_mean <span class="op">-</span> y_pred_std, y_pred_mean <span class="op">+</span> y_pred_std,</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'±1sigma predictive'</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>ax.fill_between(x_test, y_pred_mean <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>y_pred_std, y_pred_mean <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>y_pred_std,</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'±2sigma predictive'</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Linear Model: Cannot Capture Nonlinearity'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">10</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>])</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Function samples from posterior</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample weight vectors and plot corresponding functions</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>w_samples <span class="op">=</span> blr.sample_weights(n_samples<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>Phi_test <span class="op">=</span> np.concatenate([x_test.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    y_sample <span class="op">=</span> Phi_test <span class="op">@</span> w_samples[i]</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, y_sample, <span class="st">'b-'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="co"># True function and data</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True function'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">80</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="fl">1.5</span>, zorder<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Posterior Function Samples (50 draws)'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>])</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="a-small-neural-network-nonlinear-model" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="a-small-neural-network-nonlinear-model"><span class="header-section-number">7.3</span> A Small Neural Network (Nonlinear Model)</h2>
<p>The linear model fails because the true function is nonlinear. What if we try to use a simple <strong>neural network</strong> instead?</p>
<section id="architecture" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="architecture"><span class="header-section-number">7.3.1</span> Architecture</h3>
<p>We’ll use a simple 1-hidden-layer network: <span class="math display">\[f(x; \mathbf{w}) = \mathbf{w}_2^T \sigma(\mathbf{w}_1 x + \mathbf{b}_1) + b_2\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is ReLU activation. With just 5 hidden units, this can approximate smooth nonlinear functions.</p>
</section>
<section id="the-new-challenge-intractable-posterior" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="the-new-challenge-intractable-posterior"><span class="header-section-number">7.3.2</span> The New Challenge: Intractable Posterior</h3>
<p>Unlike linear regression, the posterior <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span> is <strong>no longer Gaussian</strong>! We cannot compute it analytically.</p>
<p>However, we can still: 1. Define a <strong>prior</strong> <span class="math inline">\(p(\mathbf{w})\)</span> (e.g., Gaussian on all weights) 2. Sample functions from the prior to see what kinds of functions are admissible under the model 3. Later, we can use <strong>VI or MCMC</strong> to approximate the posterior</p>
<p>Let’s first see what the <strong>prior predictive distribution</strong> looks like.</p>
<div id="7333ddf4" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SmallNN(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simple 1-hidden-layer neural network for regression.</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">1</span>, hidden_size)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_size, <span class="dv">1</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.fc2(h)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y.squeeze()</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_weights_flat(<span class="va">self</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get all weights as a flat vector."""</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat([p.flatten() <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters()])</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_weights_flat(<span class="va">self</span>, w_flat: torch.Tensor):</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Set all weights from a flat vector."""</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        offset <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters():</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>            n_params <span class="op">=</span> p.numel()</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>            p.data <span class="op">=</span> w_flat[offset:offset<span class="op">+</span>n_params].view(p.shape)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>            offset <span class="op">+=</span> n_params</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Create network and count parameters</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>hidden_size <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SmallNN(hidden_size<span class="op">=</span>hidden_size)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>n_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Neural Network Architecture:"</span>)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Input → </span><span class="sc">{</span>hidden_size<span class="sc">}</span><span class="ss"> hidden (ReLU) → 1 output"</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Total parameters: </span><span class="sc">{</span>n_params<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Parameter breakdown:"</span>)</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: shape </span><span class="sc">{</span>param<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, count </span><span class="sc">{</span>param<span class="sc">.</span>numel()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Neural Network Architecture:
  Input → 5 hidden (ReLU) → 1 output
  Total parameters: 16
Parameter breakdown:
  fc1.weight: shape torch.Size([5, 1]), count 5
  fc1.bias: shape torch.Size([5]), count 5
  fc2.weight: shape torch.Size([1, 5]), count 5
  fc2.bias: shape torch.Size([1]), count 1</code></pre>
</div>
</div>
<div id="7cdc97d5" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample functions from the PRIOR (before seeing any data)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_prior_functions(model, x_test, n_samples<span class="op">=</span><span class="dv">20</span>, prior_std<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Sample function predictions from prior p(w) = N(0, prior_std^2 * I).</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    x_test_tensor <span class="op">=</span> torch.FloatTensor(x_test)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Sample weights from prior</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>                param.data.normal_(<span class="dv">0</span>, prior_std)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(x_test_tensor).numpy()</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            predictions.append(y_pred)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(predictions)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate prior predictive samples</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>prior_std <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>n_prior_samples <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>prior_predictions <span class="op">=</span> sample_prior_functions(model, x_test, n_prior_samples, prior_std)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize prior predictive distribution</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">7</span>))</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot prior function samples</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_prior_samples):</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, prior_predictions[i], <span class="st">'purple'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot true function and data</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True function: $y = </span><span class="er">\</span><span class="st">sin(x)$'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="dv">2</span>, zorder<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Training data (not yet used!)'</span>)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Add dummy line for legend</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>ax.plot([], [], <span class="st">'purple'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="ss">f'Prior samples (n=</span><span class="sc">{</span>n_prior_samples<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'Prior Predictive: Neural Network Before Training</span><span class="ch">\n</span><span class="ss">(Prior: $w </span><span class="er">\</span><span class="ss">sim </span><span class="er">\</span><span class="ss">mathcal</span><span class="ch">{{</span><span class="ss">N</span><span class="ch">}}</span><span class="ss">(0, </span><span class="sc">{</span>prior_std<span class="sc">}</span><span class="ss">^2 I)$)'</span>, </span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>])</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see that just sampling functions from the prior, the possible Neural Network functions are a lot more diverse that those we could sample from the linear model. But how do we find out which of these weights are (probabilistically) likely?</p>
</section>
</section>
<section id="mcmc-approximation" class="level2 page-columns page-full" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="mcmc-approximation"><span class="header-section-number">7.4</span> MCMC Approximation</h2>
<p>The neural network posterior <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span> is intractable, but we can <strong>sample</strong> from it using <strong>Markov Chain Monte Carlo (MCMC)</strong>. Before we dive into how to apply this to neural networks, let’s first gain some intuition on how and why MCMC works on a simple 2D example.</p>
<section id="why-cant-we-just-compute-the-posterior-using-regular-monte-carlo-integration" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="why-cant-we-just-compute-the-posterior-using-regular-monte-carlo-integration"><span class="header-section-number">7.4.1</span> Why can’t we just compute the posterior using regular Monte Carlo Integration?</h3>
<p>A classical Monte Carlo approach would involve: 1. Drawing many samples <span class="math inline">\(\mathbf{w}^{(i)}\)</span> from the prior <span class="math inline">\(p(\mathbf{w})\)</span>, or (as is more common) from a proposal distribution <span class="math inline">\(q(\mathbf{w})\)</span> that we hope covers the posterior well. 2. Weighting each sample by its likelihood <span class="math inline">\(p(Y | X, \mathbf{w}^{(i)})\)</span> 3. Normalizing empirically to get an approximate posterior.</p>
<p>This is called <strong>importance sampling</strong> and while it is conceptually simple, it doesn’t work for many complex models or distributions because the proposal distribution needs to be well matched to the true posterior. If the posterior is concentrated in a small region of parameter space, most samples will have negligible weight, and this will make our “Effective Sample Size” (ESS) very small, i.e., we will need to do lots of sampling, but ultimately throw out most of those samples, so this is inefficient. For low-dimensional, fairly simple distributions, importance sampling can work well, but for high-dimensional models like neural networks or very complex posterior distributions, it typically fails (as we will see below).</p>
<p>To address this, we can use Markov Chain Monte Carlo (MCMC) methods, which adaptively explore the posterior distribution. The key idea is to construct a Markov chain whose stationary distribution is the target posterior distribution. By running the chain for a long time, we can obtain samples that are approximately distributed according to the posterior. It’s main disadvantage is that because many of the samples are correlated, we need many more samples to get a good approximation of the posterior compared to independent sampling methods, but this downside is offset by the fact that MCMC can explore more complex distributions more effectively even using a simple proposal distribution.</p>
</section>
<section id="what-is-mcmc" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="what-is-mcmc"><span class="header-section-number">7.4.2</span> What is MCMC?</h3>
<p>MCMC methods (like Metropolis-Hastings, Hamiltonian Monte Carlo, and others) generate samples <span class="math inline">\(\mathbf{w}^{(1)}, \mathbf{w}^{(2)}, \ldots\)</span> from the posterior by: 1. Starting at a random initial weight vector 2. Proposing updated samples that respect the posterior density 3. Eventually converging to samples from <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span></p>
<p>So, in a nutshell, it allows you to generate plausible samples from <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span> without ever having to strictly compute the posterior.</p>
<p><strong>Pros</strong>: Asymptotically exact (given infinite samples)<br>
<strong>Cons</strong>: Slow, especially for high-dimensional models</p>
<p>For our small network, MCMC is feasible. We will use it as a reference to compare against variational inference at the end of this notebook.</p>
</section>
<section id="note-on-implementation" class="level3 page-columns page-full" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="note-on-implementation"><span class="header-section-number">7.4.3</span> Note on Implementation</h3>
<p>For this notebook, we’ll use a simple <strong>Metropolis-Hastings</strong> sampler, although many more advanced algorithms exist.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;There are some tricks to reduce the variance by subtracting a moving “baseline” average to the advantage term, which is implemented below, but we will not go into the details of this here.</p></div></div><section id="the-metropolis-hastings-algorithm" class="level4" data-number="7.4.3.1">
<h4 data-number="7.4.3.1" class="anchored" data-anchor-id="the-metropolis-hastings-algorithm"><span class="header-section-number">7.4.3.1</span> The Metropolis-Hastings Algorithm</h4>
<p>The algorithm works as follows:</p>
<ol type="1">
<li><strong>Initialize</strong>: Start with random weights <span class="math inline">\(\mathbf{w}^{(0)}\)</span></li>
<li><strong>Propose</strong>: Generate a candidate <span class="math inline">\(\mathbf{w}^* \sim q(\mathbf{w}^* | \mathbf{w}^{(t)})\)</span> (typically a Gaussian centered at current weights)</li>
<li><strong>Accept/Reject</strong>: Compute the acceptance probability: <span class="math display">\[\alpha = \min\left(1, \frac{p(\mathbf{w}^* | X, Y)}{p(\mathbf{w}^{(t)} | X, Y)}\right) = \min\left(1, \frac{p(Y | X, \mathbf{w}^*) p(\mathbf{w}^*)}{p(Y | X, \mathbf{w}^{(t)}) p(\mathbf{w}^{(t)})}\right)\]</span> Accept <span class="math inline">\(\mathbf{w}^{(t+1)} = \mathbf{w}^*\)</span> with probability <span class="math inline">\(\alpha\)</span>, otherwise keep <span class="math inline">\(\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)}\)</span></li>
<li><strong>Repeat</strong>: Continue for many iterations until the chain converges</li>
</ol>
<p>The useful thing about this sampler (and indeed MCMC approaches in general), is that we only ever need to evaluate the <strong>unnormalized</strong> posterior (likelihood x prior), and thus can avoid computing the intractable normalizing constant.</p>
<p><strong>Key Caveats (with Metropolis-Hastings or MCMC methods in general)</strong>: - You cannot just take individual samples and treat them as independent draws from the posterior, since the starting location of one sample was dependent on the location of the last sample. This means that the samples are correlated. In practice, this means you will need to discard the first certain number of samples (this is called “Burn In”) and also consider only keeping every K number of samples (this is called “Thinning”) so that the MCMC samples become less correlated with one another and more like true independent samples from the posterior. - While the proposal distribution can be simple (e.g., Gaussian), its parameters (like standard deviation) need to be carefully tuned since if it is too large we will have low sample acceptance rates, but it is is too small we will explore the space very slowly, requiring many more samples to converge.</p>
<p>For the 2D example we will do next, MCMC works very well and will give us a good approximation of the posterior. For our 16-parameter network that we will return to later in the notebook, MCMC sampling is still tractable. For modern deep networks with millions of parameters, however, MCMC becomes prohibitively slow and this is where Variational Inference can come to the rescue, as we will see later in the second part of the notebook.</p>
</section>
</section>
<section id="toy-example-2d-posterior-sampling-from-the-banana-distribution" class="level3" data-number="7.4.4">
<h3 data-number="7.4.4" class="anchored" data-anchor-id="toy-example-2d-posterior-sampling-from-the-banana-distribution"><span class="header-section-number">7.4.4</span> Toy Example: 2D Posterior Sampling from the Banana Distribution</h3>
<p>We will start with a simple 2D example to illustrate how MCMC works. This will come from the famous “Banana Distribution”, which is a common test case for MCMC methods due to its curved, non-Gaussian shape, as we can see below.</p>
<div id="6fcda86c" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reuse global RNG seed for reproducibility (already set above)</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">42</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Unnormalized log-density of banana target:</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># log p~(x,y) = -0.5 * ( x^2/9 + (y - 0.1 x^2)^2 )</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_p_tilde(x: np.ndarray, y: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.asarray(x)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.asarray(y)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ( (x<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> <span class="fl">9.0</span> <span class="op">+</span> (y <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span> )</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple isotropic Gaussian proposal q(x,y) = N(0, I)</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># log q(x,y) = -0.5 * (x^2 + y^2) + const; constants cancel in ratios, so we can drop them</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_q(x: np.ndarray, y: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.asarray(x)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.asarray(y)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Defined banana target log-density and Gaussian proposal."</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Now plot the target distribution for visualization</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">300</span>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>y_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">15</span>, <span class="dv">300</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x_vals, y_vals)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.exp(log_p_tilde(X, Y))</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> ax.contourf(X, Y, Z, levels<span class="op">=</span><span class="dv">50</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>plt.colorbar(contour, ax<span class="op">=</span>ax, label<span class="op">=</span><span class="st">'Density'</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Banana-shaped Target Distribution'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Defined banana target log-density and Gaussian proposal.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can attempt to do Monte Carlo integration on this distribution, but because of its curved shape, simple importance sampling is unlikely to cover this distribution well, as we can see below:</p>
<div id="b954c2db" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importance Sampling on the banana target</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>N_is <span class="op">=</span> <span class="dv">10_000</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw from q ~ N(0, I)</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>x_is <span class="op">=</span> rng.normal(size<span class="op">=</span>N_is)<span class="op">*</span><span class="fl">.3</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>y_is <span class="op">=</span> rng.normal(size<span class="op">=</span>N_is)<span class="op">*</span><span class="fl">.3</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Unnormalized log weights: log w_i = log p~(x_i,y_i) - log q(x_i,y_i)</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>log_w <span class="op">=</span> log_p_tilde(x_is, y_is) <span class="op">-</span> log_q(x_is, y_is)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Stabilize weights by subtracting max log_w</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>log_w_shift <span class="op">=</span> log_w <span class="op">-</span> np.<span class="bu">max</span>(log_w)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.exp(log_w_shift)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>w_norm <span class="op">=</span> w <span class="op">/</span> np.<span class="bu">sum</span>(w)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Effective sample size</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>ESS <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> np.<span class="bu">sum</span>(w_norm<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Weighted means</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>mean_x_is <span class="op">=</span> np.<span class="bu">sum</span>(w_norm <span class="op">*</span> x_is)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>mean_y_is <span class="op">=</span> np.<span class="bu">sum</span>(w_norm <span class="op">*</span> y_is)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Importance Sampling: N=</span><span class="sc">{</span>N_is<span class="sc">}</span><span class="ss">, ESS=</span><span class="sc">{</span>ESS<span class="sc">:.1f}</span><span class="ss"> (</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>ESS<span class="op">/</span>N_is<span class="sc">:.1f}</span><span class="ss">% of N)"</span>)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted means: E[x]=</span><span class="sc">{</span>mean_x_is<span class="sc">:.3f}</span><span class="ss">, E[y]=</span><span class="sc">{</span>mean_y_is<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Importance Sampling: N=10000, ESS=9959.7 (99.6% of N)
Weighted means: E[x]=-0.003, E[y]=0.007</code></pre>
</div>
</div>
<div id="a737dd4c" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize weighted samples</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>sc <span class="op">=</span> ax.scatter(x_is, y_is, s<span class="op">=</span><span class="dv">8</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">#cb = plt.colorbar(sc, ax=ax)</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">#cb.set_label('Normalized weight')</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'Importance Sampling</span><span class="ch">\n</span><span class="ss">ESS=</span><span class="sc">{</span>ESS<span class="sc">:.0f}</span><span class="ss"> of N=</span><span class="sc">{</span>N_is<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay contour of target distribution</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">300</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>y_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">300</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x_vals, y_vals)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.exp(log_p_tilde(X, Y))</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> ax.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">10</span>, colors<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Experiment: Modifying the Importance Sampling Proposal Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What happens if you change the proposal distribution to have a larger or smaller variance? Does this improve the coverage of the posterior?</li>
<li>Compare the Effective Sample Size (ESS) for different proposal variances – does this align with your intuition when you compare the coverage of the sample points with the posterior?</li>
</ul>
</div>
</div>
<p>Now that we have seen the limitations of importance sampling, let’s see how MCMC can help us better sample from this complex posterior. We will implement Metropolis-Hastings below:</p>
<div id="fcd9aa48" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Metropolis–Hastings MCMC on the banana target</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mcmc_banana(N<span class="op">=</span><span class="dv">10_000</span>, sigma<span class="op">=</span><span class="fl">0.5</span>, burn_in<span class="op">=</span><span class="dv">1_000</span>, start<span class="op">=</span>(<span class="fl">0.0</span>, <span class="fl">0.0</span>), seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run a simple MH sampler with isotropic Gaussian proposals."""</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    rng_local <span class="op">=</span> np.random.default_rng(seed)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    x_curr, y_curr <span class="op">=</span> <span class="bu">float</span>(start[<span class="dv">0</span>]), <span class="bu">float</span>(start[<span class="dv">1</span>])</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    logp_curr <span class="op">=</span> log_p_tilde(x_curr, y_curr)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.zeros((N, <span class="dv">2</span>), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    accepts <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Propose from q(x'|x) = N(x, sigma^2 I)</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        dx, dy <span class="op">=</span> rng_local.normal(scale<span class="op">=</span>sigma, size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        x_prop, y_prop <span class="op">=</span> x_curr <span class="op">+</span> dx, y_curr <span class="op">+</span> dy</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        logp_prop <span class="op">=</span> log_p_tilde(x_prop, y_prop)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Symmetric proposal -&gt; Hastings ratio reduces to p~(x')/p~(x)</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        log_alpha <span class="op">=</span> logp_prop <span class="op">-</span> logp_curr</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.log(rng_local.uniform()) <span class="op">&lt;</span> <span class="bu">min</span>(<span class="fl">0.0</span>, log_alpha):</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>            x_curr, y_curr <span class="op">=</span> x_prop, y_prop</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>            logp_curr <span class="op">=</span> logp_prop</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>            accepts <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        samples[i] <span class="op">=</span> (x_curr, y_curr)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    acc_rate <span class="op">=</span> accepts <span class="op">/</span> N</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    chain <span class="op">=</span> samples[burn_in:]</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    mean_x_mcmc <span class="op">=</span> <span class="bu">float</span>(np.mean(chain[:, <span class="dv">0</span>]))</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    mean_y_mcmc <span class="op">=</span> <span class="bu">float</span>(np.mean(chain[:, <span class="dv">1</span>]))</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> chain, acc_rate, mean_x_mcmc, mean_y_mcmc</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="4c543b83" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run MCMC</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co">## Try changing these and see what happens to the behavior of the MCMC samples</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (e.g., no burn-in, different starting point, low vs high sigma)</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>N_mcmc <span class="op">=</span> <span class="dv">100_000</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>burn_in <span class="op">=</span> <span class="dv">1_000</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> (<span class="fl">0.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>chain, acc_rate, mean_x_mcmc, mean_y_mcmc <span class="op">=</span> mcmc_banana(N<span class="op">=</span>N_mcmc, sigma<span class="op">=</span>sigma, burn_in<span class="op">=</span>burn_in, start<span class="op">=</span>start, seed<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MCMC: N=</span><span class="sc">{</span>N_mcmc<span class="sc">}</span><span class="ss">, burn-in=</span><span class="sc">{</span>burn_in<span class="sc">}</span><span class="ss">, sigma=</span><span class="sc">{</span>sigma<span class="sc">}</span><span class="ss">, acceptance=</span><span class="sc">{</span>acc_rate<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MCMC means: E[x]=</span><span class="sc">{</span>mean_x_mcmc<span class="sc">:.3f}</span><span class="ss">, E[y]=</span><span class="sc">{</span>mean_y_mcmc<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>MCMC: N=100000, burn-in=1000, sigma=0.1, acceptance=0.96
MCMC means: E[x]=1.061, E[y]=0.729</code></pre>
</div>
</div>
<div id="dd7c9b72" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">13</span>, <span class="dv">5</span>))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Left: MCMC (plot scatter and path)</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>ax.scatter(chain[:,<span class="dv">0</span>], chain[:,<span class="dv">1</span>], s<span class="op">=</span><span class="dv">5</span>, c<span class="op">=</span><span class="st">'tab:orange'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="st">'MCMC samples'</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a thin path for the first few thousand steps to show exploration</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>path_len <span class="op">=</span> <span class="bu">min</span>(<span class="dv">2000</span>, <span class="bu">len</span>(chain))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>ax.plot(chain[:path_len,<span class="dv">0</span>], chain[:path_len,<span class="dv">1</span>], color<span class="op">=</span><span class="st">'tab:red'</span>, linewidth<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Trace (first 2k)'</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'MCMC (MH)</span><span class="ch">\n</span><span class="ss">accept.=</span><span class="sc">{</span>acc_rate<span class="sc">:.2f}</span><span class="ss">, N=</span><span class="sc">{</span>N_mcmc<span class="sc">}</span><span class="ss">, burn-in=</span><span class="sc">{</span>burn_in<span class="sc">}</span><span class="ss">, sigma=</span><span class="sc">{</span>sigma<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'upper right'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># overlay contour of target distribution</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">300</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>y_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">300</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x_vals, y_vals)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.exp(log_p_tilde(X, Y))</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> ax.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">10</span>, colors<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Right: Line plot of MCMC trace</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>ax.plot(chain[:,<span class="dv">0</span>], label<span class="op">=</span><span class="st">'x'</span>, color<span class="op">=</span><span class="st">'tab:blue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>ax.plot(chain[:,<span class="dv">1</span>], label<span class="op">=</span><span class="st">'y'</span>, color<span class="op">=</span><span class="st">'tab:green'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'MCMC Trace Plot'</span>)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Value'</span>)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Summary diagnostics:"</span>)</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Importance Sampling: ESS=</span><span class="sc">{</span>ESS<span class="sc">:.1f}</span><span class="ss">/</span><span class="sc">{</span>N_is<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span>ESS<span class="op">/</span>N_is<span class="sc">:.1f}</span><span class="ss">%), E[x]=</span><span class="sc">{</span>mean_x_is<span class="sc">:.3f}</span><span class="ss">, E[y]=</span><span class="sc">{</span>mean_y_is<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  MCMC: acceptance=</span><span class="sc">{</span>acc_rate<span class="sc">:.2f}</span><span class="ss">, E[x]=</span><span class="sc">{</span>mean_x_mcmc<span class="sc">:.3f}</span><span class="ss">, E[y]=</span><span class="sc">{</span>mean_y_mcmc<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Summary diagnostics:
  Importance Sampling: ESS=9959.7/10000 (99.6%), E[x]=-0.003, E[y]=0.007
  MCMC: acceptance=0.96, E[x]=1.061, E[y]=0.729</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Experiment: Metropolis-Hastings sampling parameters
</div>
</div>
<div class="callout-body-container callout-body">
<p>Modify the number of samples, proposal variance, burn-in period, and the starting point. - What happens if the proposal variance is very small? Is the chain able to explore the posterior well? How is this behavior reflected in the acceptance rate or the trace plot? - What happens if the starting point is very far from the high-density region of the posterior? How long does it take for the chain to converge to the target distribution? What does this imply for the burn-in period and how is this reflected in the trace plot? - How do the various factors (proposal variance, starting point, burn-in) interact to affect the acceptance rate and the quality of the samples? Is a higher or lower acceptance rate always better? Why do you think this is?</p>
</div>
</div>
</section>
<section id="why-is-struggles-and-mcmc-succeeds" class="level3" data-number="7.4.5">
<h3 data-number="7.4.5" class="anchored" data-anchor-id="why-is-struggles-and-mcmc-succeeds"><span class="header-section-number">7.4.5</span> Why IS struggles and MCMC succeeds</h3>
<ul>
<li>Importance Sampling (IS) relies on a global reweighting of proposal samples. When <span class="math inline">\(q\)</span> poorly overlaps with the curved high-density region of the banana target, almost all weights are near zero. This leads to a tiny effective sample size (ESS) and high estimator variance.</li>
<li>MCMC with local proposals uses acceptance ratios that only depend on pairwise density ratios. Even with an unnormalized target <span class="math inline">\(\tilde p\)</span>, the normalizing constant cancels and the chain can move along the curved ridge, accumulating many effective samples.</li>
<li>Both methods can use unnormalized <span class="math inline">\(p\)</span>; the practical difference is normalization: IS requires a global normalization of weights, while MCMC only needs local ratios. As dimensionality and curvature increase, IS becomes very brittle unless <span class="math inline">\(q\)</span> is very well matched to <span class="math inline">\(p\)</span>, whereas MCMC typically remains viable with reasonable proposals.</li>
</ul>
</section>
<section id="returning-to-neural-networks-now-with-mcmc-sampling" class="level3" data-number="7.4.6">
<h3 data-number="7.4.6" class="anchored" data-anchor-id="returning-to-neural-networks-now-with-mcmc-sampling"><span class="header-section-number">7.4.6</span> Returning to Neural Networks (now with MCMC sampling)</h3>
<p>Now that we have a good understanding of how MCMC works on a simple 2D example, let’s return to our small neural network and use MCMC to sample from its posterior distribution over weights. This will allow us to approximate the posterior predictive distribution for our nonlinear regression problem. We will add on a few tricks that we didn’t have in our simple 2D example, namely: - We’ll thin out samples in the chain to reduce correlation between samples - We’ll discard a burn-in period to allow the chain to converge before collecting samples (as we did before) - Before starting the MCMC chain, we’ll pretrain the network using MAP estimation to find a good starting point for the chain.</p>
<div id="3417dac5" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple MCMC sampler for neural network weights</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(model, x, y, noise_std<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute log p(y|x,w) for current weights."""</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        x_tensor <span class="op">=</span> torch.FloatTensor(x)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(x_tensor).numpy()</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(((y <span class="op">-</span> y_pred) <span class="op">/</span> noise_std) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">-=</span> <span class="bu">len</span>(y) <span class="op">*</span> np.log(noise_std <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_lik</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_prior(model, prior_std<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute log p(w) for current weights."""</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    log_p <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        log_p <span class="op">-=</span> <span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>((param <span class="op">/</span> prior_std) <span class="op">**</span> <span class="dv">2</span>).item()</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_p</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metropolis_hastings_nn(x_train, y_train, n_samples<span class="op">=</span><span class="dv">5000</span>, proposal_std<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>                           prior_std<span class="op">=</span><span class="fl">1.0</span>, noise_std<span class="op">=</span><span class="fl">0.1</span>, thin<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Simple Metropolis-Hastings MCMC for neural network weights.</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="co">        thin: Only keep every 'thin'-th sample to reduce autocorrelation</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="co">        weight_samples: List of weight vectors</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="co">        acceptance_rate: Fraction of proposals accepted</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SmallNN(hidden_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize at MAP estimate (rough approximation: just train briefly)</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>    x_tensor <span class="op">=</span> torch.FloatTensor(x_train).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>    y_tensor <span class="op">=</span> torch.FloatTensor(y_train)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Finding initial weights (quick MAP estimate)..."</span>)</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.mse_loss(model(x_tensor.squeeze()), y_tensor)</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Starting MCMC sampling (</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss"> iterations)..."</span>)</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Current state</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>    current_log_posterior <span class="op">=</span> log_likelihood(model, x_train, y_train, noise_std) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>                            log_prior(model, prior_std)</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    weight_samples <span class="op">=</span> []</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>    n_accepted <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Propose new weights</span></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>        proposed_model <span class="op">=</span> SmallNN(hidden_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> p_curr, p_prop <span class="kw">in</span> <span class="bu">zip</span>(model.parameters(), proposed_model.parameters()):</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>                p_prop.data <span class="op">=</span> p_curr.data <span class="op">+</span> proposal_std <span class="op">*</span> torch.randn_like(p_curr)</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute proposed log posterior</span></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>        proposed_log_posterior <span class="op">=</span> log_likelihood(proposed_model, x_train, y_train, noise_std) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>                                  log_prior(proposed_model, prior_std)</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accept/reject</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>        log_alpha <span class="op">=</span> proposed_log_posterior <span class="op">-</span> current_log_posterior</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.log(np.random.rand()) <span class="op">&lt;</span> log_alpha:</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Accept</span></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> proposed_model</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>            current_log_posterior <span class="op">=</span> proposed_log_posterior</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>            n_accepted <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store sample (with thinning)</span></span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> thin <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>            weight_samples.append(model.get_weights_flat().clone().detach().numpy())</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">2000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>            acc_rate <span class="op">=</span> n_accepted <span class="op">/</span> (i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Iteration </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">, acceptance rate: </span><span class="sc">{</span>acc_rate<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>    acceptance_rate <span class="op">=</span> n_accepted <span class="op">/</span> n_samples</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"MCMC complete! Final acceptance rate: </span><span class="sc">{</span>acceptance_rate<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weight_samples, acceptance_rate</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Run MCMC (this will take a minute or two)</span></span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Running MCMC to sample from posterior p(w|X,Y)..."</span>)</span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This may take 1-2 minutes for accurate results."</span>)</span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a>mcmc_samples, acc_rate <span class="op">=</span> metropolis_hastings_nn(</span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a>    x_train, y_train, </span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">10_000</span>, </span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a>    proposal_std<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>    prior_std<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>    noise_std<span class="op">=</span>noise_std,</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a>    thin<span class="op">=</span><span class="dv">10</span></span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Collected </span><span class="sc">{</span><span class="bu">len</span>(mcmc_samples)<span class="sc">}</span><span class="ss"> posterior samples (after thinning)"</span>)</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Acceptance rate: </span><span class="sc">{</span>acc_rate<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Running MCMC to sample from posterior p(w|X,Y)...
This may take 1-2 minutes for accurate results.
Finding initial weights (quick MAP estimate)...
Starting MCMC sampling (10000 iterations)...
Finding initial weights (quick MAP estimate)...
Starting MCMC sampling (10000 iterations)...
  Iteration 2000/10000, acceptance rate: 0.460
  Iteration 2000/10000, acceptance rate: 0.460
  Iteration 4000/10000, acceptance rate: 0.458
  Iteration 4000/10000, acceptance rate: 0.458
  Iteration 6000/10000, acceptance rate: 0.462
  Iteration 6000/10000, acceptance rate: 0.462
  Iteration 8000/10000, acceptance rate: 0.466
  Iteration 8000/10000, acceptance rate: 0.466
  Iteration 10000/10000, acceptance rate: 0.455
MCMC complete! Final acceptance rate: 0.455
Collected 1000 posterior samples (after thinning)
Acceptance rate: 45.5%
  Iteration 10000/10000, acceptance rate: 0.455
MCMC complete! Final acceptance rate: 0.455
Collected 1000 posterior samples (after thinning)
Acceptance rate: 45.5%</code></pre>
</div>
</div>
<div id="a56f8901" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the trace plots for all of the weights as a quick diagnostic using figure subplots:</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>mcmc_samples_array <span class="op">=</span> np.array(mcmc_samples)  <span class="co"># shape (n_samples, n_params)</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>n_params <span class="op">=</span> mcmc_samples_array.shape[<span class="dv">1</span>]</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(n_params, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, n_params), sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_params):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    axes[i].plot(mcmc_samples_array[:, i], color<span class="op">=</span><span class="st">'tab:blue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    axes[i].set_ylabel(<span class="ss">f'W[</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">]'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    axes[i].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>axes[<span class="op">-</span><span class="dv">1</span>].set_xlabel(<span class="st">'MCMC Sample Index'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'MCMC Trace Plots for Neural Network Weights'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>plt.tight_layout(rect<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.96</span>])</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="a4e7a3ea" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize MCMC posterior predictive</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_with_weight_samples(weight_samples, x_test, n_samples<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generate predictions using sampled weights."""</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n_samples <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> <span class="bu">len</span>(weight_samples)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    model_temp <span class="op">=</span> SmallNN(hidden_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    x_test_tensor <span class="op">=</span> torch.FloatTensor(x_test)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Randomly select samples to plot</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    sample_indices <span class="op">=</span> np.random.choice(<span class="bu">len</span>(weight_samples), size<span class="op">=</span><span class="bu">min</span>(n_samples, <span class="bu">len</span>(weight_samples)), replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx <span class="kw">in</span> sample_indices:</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>            model_temp.set_weights_flat(torch.FloatTensor(weight_samples[idx]))</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model_temp(x_test_tensor).numpy()</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>            predictions.append(y_pred)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(predictions)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate MCMC posterior predictions</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>mcmc_predictions <span class="op">=</span> predict_with_weight_samples(mcmc_samples, x_test, n_samples<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize</span></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">7</span>))</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot MCMC posterior samples</span></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mcmc_predictions)):</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, mcmc_predictions[i], <span class="st">'orange'</span>, alpha<span class="op">=</span><span class="fl">0.15</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a><span class="co"># True function and data</span></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True function'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="dv">2</span>, zorder<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior mean</span></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>mcmc_mean <span class="op">=</span> mcmc_predictions.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, mcmc_mean, <span class="st">'orange'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'MCMC posterior mean'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Dummy line for legend</span></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>ax.plot([], [], <span class="st">'orange'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MCMC posterior samples'</span>)</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'MCMC Posterior Predictive Distribution</span><span class="ch">\n</span><span class="st">(Reference "Ground Truth")'</span>, </span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="advanced-sampling-hamiltonian-monte-carlo-hmc" class="level3" data-number="7.4.7">
<h3 data-number="7.4.7" class="anchored" data-anchor-id="advanced-sampling-hamiltonian-monte-carlo-hmc"><span class="header-section-number">7.4.7</span> Advanced Sampling: Hamiltonian Monte Carlo (HMC)</h3>
<p>There have been many substantial improvements to MCMC sampling methods over the past several decades, and one of the main advances came through the recognition that we need not only sample with knowledge of <span class="math inline">\(q\)</span> and <span class="math inline">\(p\)</span>, but that we could leverage the <em>gradients</em> of <span class="math inline">\(p\)</span>. This was previously very limited or difficult to do, but with the widespread use of Automatic Differentiation libraries became much more powerful.</p>
<p>Upon widespread access to gradients of <span class="math inline">\(p\)</span>, we could now think about sampling processes that could leverage the <em>slope</em> and <em>curvature</em> of the likelihood to guide us toward better samples. One of the mainstream ways to do this is <strong>Hamiltonian Monte Carlo</strong> which treats sampling as analogous to estimating a dynamical system. Specifically, we treat the samples/parameters as positions, introduce an auxiliary momentum variable, and simulate Hamiltonian dynamics that preserve the joint density, typically using a symplectic integretor, such as the leapfrog integrator. Because the leapfrog integrator follows smooth trajectories, HMC avoids the random walk behaviour that slowed down our Metropolis–Hastings chains above. HMC is still fundamentally accepting or rejecting samples like MH did, accept that it’s method of producing new samples is significantly more likely to lead to acceptable samples, and thus can be far more sample efficient.</p>
<p>It does this through a few basic concepts: - It uses the likelihood as a Potential Energy function <span class="math inline">\(U(\theta) = -\log p(\theta \mid \text{data})\)</span> which pulls particle trajectories toward higher posterior density. - Each particle has Kinetic Energy <span class="math inline">\(K(\mathbf{p}) = \tfrac{1}{2} \mathbf{p}^\top M^{-1} \mathbf{p}\)</span> which comes from a tunable mass matrix <span class="math inline">\(M\)</span> that can re-scale parameters. - A symplectic integrator (Leapfrog integration) alternates half-steps in momentum with full steps in position and nearly conserves energy.</p>
<p>The algorith itself is fairly straightforward in that you:</p>
<ol type="1">
<li>Sample a fresh momentum from <span class="math inline">\(\mathcal{N}(0, M)\)</span> to randomize the next trajectory.</li>
<li>Integrate the Hamiltonian dynamics for <span class="math inline">\(L\)</span> leapfrog steps with step size <span class="math inline">\(\varepsilon\)</span>.</li>
<li>Apply a Metropolis accept/reject step using the Hamiltonian.</li>
</ol>
<p>Some pecularities with HMC include: - Smaller step size <span class="math inline">\(\varepsilon\)</span> improves accuracy but increases computation. If step sizes are too large, it can overshoot and causes many sample rejections. - The number of leapfrog steps <span class="math inline">\(L\)</span> controls how far each proposal travels across the posterior ridge. Too small and the samples become auto-correlated since the sample cannot move far enough away from the current sample. - A well-chosen mass matrix and initial adaptation help HMC cope with areas of strong curvature in the posterior. - HMC relies on gradients, so we need target distributions that are differentiable and numerically stable.</p>
<p>Below we implement a minimal HMC sampler for the banana distribution so you can compare its behaviour with the random-walk MH sampler above.</p>
<div id="17d8269c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hamiltonian Monte Carlo on the banana target</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_log_p_tilde(x: np.ndarray, y: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute the gradient of the banana log-density with respect to (x, y)."""</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.asarray(x)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.asarray(y)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    grad_x <span class="op">=</span> <span class="op">-</span>x <span class="op">/</span> <span class="fl">9.0</span> <span class="op">+</span> <span class="fl">0.2</span> <span class="op">*</span> x <span class="op">*</span> (y <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    grad_y <span class="op">=</span> <span class="op">-</span>(y <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.stack([grad_x, grad_y], axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> hmc_banana(num_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8_000</span>, burn_in: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1_000</span>, step_size: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.25</span>,</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>                num_leapfrog: <span class="bu">int</span> <span class="op">=</span> <span class="dv">20</span>, mass: np.ndarray <span class="op">|</span> <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>                start: <span class="bu">tuple</span>[<span class="bu">float</span>, <span class="bu">float</span>] <span class="op">=</span> (<span class="fl">0.0</span>, <span class="fl">0.0</span>), seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2024</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[np.ndarray, <span class="bu">float</span>]:</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Run a basic HMC sampler with (scalar or diagonal) mass matrix for the banana target."""</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    rng_local <span class="op">=</span> np.random.default_rng(seed)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    q_current <span class="op">=</span> np.array(start, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> q_current.shape <span class="op">!=</span> (<span class="dv">2</span>,):</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"start must be a length-2 tuple or array."</span>)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    mass_array <span class="op">=</span> np.asarray(mass, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mass_array.ndim <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>        mass_vec <span class="op">=</span> np.full(<span class="dv">2</span>, mass_array.item())</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> mass_array.shape <span class="op">==</span> (<span class="dv">2</span>,):</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>        mass_vec <span class="op">=</span> mass_array</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"mass must be a scalar or length-2 array for this example."</span>)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.<span class="bu">any</span>(mass_vec <span class="op">&lt;=</span> <span class="dv">0</span>):</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"mass components must be positive."</span>)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    inv_mass <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> mass_vec</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>    logp_current <span class="op">=</span> log_p_tilde(q_current[<span class="dv">0</span>], q_current[<span class="dv">1</span>])</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.zeros((num_samples, <span class="dv">2</span>), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>    accepts <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_samples):</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample fresh momentum from N(0, M) with diagonal mass matrix.</span></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>        p_current <span class="op">=</span> rng_local.normal(scale<span class="op">=</span>np.sqrt(mass_vec), size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q_current.copy()</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> p_current.copy()</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Leapfrog integration</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> grad_log_p_tilde(q[<span class="dv">0</span>], q[<span class="dv">1</span>])</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>        p <span class="op">+=</span> <span class="fl">0.5</span> <span class="op">*</span> step_size <span class="op">*</span> grad</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> lf_step <span class="kw">in</span> <span class="bu">range</span>(num_leapfrog):</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>            q <span class="op">+=</span> step_size <span class="op">*</span> p <span class="op">*</span> inv_mass</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">=</span> grad_log_p_tilde(q[<span class="dv">0</span>], q[<span class="dv">1</span>])</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> lf_step <span class="op">!=</span> num_leapfrog <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>                p <span class="op">+=</span> step_size <span class="op">*</span> grad</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>        p <span class="op">+=</span> <span class="fl">0.5</span> <span class="op">*</span> step_size <span class="op">*</span> grad</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="op">-</span>p  <span class="co"># Reverse momentum for detailed balance</span></span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>        logp_prop <span class="op">=</span> log_p_tilde(q[<span class="dv">0</span>], q[<span class="dv">1</span>])</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>        current_H <span class="op">=</span> <span class="op">-</span>logp_current <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>((p_current<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> inv_mass)</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>        proposed_H <span class="op">=</span> <span class="op">-</span>logp_prop <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>((p<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> inv_mass)</span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>        log_alpha <span class="op">=</span> current_H <span class="op">-</span> proposed_H</span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.log(rng_local.uniform()) <span class="op">&lt;</span> <span class="bu">min</span>(<span class="fl">0.0</span>, log_alpha):</span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>            q_current <span class="op">=</span> q</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a>            logp_current <span class="op">=</span> logp_prop</span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>            accepts <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>        samples[i] <span class="op">=</span> q_current</span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> burn_in <span class="op">&gt;=</span> num_samples:</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"burn_in must be smaller than num_samples."</span>)</span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>    chain <span class="op">=</span> samples[burn_in:]</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>    acc_rate <span class="op">=</span> accepts <span class="op">/</span> num_samples</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> chain, acc_rate</span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Run HMC and summarize diagnostics</span></span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>N_hmc <span class="op">=</span> <span class="dv">8_000</span></span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a>burn_in_hmc <span class="op">=</span> <span class="dv">1_000</span></span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a>step_size <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>num_leapfrog <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a>mass_vec <span class="op">=</span> np.array([<span class="fl">0.7</span>, <span class="fl">1.3</span>])  <span class="co"># Slight re-scaling along x and y</span></span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a>hmc_chain, hmc_acc_rate <span class="op">=</span> hmc_banana(num_samples<span class="op">=</span>N_hmc, burn_in<span class="op">=</span>burn_in_hmc,</span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a>                                     step_size<span class="op">=</span>step_size, num_leapfrog<span class="op">=</span>num_leapfrog,</span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a>                                     mass<span class="op">=</span>mass_vec, start<span class="op">=</span>(<span class="fl">10.0</span>, <span class="fl">10.0</span>), seed<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a>mean_x_hmc <span class="op">=</span> <span class="bu">float</span>(np.mean(hmc_chain[:, <span class="dv">0</span>]))</span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a>mean_y_hmc <span class="op">=</span> <span class="bu">float</span>(np.mean(hmc_chain[:, <span class="dv">1</span>]))</span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"HMC: N=</span><span class="sc">{</span>N_hmc<span class="sc">}</span><span class="ss">, burn-in=</span><span class="sc">{</span>burn_in_hmc<span class="sc">}</span><span class="ss">, step size=</span><span class="sc">{</span>step_size<span class="sc">}</span><span class="ss">, L=</span><span class="sc">{</span>num_leapfrog<span class="sc">}</span><span class="ss">, acceptance=</span><span class="sc">{</span>hmc_acc_rate<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"HMC means: E[x]=</span><span class="sc">{</span>mean_x_hmc<span class="sc">:.3f}</span><span class="ss">, E[y]=</span><span class="sc">{</span>mean_y_hmc<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>HMC: N=8000, burn-in=1000, step size=0.25, L=20, acceptance=1.00
HMC means: E[x]=0.046, E[y]=0.860</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-17-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="39a4fee4" class="cell">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize samples and trace using the same style as earlier</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">13</span>, <span class="dv">5</span>))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>ax.scatter(hmc_chain[:, <span class="dv">0</span>], hmc_chain[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">5</span>, c<span class="op">=</span><span class="st">'tab:purple'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="st">'HMC samples'</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>path_len <span class="op">=</span> <span class="bu">min</span>(<span class="dv">2_000</span>, <span class="bu">len</span>(hmc_chain))</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>ax.plot(hmc_chain[:path_len, <span class="dv">0</span>], hmc_chain[:path_len, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'tab:purple'</span>, linewidth<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Trace (first 2k)'</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'HMC</span><span class="ch">\n</span><span class="ss">accept.=</span><span class="sc">{</span>hmc_acc_rate<span class="sc">:.2f}</span><span class="ss">, N=</span><span class="sc">{</span>N_hmc<span class="sc">}</span><span class="ss">, burn-in=</span><span class="sc">{</span>burn_in_hmc<span class="sc">}</span><span class="ss">, step=</span><span class="sc">{</span>step_size<span class="sc">}</span><span class="ss">, L=</span><span class="sc">{</span>num_leapfrog<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'upper right'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">300</span>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>y_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">300</span>)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x_vals, y_vals)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.exp(log_p_tilde(X, Y))</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>ax.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">10</span>, colors<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>ax.plot(hmc_chain[:, <span class="dv">0</span>], label<span class="op">=</span><span class="st">'x'</span>, color<span class="op">=</span><span class="st">'tab:blue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>ax.plot(hmc_chain[:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">'y'</span>, color<span class="op">=</span><span class="st">'tab:green'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'HMC Trace Plot'</span>)</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Value'</span>)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Experiment: Tuning HMC Hyperparameters
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Decrease the step size while increasing the leapfrog count so the trajectory length stays similar. What happens to acceptance and exploration?</li>
<li>Try a very large step size or very few leapfrog steps. Do you see the chain reverting to random-walk behaviour?</li>
<li>Replace the scalar mass with a small diagonal matrix (e.g., different masses for <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>). Does this help the sampler hug the banana ridge more quickly?</li>
</ul>
</div>
</div>
</section>
</section>
<section id="variational-inference" class="level2 page-columns page-full" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="variational-inference"><span class="header-section-number">7.5</span> Variational Inference</h2>
<p>MCMC works but is slow, since it essentially has to generate thousands of good samples. In constrast, Variational Inference methods turn the <strong>integration</strong> problem into an <strong>optimization</strong> problem. Instead of sampling from <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span>, we:</p>
<ol type="1">
<li>Choose a tractable family <span class="math inline">\(q_\phi(\mathbf{w})\)</span> (e.g., Gaussian)</li>
<li>Find <span class="math inline">\(\phi\)</span> that makes <span class="math inline">\(q_\phi\)</span> close to the true posterior</li>
<li>Measure “closeness” using the <strong>ELBO</strong> (Evidence Lower Bound)</li>
</ol>
<section id="chosing-a-tractable-family-q_phimathbfw" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="chosing-a-tractable-family-q_phimathbfw"><span class="header-section-number">7.5.1</span> Chosing a tractable family <span class="math inline">\(q_\phi(\mathbf{w})\)</span></h3>
<p>There are many options for this, as we will see in later more advanced notebooks, but for now let’s use something simple: a <strong>mean-field Gaussian</strong> approximation: <span class="math display">\[q_\phi(\mathbf{w}) = \mathcal{N}(\boldsymbol{\mu}_\phi, \text{diag}(\boldsymbol{\sigma}_\phi^2))\]</span></p>
<p>We will look at two examples below: the first is just a simple 2D Gaussian with a 2D mean vector and two <span class="math inline">\(\sigma\)</span> s corresponding to a diagonal covariance. Later, we extend this concept to a Neural Network (like we did with MCMC above) each weight in the network will get its own mean and variance, and for our network with <code>n_params</code> parameters, we would then optimize both for all weights, leading to <span class="math inline">\(2 \times {n_{params}}\)</span> variational parameters.</p>
<p>But, in either case, what should we be optimizing, exactly?</p>
</section>
<section id="marginal-likelihood-evidence" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="marginal-likelihood-evidence"><span class="header-section-number">7.5.2</span> 1. Marginal Likelihood (Evidence)</h3>
<p>The goal in Bayesian inference is to compute the <strong>marginal likelihood</strong> (also called the evidence):</p>
<p><span class="math display">\[
p(Y|X) = \int p(Y|X, \mathbf{w})\, p(\mathbf{w})\, d\mathbf{w}
\]</span></p>
<p>Which sounds fine in theory, but this integral is usually <strong>intractable</strong> for neural networks or other complex models we might be interested in. How can we solve this?</p>
</section>
<section id="introducing-a-variational-distribution" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="introducing-a-variational-distribution"><span class="header-section-number">7.5.3</span> 2. Introducing a Variational Distribution</h3>
<p>We introduce a tractable distribution <span class="math inline">\(q_\phi(\mathbf{w})\)</span> to approximate the true posterior <span class="math inline">\(p(\mathbf{w}|X,Y)\)</span>.</p>
<p>We can rewrite the log evidence using <span class="math inline">\(q_\phi\)</span>:</p>
<p><span class="math display">\[
\log p(Y|X) = \log \int p(Y|X, \mathbf{w})\, p(\mathbf{w})\, d\mathbf{w}
\]</span></p>
<p><span class="math display">\[
= \log \int q_\phi(\mathbf{w})\, \frac{p(Y|X, \mathbf{w})\, p(\mathbf{w})}{q_\phi(\mathbf{w})}\, d\mathbf{w}
\]</span></p>
<p>So far, this mathematical maneuver hasn’t really bought us much, but we can apply an approximation to help us separate out some terms.</p>
</section>
<section id="applying-jensens-inequality" class="level3" data-number="7.5.4">
<h3 data-number="7.5.4" class="anchored" data-anchor-id="applying-jensens-inequality"><span class="header-section-number">7.5.4</span> 3. Applying Jensen’s Inequality</h3>
<p>Jensen’s Inequality states that, for a convex function <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
f(\mathbb{E}[x]) \le \mathbb{E}[f(x)]
\]</span></p>
<p>and if we apply Jensen’s inequality to the above assuming <span class="math inline">\(f=\log\)</span> and <span class="math inline">\(\mathbb{E}=\int\)</span> (note we flip the sign of the inequality since <span class="math inline">\(\log\)</span> is concave, not convex), we get:</p>
<p><span class="math display">\[
\log \mathbb{E}_{q_\phi}\left[\frac{p(Y|X, \mathbf{w})\, p(\mathbf{w})}{q_\phi(\mathbf{w})}\right]
\geq \mathbb{E}_{q_\phi}\left[\log \frac{p(Y|X, \mathbf{w})\, p(\mathbf{w})}{q_\phi(\mathbf{w})}\right]
\]</span></p>
<p>Now we can bring the log inside to help break out some of the terms and rearrange them.</p>
</section>
<section id="the-elbo-expression" class="level3" data-number="7.5.5">
<h3 data-number="7.5.5" class="anchored" data-anchor-id="the-elbo-expression"><span class="header-section-number">7.5.5</span> 4. The ELBO Expression</h3>
<p>This gives us the <strong>Evidence Lower Bound (ELBO)</strong>:</p>
<p><span class="math display">\[
\log p(Y|X) \geq \mathbb{E}_{q_\phi}\left[\log p(Y|X, \mathbf{w})\right]
+ \mathbb{E}_{q_\phi}\left[\log p(\mathbf{w})\right]
- \mathbb{E}_{q_\phi}\left[\log q_\phi(\mathbf{w})\right]
\]</span></p>
<p>Or, by grouping <span class="math inline">\(\mathbb{E}_{q_\phi}\left[\log p(\mathbf{w})\right]\)</span> and <span class="math inline">\(\mathbb{E}_{q_\phi}\left[\log q_\phi(\mathbf{w})\right]\)</span> into the (negative) KL Divergence:</p>
<p><span class="math display">\[
\mathcal{L}(\phi) = \mathbb{E}_{q_\phi}\left[\log p(Y|X, \mathbf{w})\right]
- \mathrm{KL}\left(q_\phi(\mathbf{w})\,\|\,p(\mathbf{w})\right)
\]</span></p>
<p>We arrive at the common form of the ELBO, where:</p>
<ul>
<li>The <strong>first term</strong> encourages <span class="math inline">\(q_\phi\)</span> to place mass on weights that explain the data well. This is the <strong>Expected log-likelihood</strong> and measures How well <span class="math inline">\(q_\phi\)</span> explains the data.</li>
<li>The <strong>second term</strong> (the <strong>KL divergence</strong>) regularizes <span class="math inline">\(q_\phi\)</span> to stay close to the prior.</li>
</ul>
<p>This is the objective we will then optimize when we perform variational inference. We will see this first applied to a simple 2D example so that we can understand what is going on, and then later move to a more complex and higher dimension example of a Neural Network.</p>
</section>
<section id="example-of-using-vi-for-the-banana-distribution" class="level3" data-number="7.5.6">
<h3 data-number="7.5.6" class="anchored" data-anchor-id="example-of-using-vi-for-the-banana-distribution"><span class="header-section-number">7.5.6</span> Example of using VI for the Banana Distribution</h3>
<p>Let’s use our concrete example from earlier, the Banana distribution, to illustrate how Variational Inference works. Here we will use a very simple proposal distribution—a Gaussian with a mean and diagonal covariance—which is often called the <em>mean-fild Gaussian approximation</em> as <span class="math inline">\(q_\phi(\mathbf{w})\)</span>. We will then optimize the ELBO to approximate the posterior. That is, we will optimize:</p>
<p><span class="math display">\[
\begin{align}
\mathcal{L}(\phi) &amp;= \mathbb{E}_{q_\phi}\left[\log p(Y|X, \mathbf{w})\right]
- \mathrm{KL}\left(q_\phi(\mathbf{w})\,\|\,p(\mathbf{w})\right)\\
&amp; = \mathbb{E}_{q_\phi}\left[\log p(Y|X, \mathbf{w})\right]
+ \mathbb{E}_{q_\phi}\left[\log p(\mathbf{w})\right]
- \mathbb{E}_{q_\phi}\left[\log q_\phi(\mathbf{w})\right]\\
&amp; = \mathbb{E}_{q_\phi}\left[\log p(Y,w|X)\right]
- \mathbb{E}_{q_\phi}\left[\log q_\phi(\mathbf{w})\right]
\end{align}
\]</span></p>
<p>The reason that we are only computing <span class="math inline">\(\mathbb{E}_{q_\phi}\left[\log p(Y,w|X)\right]\)</span> in this case is because we are using a simple toy distribution where we are computing the joint distribution <span class="math inline">\(p(Y,w|X)\)</span> directly, rather than having a likelihood and prior separately as we would in a more complex model like a neural network.</p>
<div id="22957a28" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variational Inference on the banana target distribution with interactive visualization</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>rng_vi <span class="op">=</span> np.random.default_rng(<span class="dv">0</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>LOG_2PI <span class="op">=</span> math.log(<span class="fl">2.0</span> <span class="op">*</span> math.pi)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mf_elbo(mu_vec: torch.Tensor, raw_diag: torch.Tensor, num_samples: <span class="bu">int</span> <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Analytic ELBO for a mean-field (diagonal) Gaussian q.</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Assumes raw_diag stores log(sigma) for each dimension.</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes E_q[log p] analytically for the banana target:</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co">        log p(x,y) = -0.5 * ( x^2/9 + (y - 0.1 x^2)^2 )</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co">    and uses the closed-form moments of a Gaussian.</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># diagonal std and variances</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> torch.exp(raw_diag)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> sigma<span class="op">**</span><span class="dv">2</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    mu_x, mu_y <span class="op">=</span> mu_vec[<span class="dv">0</span>], mu_vec[<span class="dv">1</span>]</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    var_x, var_y <span class="op">=</span> var[<span class="dv">0</span>], var[<span class="dv">1</span>]</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments for x</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    E_x2 <span class="op">=</span> var_x <span class="op">+</span> mu_x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Var(x^2) for Gaussian x ~ N(mu_x, var_x)</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    Var_x2 <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> var_x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">4.0</span> <span class="op">*</span> mu_x<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> var_x</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First term in the ELBO</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># E_q[ (y - 0.1 x^2)^2 ] = Var(y) + (E[y] - 0.1 E[x^2])^2 + 0.01 Var(x^2)</span></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>    E_y_term <span class="op">=</span> var_y <span class="op">+</span> (mu_y <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> E_x2) <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> Var_x2</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second term in the ELBO</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># E_q[log p]</span></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    E_log_p <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (E_x2 <span class="op">/</span> <span class="fl">9.0</span> <span class="op">+</span> E_y_term)</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Third term in the ELBO</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># E_q[log q] for diagonal Gaussian: -0.5*(D + D*log(2π) + 2 * sum(log sigma))</span></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> mu_vec.numel()</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>    sum_log_sigma <span class="op">=</span> torch.<span class="bu">sum</span>(raw_diag)</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>    E_log_q <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (D <span class="op">+</span> D <span class="op">*</span> LOG_2PI <span class="op">+</span> <span class="fl">2.0</span> <span class="op">*</span> sum_log_sigma)</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> E_log_p <span class="op">-</span> E_log_q</span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_vi_optimization(num_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">800</span>, lr: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.08</span>, record_points: <span class="bu">int</span> <span class="op">=</span> <span class="dv">60</span>):</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Optimize the ELBO using the mean-field (diagonal) analytic ELBO (mf_elbo).</span></span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a><span class="co">    Records intermediate means, covariances (diagonal), and ELBO values for visualization.</span></span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>, <span class="fl">0.0</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>    raw_diag <span class="op">=</span> torch.log(torch.ones(<span class="dv">2</span>) <span class="op">*</span> <span class="fl">2.5</span>)</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>    raw_diag.requires_grad_(<span class="va">True</span>)</span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> Adam([mu, raw_diag], lr<span class="op">=</span>lr)</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>    record_every <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> {<span class="st">"steps"</span>: [], <span class="st">"mu"</span>: [], <span class="st">"cov"</span>: [], <span class="st">"elbo"</span>: []}</span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>        elbo <span class="op">=</span> mf_elbo(mu, raw_diag)</span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>elbo</span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (step <span class="op">%</span> record_every <span class="op">==</span> <span class="dv">0</span>) <span class="kw">or</span> (step <span class="op">==</span> num_steps <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>            <span class="co"># build diagonal cov_diagonal and covariance for recording/plotting</span></span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>            cov_diagonal <span class="op">=</span> np.diag(np.exp(raw_diag.detach().cpu().numpy()))</span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>            cov_np <span class="op">=</span> cov_diagonal <span class="op">@</span> cov_diagonal.T</span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a>            history[<span class="st">"steps"</span>].append(step)</span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a>            history[<span class="st">"mu"</span>].append(mu.detach().cpu().numpy())</span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a>            history[<span class="st">"cov"</span>].append(cov_np)</span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a>            history[<span class="st">"elbo"</span>].append(elbo.detach().cpu().item())</span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> history</span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Run VI once and cache the trajectory for the interactive plot</span></span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a>vi_history <span class="op">=</span> run_vi_optimization(num_steps<span class="op">=</span><span class="dv">150</span>, record_points<span class="op">=</span><span class="dv">150</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c913fcff" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>history_steps <span class="op">=</span> np.array(vi_history[<span class="st">"steps"</span>])</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>history_mu <span class="op">=</span> np.stack(vi_history[<span class="st">"mu"</span>])</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>history_cov <span class="op">=</span> np.stack(vi_history[<span class="st">"cov"</span>])</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>history_elbo <span class="op">=</span> np.array(vi_history[<span class="st">"elbo"</span>])</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">240</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>y_grid <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">220</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>X_grid, Y_grid <span class="op">=</span> np.meshgrid(x_grid, y_grid)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>target_density_grid <span class="op">=</span> np.exp(log_p_tilde(X_grid, Y_grid))</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gaussian_contour_density(X: np.ndarray, Y: np.ndarray, mu: np.ndarray, cov: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate a full-covariance Gaussian density on a meshgrid."""</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    diff_x <span class="op">=</span> X <span class="op">-</span> mu[<span class="dv">0</span>]</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    diff_y <span class="op">=</span> Y <span class="op">-</span> mu[<span class="dv">1</span>]</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    inv_cov <span class="op">=</span> np.linalg.inv(cov)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    det_cov <span class="op">=</span> <span class="bu">float</span>(np.maximum(np.linalg.det(cov), <span class="fl">1e-12</span>))</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    exponent <span class="op">=</span> (inv_cov[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">*</span> diff_x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>                <span class="fl">2.0</span> <span class="op">*</span> inv_cov[<span class="dv">0</span>, <span class="dv">1</span>] <span class="op">*</span> diff_x <span class="op">*</span> diff_y <span class="op">+</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>                inv_cov[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">*</span> diff_y<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    norm_const <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">2.0</span> <span class="op">*</span> math.pi <span class="op">*</span> np.sqrt(det_cov))</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> norm_const <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> exponent)</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_vi_state(frame_idx: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Render the variational approximation and ELBO trace for the given optimization step."""</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> history_mu[frame_idx]</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>    cov <span class="op">=</span> history_cov[frame_idx]</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>    elbo_val <span class="op">=</span> history_elbo[frame_idx]</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>    vi_density <span class="op">=</span> gaussian_contour_density(X_grid, Y_grid, mu, cov)</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">13</span>, <span class="dv">5</span>))</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>    ax.contour(X_grid, Y_grid, target_density_grid, levels<span class="op">=</span><span class="dv">14</span>, colors<span class="op">=</span><span class="st">'#b3b3b3'</span>, linewidths<span class="op">=</span><span class="fl">1.0</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>    contourf <span class="op">=</span> ax.contourf(X_grid, Y_grid, vi_density, levels<span class="op">=</span><span class="dv">14</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, alpha<span class="op">=</span><span class="fl">0.35</span>)</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>    ax.contour(X_grid, Y_grid, vi_density, levels<span class="op">=</span><span class="dv">14</span>, colors<span class="op">=</span><span class="st">'#1f77b4'</span>, linewidths<span class="op">=</span><span class="fl">1.1</span>, alpha<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>    ax.scatter(mu[<span class="dv">0</span>], mu[<span class="dv">1</span>], color<span class="op">=</span><span class="st">'tab:blue'</span>, s<span class="op">=</span><span class="dv">40</span>, label<span class="op">=</span><span class="st">'VI mean'</span>)</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"VI approximation (step </span><span class="sc">{</span>history_steps[frame_idx]<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>    ax.set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>    ax.legend(loc<span class="op">=</span><span class="st">'upper right'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>    ax_elbo <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>    ax_elbo.plot(history_steps, history_elbo, color<span class="op">=</span><span class="st">'tab:blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>    ax_elbo.axvline(history_steps[frame_idx], color<span class="op">=</span><span class="st">'tab:purple'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>    ax_elbo.scatter(history_steps[frame_idx], elbo_val, color<span class="op">=</span><span class="st">'tab:purple'</span>, s<span class="op">=</span><span class="dv">40</span>)</span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a>    ax_elbo.set_title(<span class="st">'ELBO during VI optimization'</span>)</span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a>    ax_elbo.set_xlabel(<span class="st">'Optimization step'</span>)</span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>    ax_elbo.set_ylabel(<span class="st">'ELBO (MC estimate)'</span>)</span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a>    ax_elbo.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>step_slider <span class="op">=</span> widgets.IntSlider(value<span class="op">=</span><span class="bu">len</span>(history_steps) <span class="op">-</span> <span class="dv">1</span>, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="bu">len</span>(history_steps) <span class="op">-</span> <span class="dv">1</span>, step<span class="op">=</span><span class="dv">1</span>, description<span class="op">=</span><span class="st">'Frame'</span>)</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a>play_ctrl <span class="op">=</span> widgets.Play(interval<span class="op">=</span><span class="dv">120</span>, value<span class="op">=</span><span class="dv">0</span>, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="bu">len</span>(history_steps) <span class="op">-</span> <span class="dv">1</span>, step<span class="op">=</span><span class="dv">1</span>, description<span class="op">=</span><span class="st">'Press play'</span>)</span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a>widgets.jslink((play_ctrl, <span class="st">'value'</span>), (step_slider, <span class="st">'value'</span>))</span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>controls <span class="op">=</span> widgets.HBox([play_ctrl, step_slider])</span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a>interactive_plot <span class="op">=</span> widgets.interactive_output(plot_vi_state, {<span class="st">'frame_idx'</span>: step_slider})</span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>display(controls, interactive_plot)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"6ed27e106d8d4a00ac2ea9df5345ea4e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"26fc3cda6b254c7d9dfa72f876f10b44","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>What do we notice about the VI approximation compared to MCMC? In what ways is it better or worse? Comparing the ELBO plot versus the MCMC trace plot, what differences do you notice about the convergence behavior of the two methods?</p>
</section>
<section id="monte-carlo-variational-inference" class="level3 page-columns page-full" data-number="7.5.7">
<h3 data-number="7.5.7" class="anchored" data-anchor-id="monte-carlo-variational-inference"><span class="header-section-number">7.5.7</span> Monte Carlo Variational Inference</h3>
<p>By assuming a certain form of the proposal distribution <span class="math inline">\(q_\phi(\mathbf{w})\)</span>, we were able to compute the KL divergence term in closed form and perform VI analytically. However, in more complex models (like neural networks), we often cannot compute these expectations exactly. In such cases, we can resort to using <strong>Monte Carlo Variational Inference</strong> to approximate the ELBO and its gradients.</p>
<p>Instead of integrating analytically, we draw samples from our Gaussian variational family and use the <em>score-function identity</em> below to update the parameters. This reliance on Monte Carlo introduces sampling noise, so we will see below how the ELBO estimate bounces around and how the mean and covariance of <span class="math inline">\(q_\phi\)</span> drift across the banana distribution. The only change from the closed-form solution above is how we estimate expectations, where now our gradients still come from the “score function” of the Gaussian.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;There are some tricks to reduce the variance by subtracting a moving “baseline” average to the advantage term, which is implemented below, but we will not go into the details of this here.</p></div></div><p>The <strong>score-function identity</strong> states that for any distribution <span class="math inline">\(q_\phi(\mathbf{w})\)</span> parameterized by <span class="math inline">\(\phi\)</span>: <span class="math display">\[\nabla_\phi \mathbb{E}_{q_\phi} [f(\mathbf{w})] = \mathbb{E}_{q_\phi} [f(\mathbf{w}) \nabla_\phi \log q_\phi(\mathbf{w})]\]</span></p>
<p>where in our case, <span class="math inline">\(f(\mathbf{w}) = \log p(Y|X, \mathbf{w}) + \log p(\mathbf{w}) - \log q_\phi(\mathbf{w}) = \log p(Y,\mathbf{w}|X) - \log q_\phi(\mathbf{w})\)</span>. (This allows us to compute gradients of the ELBO with respect to <span class="math inline">\(\phi\)</span> using samples from <span class="math inline">\(q_\phi\)</span>.)</p>
<p>This score-function identity is also sometimes called the “REINFORCE” gradient estimator in the reinforcement learning literature, as it was first popularized there and uses something called the “log-derivative trick”<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> to move the gradient operator inside the expectation: <span class="math display">\[
\begin{align*}
\nabla_\phi \mathbb{E}_{q_\phi} [f(\mathbf{w})] &amp;= \nabla_\phi \int q_\phi(\mathbf{w}) f(\mathbf{w}) d\mathbf{w}\\
&amp;= \int f(\mathbf{w}) \nabla_\phi q_\phi(\mathbf{w}) d\mathbf{w} \\
&amp;= \int f(\mathbf{w}) q_\phi(\mathbf{w}) \nabla_\phi \log q_\phi(\mathbf{w}) d\mathbf{w} \\
&amp;= \mathbb{E}_{q_\phi} [f(\mathbf{w}) \nabla_\phi \log q_\phi(\mathbf{w})]\\
\end{align*}
\]</span></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Recall that <span class="math inline">\(\nabla_\phi \log(z) = \frac{\nabla_\phi z}{z}\)</span>.</p></div></div><p>So putting this all together for our toy example case here, we can estimate the gradient of the ELBO as: <span class="math display">\[
\begin{align*}
\nabla_\phi \mathcal{L}(\phi) &amp;= \nabla_\phi \mathbb{E}_{q_\phi}\left[\log p(Y,\mathbf{w}|X)\right]
- \nabla_\phi \mathbb{E}_{q_\phi}\left[\log q_\phi(\mathbf{w})\right]\\
&amp;= \mathbb{E}_{q_\phi}\left[\left(\log p(Y,\mathbf{w}|X) - \log q_\phi(\mathbf{w})\right) \nabla_\phi \log q_\phi(\mathbf{w})\right]\\
&amp;\approx \frac{1}{S} \sum_{s=1}^S \left(\log p(Y,\mathbf{w}^{(s)}|X) - \log q_\phi(\mathbf{w}^{(s)})\right) \nabla_\phi \log q_\phi(\mathbf{w}^{(s)}) \quad \text{where } \mathbf{w}^{(s)} \sim q_\phi(\mathbf{w})
\end{align*}
\]</span></p>
<p>So we can see that this will break down into the following steps:</p>
<ol type="1">
<li>Sample <span class="math inline">\(S\)</span> weight vectors <span class="math inline">\(\mathbf{w}^{(s)}\)</span> from the variational distribution <span class="math inline">\(q_\phi(\mathbf{w})\)</span></li>
<li>For each sample, compute the log joint probability <span class="math inline">\(\log p(Y,\mathbf{w}^{(s)}|X)\)</span> and the log variational probability <span class="math inline">\(\log q_\phi(\mathbf{w}^{(s)})\)</span>. We subtract these to get what is commonly called the “advantage” term, though we can see that it is just the difference between how well the sample explains the data versus how probable it is under the variational distribution, which is also akin to something called the “likelihood ratio”.</li>
<li>Compute the score function <span class="math inline">\(\nabla_\phi \log q_\phi(\mathbf{w}^{(s)})\)</span> for each sample, which in this case is intuitively the direction (in the model weights) that will increase the log likelihood of the variational distribution.</li>
<li>We multiply these gradients by the advantage term of that sample from step 2. Intuitively, what this is doing is weighting the gradient direction by how much better that sample explains the data versus how probable it is under the variational distribution. If a sample explains the data well but is improbable under <span class="math inline">\(q_\phi\)</span>, we want to move <span class="math inline">\(q_\phi\)</span> in that direction to increase its probability.</li>
<li>Average these gradients over all <span class="math inline">\(S\)</span> samples to get an estimate of <span class="math inline">\(\nabla_\phi \mathcal{L}(\phi)\)</span></li>
</ol>
<div id="e249669c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo VI using score-function gradients for a diagonal Gaussian</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> torch_log_p(z: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> z[:, <span class="dv">0</span>]</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> z[:, <span class="dv">1</span>]</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> <span class="fl">9.0</span> <span class="op">+</span> (y <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_q_diag(z: torch.Tensor, mu: torch.Tensor, log_sigma: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> z <span class="op">-</span> mu</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    inv_var <span class="op">=</span> torch.exp(<span class="op">-</span><span class="fl">2.0</span> <span class="op">*</span> log_sigma)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    quad <span class="op">=</span> torch.<span class="bu">sum</span>(diff<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> inv_var, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    log_det <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> torch.<span class="bu">sum</span>(log_sigma)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (quad <span class="op">+</span> log_det <span class="op">+</span> <span class="fl">2.0</span> <span class="op">*</span> LOG_2PI)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> <span class="dv">600</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.08</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>record_every <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>log_sigma <span class="op">=</span> torch.log(torch.ones(<span class="dv">2</span>) <span class="op">*</span> <span class="fl">2.5</span>)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>mc_history <span class="op">=</span> {</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"steps"</span>: [],</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: [],</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">"cov"</span>: [],</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"elbo"</span>: [],</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">"grad_var_mu"</span>: [],</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">"grad_var_log_sigma"</span>: []</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample from q(z; mu, sigma)</span></span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> torch.exp(log_sigma)</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> torch.randn(batch_size, <span class="dv">2</span>)</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>    z_samples <span class="op">=</span> (mu <span class="op">+</span> sigma <span class="op">*</span> eps).detach()</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute score-function gradients under samples from q</span></span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>    log_p <span class="op">=</span> torch_log_p(z_samples)</span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>    log_q <span class="op">=</span> log_q_diag(z_samples, mu, log_sigma)</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the ELBO gradient estimates (also called the advantage in RL))</span></span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>    advantage <span class="op">=</span> log_p <span class="op">-</span> log_q</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute score functions for q at the samples</span></span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>    score_mu <span class="op">=</span> (z_samples <span class="op">-</span> mu) <span class="op">/</span> (sigma<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a>    score_log_sigma <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">+</span> (z_samples <span class="op">-</span> mu) <span class="op">**</span> <span class="dv">2</span> <span class="op">/</span> (sigma<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Multiply the advantage with the score functions to get gradient estimates</span></span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>    grad_mu_samples <span class="op">=</span> advantage.unsqueeze(<span class="dv">1</span>) <span class="op">*</span> score_mu</span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>    grad_log_sigma_samples <span class="op">=</span> advantage.unsqueeze(<span class="dv">1</span>) <span class="op">*</span> score_log_sigma</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-55"><a href="#cb29-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Take the average over the MC samples</span></span>
<span id="cb29-56"><a href="#cb29-56" aria-hidden="true" tabindex="-1"></a>    grad_mu <span class="op">=</span> grad_mu_samples.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb29-57"><a href="#cb29-57" aria-hidden="true" tabindex="-1"></a>    grad_log_sigma <span class="op">=</span> grad_log_sigma_samples.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb29-58"><a href="#cb29-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-59"><a href="#cb29-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now we can do a simple gradient ascent step</span></span>
<span id="cb29-60"><a href="#cb29-60" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> mu <span class="op">+</span> lr <span class="op">*</span> grad_mu</span>
<span id="cb29-61"><a href="#cb29-61" aria-hidden="true" tabindex="-1"></a>    log_sigma <span class="op">=</span> log_sigma <span class="op">+</span> lr <span class="op">*</span> grad_log_sigma</span>
<span id="cb29-62"><a href="#cb29-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-63"><a href="#cb29-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> record_every <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> step <span class="op">==</span> steps <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb29-64"><a href="#cb29-64" aria-hidden="true" tabindex="-1"></a>        cov <span class="op">=</span> np.diag(torch.exp(<span class="fl">2.0</span> <span class="op">*</span> log_sigma).cpu().numpy())</span>
<span id="cb29-65"><a href="#cb29-65" aria-hidden="true" tabindex="-1"></a>        mc_history[<span class="st">"steps"</span>].append(step)</span>
<span id="cb29-66"><a href="#cb29-66" aria-hidden="true" tabindex="-1"></a>        mc_history[<span class="st">"mu"</span>].append(mu.cpu().numpy())</span>
<span id="cb29-67"><a href="#cb29-67" aria-hidden="true" tabindex="-1"></a>        mc_history[<span class="st">"cov"</span>].append(cov)</span>
<span id="cb29-68"><a href="#cb29-68" aria-hidden="true" tabindex="-1"></a>        mc_history[<span class="st">"elbo"</span>].append(advantage.mean().item())</span>
<span id="cb29-69"><a href="#cb29-69" aria-hidden="true" tabindex="-1"></a>        mc_history[<span class="st">"grad_var_mu"</span>].append(grad_mu_samples.var(dim<span class="op">=</span><span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">False</span>).mean().item())</span>
<span id="cb29-70"><a href="#cb29-70" aria-hidden="true" tabindex="-1"></a>        mc_history[<span class="st">"grad_var_log_sigma"</span>].append(grad_log_sigma_samples.var(dim<span class="op">=</span><span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">False</span>).mean().item())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="22048a46" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>mc_history[<span class="st">"steps"</span>] <span class="op">=</span> np.array(mc_history[<span class="st">"steps"</span>])</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>mc_history[<span class="st">"mu"</span>] <span class="op">=</span> np.stack(mc_history[<span class="st">"mu"</span>])</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>mc_history[<span class="st">"cov"</span>] <span class="op">=</span> np.stack(mc_history[<span class="st">"cov"</span>])</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>mc_history[<span class="st">"elbo"</span>] <span class="op">=</span> np.array(mc_history[<span class="st">"elbo"</span>])</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>mc_history[<span class="st">"grad_var_mu"</span>] <span class="op">=</span> np.array(mc_history[<span class="st">"grad_var_mu"</span>])</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>mc_history[<span class="st">"grad_var_log_sigma"</span>] <span class="op">=</span> np.array(mc_history[<span class="st">"grad_var_log_sigma"</span>])</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_mc_state(frame_idx: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    mu_frame <span class="op">=</span> mc_history[<span class="st">"mu"</span>][frame_idx]</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    cov_frame <span class="op">=</span> mc_history[<span class="st">"cov"</span>][frame_idx]</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>    step_val <span class="op">=</span> mc_history[<span class="st">"steps"</span>][frame_idx]</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>    elbo_val <span class="op">=</span> mc_history[<span class="st">"elbo"</span>][frame_idx]</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">13</span>, <span class="dv">5</span>))</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adapt variable names</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> mu_frame</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    history_steps <span class="op">=</span> mc_history[<span class="st">"steps"</span>]</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute VI density on the plotting grid (assumes X_grid, Y_grid are defined as numpy meshgrid)</span></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    mu_x, mu_y <span class="op">=</span> <span class="bu">float</span>(mu[<span class="dv">0</span>]), <span class="bu">float</span>(mu[<span class="dv">1</span>])</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    var_x <span class="op">=</span> <span class="bu">float</span>(cov_frame[<span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    var_y <span class="op">=</span> <span class="bu">float</span>(cov_frame[<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> np.pi <span class="op">*</span> np.sqrt(var_x <span class="op">*</span> var_y)</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>    exponent <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (((X_grid <span class="op">-</span> mu_x) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> var_x <span class="op">+</span> ((Y_grid <span class="op">-</span> mu_y) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> var_y)</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>    vi_density <span class="op">=</span> np.exp(exponent) <span class="op">/</span> denom</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>    ax.contour(X_grid, Y_grid, target_density_grid, levels<span class="op">=</span><span class="dv">14</span>, colors<span class="op">=</span><span class="st">'#b3b3b3'</span>, linewidths<span class="op">=</span><span class="fl">1.0</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>    contourf <span class="op">=</span> ax.contourf(X_grid, Y_grid, vi_density, levels<span class="op">=</span><span class="dv">14</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, alpha<span class="op">=</span><span class="fl">0.35</span>)</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>    ax.contour(X_grid, Y_grid, vi_density, levels<span class="op">=</span><span class="dv">14</span>, colors<span class="op">=</span><span class="st">'#1f77b4'</span>, linewidths<span class="op">=</span><span class="fl">1.1</span>, alpha<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    ax.scatter(mu[<span class="dv">0</span>], mu[<span class="dv">1</span>], color<span class="op">=</span><span class="st">'tab:blue'</span>, s<span class="op">=</span><span class="dv">40</span>, label<span class="op">=</span><span class="st">'VI mean'</span>)</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"MC VI (score-function) — step </span><span class="sc">{</span>history_steps[frame_idx]<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>    ax.set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>    ax.legend(loc<span class="op">=</span><span class="st">'upper right'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a>    ax.plot(mc_history[<span class="st">"steps"</span>], mc_history[<span class="st">"elbo"</span>], color<span class="op">=</span><span class="st">'tab:blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'ELBO estimate'</span>)</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>    ax.axvline(step_val, color<span class="op">=</span><span class="st">'tab:purple'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>    ax.scatter(step_val, elbo_val, color<span class="op">=</span><span class="st">'tab:purple'</span>, s<span class="op">=</span><span class="dv">40</span>)</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">'ELBO during MC VI optimisation'</span>)</span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'ELBO (MC estimate)'</span>)</span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a>    ax2 <span class="op">=</span> ax.twinx()</span>
<span id="cb30-50"><a href="#cb30-50" aria-hidden="true" tabindex="-1"></a>    ax2.plot(mc_history[<span class="st">"steps"</span>], mc_history[<span class="st">"grad_var_mu"</span>], color<span class="op">=</span><span class="st">'tab:red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.6</span>, label<span class="op">=</span><span class="st">'Var[∇μ]'</span>)</span>
<span id="cb30-51"><a href="#cb30-51" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">'Gradient variance (μ)'</span>)</span>
<span id="cb30-52"><a href="#cb30-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-53"><a href="#cb30-53" aria-hidden="true" tabindex="-1"></a>    lines, labels <span class="op">=</span> ax.get_legend_handles_labels()</span>
<span id="cb30-54"><a href="#cb30-54" aria-hidden="true" tabindex="-1"></a>    lines2, labels2 <span class="op">=</span> ax2.get_legend_handles_labels()</span>
<span id="cb30-55"><a href="#cb30-55" aria-hidden="true" tabindex="-1"></a>    ax.legend(lines <span class="op">+</span> lines2, labels <span class="op">+</span> labels2, loc<span class="op">=</span><span class="st">'lower right'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb30-56"><a href="#cb30-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-57"><a href="#cb30-57" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb30-58"><a href="#cb30-58" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb30-59"><a href="#cb30-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-60"><a href="#cb30-60" aria-hidden="true" tabindex="-1"></a>mc_slider <span class="op">=</span> widgets.IntSlider(value<span class="op">=</span><span class="bu">len</span>(mc_history[<span class="st">"steps"</span>]) <span class="op">-</span> <span class="dv">1</span>, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="bu">len</span>(mc_history[<span class="st">"steps"</span>]) <span class="op">-</span> <span class="dv">1</span>, step<span class="op">=</span><span class="dv">1</span>, description<span class="op">=</span><span class="st">'Frame'</span>)</span>
<span id="cb30-61"><a href="#cb30-61" aria-hidden="true" tabindex="-1"></a>mc_play <span class="op">=</span> widgets.Play(interval<span class="op">=</span><span class="dv">120</span>, value<span class="op">=</span><span class="dv">0</span>, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="bu">len</span>(mc_history[<span class="st">"steps"</span>]) <span class="op">-</span> <span class="dv">1</span>, step<span class="op">=</span><span class="dv">1</span>, description<span class="op">=</span><span class="st">'Press play'</span>)</span>
<span id="cb30-62"><a href="#cb30-62" aria-hidden="true" tabindex="-1"></a>widgets.jslink((mc_play, <span class="st">'value'</span>), (mc_slider, <span class="st">'value'</span>))</span>
<span id="cb30-63"><a href="#cb30-63" aria-hidden="true" tabindex="-1"></a>mc_controls <span class="op">=</span> widgets.HBox([mc_play, mc_slider])</span>
<span id="cb30-64"><a href="#cb30-64" aria-hidden="true" tabindex="-1"></a>mc_plot <span class="op">=</span> widgets.interactive_output(plot_mc_state, {<span class="st">'frame_idx'</span>: mc_slider})</span>
<span id="cb30-65"><a href="#cb30-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-66"><a href="#cb30-66" aria-hidden="true" tabindex="-1"></a>display(mc_controls, mc_plot)</span>
<span id="cb30-67"><a href="#cb30-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-68"><a href="#cb30-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final mean: </span><span class="sc">{</span>mc_history[<span class="st">'mu'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-69"><a href="#cb30-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final marginal std: </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(np.diag(mc_history[<span class="st">'cov'</span>][<span class="op">-</span><span class="dv">1</span>]))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-70"><a href="#cb30-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average gradient variance (μ): </span><span class="sc">{</span>mc_history[<span class="st">'grad_var_mu'</span>]<span class="sc">.</span>mean()<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"af99135558b542a2bb124c2608f9de6d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ce8cd5341c52437c8329c3afe6031d3f","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Final mean: [0.05686871 0.33599067]
Final marginal std: [1.9444891 1.0056936]
Average gradient variance (μ): 5.516</code></pre>
</div>
</div>
</section>
<section id="the-reparameterization-trick" class="level3" data-number="7.5.8">
<h3 data-number="7.5.8" class="anchored" data-anchor-id="the-reparameterization-trick"><span class="header-section-number">7.5.8</span> The Reparameterization Trick</h3>
<p>From the above we saw that we can now estimate VI updates even if we don’t have a closed form solution, provided that we can compute gradients w.r.t. the score function (essentially the log likelihood of <span class="math inline">\(q\)</span>), which we could in principle get through something like Automatic Differentiation. However, there is a problem with this: to compute the score function gradients, we need to differentiate through samples drawn from <span class="math inline">\(q_\phi(\mathbf{w})\)</span>. That is, our expectation is over samples <span class="math inline">\(\mathbf{w} \sim q_\phi(\mathbf{w})\)</span>, and the score function <span class="math inline">\(\nabla_\phi \log q_\phi(\mathbf{w})\)</span> depends on <span class="math inline">\(\phi\)</span> both directly (since <span class="math inline">\(q_\phi\)</span> is parameterized by <span class="math inline">\(\phi\)</span>) and indirectly through the samples <span class="math inline">\(\mathbf{w}\)</span>. This leads to stochasticity in the gradients due to the sampling process, which can result in high-variance gradient estimates.</p>
<p>To get around this, we can use something called the <strong>reparameterization trick</strong>, whose key idea is to move the stochasticity/randomness <strong>outside</strong> of the gradient computation. This is done by expressing samples from <span class="math inline">\(q_\phi(\mathbf{w})\)</span> as a deterministic function of <span class="math inline">\(\phi\)</span> and some noise variable <span class="math inline">\(\boldsymbol{\epsilon}\)</span> that is independent of <span class="math inline">\(\phi\)</span>. For example, if <span class="math inline">\(q_\phi(\mathbf{w})\)</span> is a Gaussian with mean <span class="math inline">\(\boldsymbol{\mu}_\phi\)</span> and standard deviation <span class="math inline">\(\boldsymbol{\sigma}_\phi\)</span>, we can write: <span class="math display">\[\mathbf{w} = \boldsymbol{\mu}_\phi + \boldsymbol{\sigma}_\phi \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, I)\]</span></p>
<p>Where the expectation is not over <span class="math inline">\(\mathbf{w} \sim q_\phi(\mathbf{w})\)</span> anymore, but rather over <span class="math inline">\(\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)\)</span>.</p>
<p>Note now that <span class="math inline">\(\mathbf{w}\)</span> is still a random variable, but the randomness comes solely from <span class="math inline">\(\boldsymbol{\epsilon}\)</span>, which is independent of <span class="math inline">\(\phi\)</span>. Instead <span class="math inline">\(\phi\)</span> becomes a deterministic transformation of <span class="math inline">\(\boldsymbol{\epsilon}\)</span>. This allows us to rewrite the ELBO expectation as: <span class="math display">\[
\begin{align*}
\nabla_\phi \mathcal{L}(\phi) &amp;= \mathbb{E}_{q_\phi}\left[\left(\log p(Y,\mathbf{w}|X) - \log q_\phi(\mathbf{w})\right) \nabla_\phi \log q_\phi(\mathbf{w})\right]\\
&amp;= \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)}\left[\left(\log p(Y,\mathbf{w}(\boldsymbol{\epsilon}, \phi)|X) - \log q_\phi(\mathbf{w}(\boldsymbol{\epsilon}, \phi))\right) \nabla_\phi \log q_\phi(\mathbf{w}(\boldsymbol{\epsilon}, \phi))\right]\\
&amp;\approx \frac{1}{S} \sum_{s=1}^S \left(\log p(Y,\mathbf{w}(\boldsymbol{\epsilon}^{(s)}, \phi)|X) - \log q_\phi(\mathbf{w}(\boldsymbol{\epsilon}^{(s)}, \phi))\right) \nabla_\phi \log q_\phi(\mathbf{w}(\boldsymbol{\epsilon}^{(s)}, \phi)) \quad \text{where } \boldsymbol{\epsilon}^{(s)} \sim \mathcal{N}(0, I)
\end{align*}
\]</span></p>
<p>This subtle change has a few important implications:</p>
<ol type="1">
<li>The randomness is now isolated in <span class="math inline">\(\boldsymbol{\epsilon}\)</span>, which does not depend on <span class="math inline">\(\phi\)</span>. This means that when we compute gradients w.r.t. <span class="math inline">\(\phi\)</span>, we can treat <span class="math inline">\(\boldsymbol{\epsilon}\)</span> as a constant. This reduces the variance of our gradient estimates significantly.</li>
<li>The gradients <span class="math inline">\(\nabla_\phi \log q_\phi(\mathbf{w}(\boldsymbol{\epsilon}, \phi))\)</span> can now be computed using standard automatic differentiation techniques, as <span class="math inline">\(\mathbf{w}\)</span> is a deterministic function of <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\boldsymbol{\epsilon}\)</span>.</li>
<li>This trick is particularly useful when <span class="math inline">\(q_\phi(\mathbf{w})\)</span> is a continuous distribution like a Gaussian, but it can be extended to non-continuous distributions as well, which we may cover in a later notebook.</li>
</ol>
<p>This approach is sometimes called the “pathwise derivative estimator” because it allows us to compute gradients by following a deterministic path through the parameter space, rather than relying on stochastic sampling, and sits in contrast to “score-function” estimators which rely on sampling directly from the distribution.</p>
<div id="b0c2c776" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reparameterized VI with automatic differentiation</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>mu_param <span class="op">=</span> torch.nn.Parameter(torch.tensor([<span class="fl">0.0</span>, <span class="fl">0.0</span>]))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>log_sigma_param <span class="op">=</span> torch.nn.Parameter(torch.log(torch.ones(<span class="dv">2</span>) <span class="op">*</span> <span class="fl">2.5</span>))</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> Adam([mu_param, log_sigma_param], lr<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> <span class="dv">600</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>record_every <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>reparam_history <span class="op">=</span> {</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"steps"</span>: [],</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: [],</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"cov"</span>: [],</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"elbo"</span>: [],</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"grad_var_mu"</span>: [],</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"grad_var_log_sigma"</span>: [],</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"grad_norm"</span>: []</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample from q(z; mu, sigma) using reparameterization</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> torch.exp(log_sigma_param)</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> torch.randn(batch_size, <span class="dv">2</span>)</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> mu_param <span class="op">+</span> sigma <span class="op">*</span> eps</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute ELBO using reparameterization trick</span></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>    log_p <span class="op">=</span> torch_log_p(z)</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>    log_q <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((eps<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="op">+</span> <span class="fl">2.0</span> <span class="op">*</span> torch.<span class="bu">sum</span>(log_sigma_param) <span class="op">+</span> <span class="fl">2.0</span> <span class="op">*</span> LOG_2PI)</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute ELBO (aka the advantage)</span></span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>    elbo <span class="op">=</span> (log_p <span class="op">-</span> log_q).mean()</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Call AD backward to compute gradients directly on the ELBO</span></span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>    (<span class="op">-</span>elbo).backward()</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>    grad_norm <span class="op">=</span> torch.sqrt(<span class="bu">sum</span>(param.grad.detach().<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>() <span class="cf">for</span> param <span class="kw">in</span> [mu_param, log_sigma_param])).item()</span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> record_every <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> step <span class="op">==</span> steps <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>            sigma_det <span class="op">=</span> torch.exp(log_sigma_param)</span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>            z_det <span class="op">=</span> (mu_param <span class="op">+</span> sigma_det <span class="op">*</span> eps).detach()</span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>            grad_log_p <span class="op">=</span> torch.stack([</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>                <span class="op">-</span>(z_det[:, <span class="dv">0</span>] <span class="op">/</span> <span class="fl">9.0</span>) <span class="op">+</span> <span class="fl">0.2</span> <span class="op">*</span> z_det[:, <span class="dv">0</span>] <span class="op">*</span> (z_det[:, <span class="dv">1</span>] <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> z_det[:, <span class="dv">0</span>] <span class="op">**</span> <span class="dv">2</span>),</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>                <span class="op">-</span>(z_det[:, <span class="dv">1</span>] <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> z_det[:, <span class="dv">0</span>] <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>            ], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a>            grad_mu_samples <span class="op">=</span> grad_log_p</span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>            grad_log_sigma_samples <span class="op">=</span> grad_log_p <span class="op">*</span> (sigma_det <span class="op">*</span> eps) <span class="op">+</span> <span class="fl">1.0</span></span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>            cov <span class="op">=</span> np.diag(torch.exp(<span class="fl">2.0</span> <span class="op">*</span> log_sigma_param).cpu().numpy())</span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a>            reparam_history[<span class="st">"steps"</span>].append(step)</span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>            reparam_history[<span class="st">"mu"</span>].append(mu_param.detach().cpu().numpy())</span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a>            reparam_history[<span class="st">"cov"</span>].append(cov)</span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>            reparam_history[<span class="st">"elbo"</span>].append(elbo.item())</span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>            reparam_history[<span class="st">"grad_var_mu"</span>].append(grad_mu_samples.var(dim<span class="op">=</span><span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">False</span>).mean().item())</span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>            reparam_history[<span class="st">"grad_var_log_sigma"</span>].append(grad_log_sigma_samples.var(dim<span class="op">=</span><span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">False</span>).mean().item())</span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a>            reparam_history[<span class="st">"grad_norm"</span>].append(grad_norm)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="071f83b9" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>reparam_history[<span class="st">"steps"</span>] <span class="op">=</span> np.array(reparam_history[<span class="st">"steps"</span>])</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>reparam_history[<span class="st">"mu"</span>] <span class="op">=</span> np.stack(reparam_history[<span class="st">"mu"</span>])</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>reparam_history[<span class="st">"cov"</span>] <span class="op">=</span> np.stack(reparam_history[<span class="st">"cov"</span>])</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>reparam_history[<span class="st">"elbo"</span>] <span class="op">=</span> np.array(reparam_history[<span class="st">"elbo"</span>])</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>reparam_history[<span class="st">"grad_var_mu"</span>] <span class="op">=</span> np.array(reparam_history[<span class="st">"grad_var_mu"</span>])</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>reparam_history[<span class="st">"grad_var_log_sigma"</span>] <span class="op">=</span> np.array(reparam_history[<span class="st">"grad_var_log_sigma"</span>])</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>reparam_history[<span class="st">"grad_norm"</span>] <span class="op">=</span> np.array(reparam_history[<span class="st">"grad_norm"</span>])</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_reparam_state(frame_idx: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    mu_frame <span class="op">=</span> reparam_history[<span class="st">"mu"</span>][frame_idx]</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    cov_frame <span class="op">=</span> reparam_history[<span class="st">"cov"</span>][frame_idx]</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    step_val <span class="op">=</span> reparam_history[<span class="st">"steps"</span>][frame_idx]</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    elbo_val <span class="op">=</span> reparam_history[<span class="st">"elbo"</span>][frame_idx]</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">13</span>, <span class="dv">5</span>))</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adapt variable names</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> mu_frame</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    history_steps <span class="op">=</span> reparam_history[<span class="st">"steps"</span>]</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute VI density on the plotting grid (assumes X_grid, Y_grid are defined as numpy meshgrid)</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    mu_x, mu_y <span class="op">=</span> <span class="bu">float</span>(mu[<span class="dv">0</span>]), <span class="bu">float</span>(mu[<span class="dv">1</span>])</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    var_x <span class="op">=</span> <span class="bu">float</span>(cov_frame[<span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    var_y <span class="op">=</span> <span class="bu">float</span>(cov_frame[<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> np.pi <span class="op">*</span> np.sqrt(var_x <span class="op">*</span> var_y)</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    exponent <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (((X_grid <span class="op">-</span> mu_x) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> var_x <span class="op">+</span> ((Y_grid <span class="op">-</span> mu_y) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> var_y)</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    vi_density <span class="op">=</span> np.exp(exponent) <span class="op">/</span> denom</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>    ax.contour(X_grid, Y_grid, target_density_grid, levels<span class="op">=</span><span class="dv">14</span>, colors<span class="op">=</span><span class="st">'#b3b3b3'</span>, linewidths<span class="op">=</span><span class="fl">1.0</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>    contourf <span class="op">=</span> ax.contourf(X_grid, Y_grid, vi_density, levels<span class="op">=</span><span class="dv">14</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, alpha<span class="op">=</span><span class="fl">0.35</span>)</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>    ax.contour(X_grid, Y_grid, vi_density, levels<span class="op">=</span><span class="dv">14</span>, colors<span class="op">=</span><span class="st">'#1f77b4'</span>, linewidths<span class="op">=</span><span class="fl">1.1</span>, alpha<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>    ax.scatter(mu[<span class="dv">0</span>], mu[<span class="dv">1</span>], color<span class="op">=</span><span class="st">'tab:blue'</span>, s<span class="op">=</span><span class="dv">40</span>, label<span class="op">=</span><span class="st">'VI mean'</span>)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"MC VI (score-function) — step </span><span class="sc">{</span>history_steps[frame_idx]<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>    ax.set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    ax.legend(loc<span class="op">=</span><span class="st">'upper right'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>    ax.plot(reparam_history[<span class="st">"steps"</span>], reparam_history[<span class="st">"elbo"</span>], color<span class="op">=</span><span class="st">'tab:blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'ELBO estimate'</span>)</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>    ax.axvline(step_val, color<span class="op">=</span><span class="st">'tab:purple'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>    ax.scatter(step_val, elbo_val, color<span class="op">=</span><span class="st">'tab:purple'</span>, s<span class="op">=</span><span class="dv">40</span>)</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">'ELBO during MC VI optimisation'</span>)</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'ELBO (MC estimate)'</span>)</span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a>    ax2 <span class="op">=</span> ax.twinx()</span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a>    ax2.plot(reparam_history[<span class="st">"steps"</span>], reparam_history[<span class="st">"grad_var_mu"</span>], color<span class="op">=</span><span class="st">'tab:red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.6</span>, label<span class="op">=</span><span class="st">'Var[∇μ]'</span>)</span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">'Gradient variance (μ)'</span>)</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a>    lines, labels <span class="op">=</span> ax.get_legend_handles_labels()</span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>    lines2, labels2 <span class="op">=</span> ax2.get_legend_handles_labels()</span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a>    ax.legend(lines <span class="op">+</span> lines2, labels <span class="op">+</span> labels2, loc<span class="op">=</span><span class="st">'lower right'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a>reparam_slider <span class="op">=</span> widgets.IntSlider(value<span class="op">=</span><span class="bu">len</span>(reparam_history[<span class="st">"steps"</span>]) <span class="op">-</span> <span class="dv">1</span>, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="bu">len</span>(reparam_history[<span class="st">"steps"</span>]) <span class="op">-</span> <span class="dv">1</span>, step<span class="op">=</span><span class="dv">1</span>, description<span class="op">=</span><span class="st">'Frame'</span>)</span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a>reparam_play <span class="op">=</span> widgets.Play(interval<span class="op">=</span><span class="dv">120</span>, value<span class="op">=</span><span class="dv">0</span>, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="bu">len</span>(reparam_history[<span class="st">"steps"</span>]) <span class="op">-</span> <span class="dv">1</span>, step<span class="op">=</span><span class="dv">1</span>, description<span class="op">=</span><span class="st">'Press play'</span>)</span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a>widgets.jslink((reparam_play, <span class="st">'value'</span>), (reparam_slider, <span class="st">'value'</span>))</span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a>reparam_controls <span class="op">=</span> widgets.HBox([reparam_play, reparam_slider])</span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a>reparam_plot <span class="op">=</span> widgets.interactive_output(plot_reparam_state, {<span class="st">'frame_idx'</span>: reparam_slider})</span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a>display(reparam_controls, reparam_plot)</span>
<span id="cb33-68"><a href="#cb33-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-69"><a href="#cb33-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final mean: </span><span class="sc">{</span>reparam_history[<span class="st">'mu'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-70"><a href="#cb33-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final marginal std: </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(np.diag(reparam_history[<span class="st">'cov'</span>][<span class="op">-</span><span class="dv">1</span>]))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-71"><a href="#cb33-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average gradient variance (μ): </span><span class="sc">{</span>reparam_history[<span class="st">'grad_var_mu'</span>]<span class="sc">.</span>mean()<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0375c680df124971aa6384df6c689d0e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2d37759cb7c341238a0d183e8ce32ea1","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Final mean: [0.03177835 0.3603806 ]
Final marginal std: [1.9253347 1.0171496]
Average gradient variance (μ): 1.408</code></pre>
</div>
</div>
</section>
<section id="black-box-variational-inference" class="level3 page-columns page-full" data-number="7.5.9">
<h3 data-number="7.5.9" class="anchored" data-anchor-id="black-box-variational-inference"><span class="header-section-number">7.5.9</span> Black-Box Variational Inference</h3>
<p>We saw above how to use the reparameterization trick to help reduce variance in estimating the VI update in cases where <span class="math inline">\(q_\phi(\mathbf{w})\)</span> is a continuous distribution, such as a Gaussian or even a simpler feedforward Neural Network. In many cases (like we will see next in VAEs) this is sufficient and we are good to go.</p>
<p>However, there are certain cases where basic reparameterization will not work. For example, if I pick a variational family <span class="math inline">\(q_\phi(\mathbf{w})\)</span> that is no longer a nice smooth function of the parameters <span class="math inline">\(w\)</span>, we will not be able to reparameterize <span class="math inline">\(q_\phi(\mathbf{w})\)</span> into a smoot deterministic function. This is common in cases where the desired <span class="math inline">\(q_\phi(\mathbf{w})\)</span> has discrete variables, such as clusters or categories.</p>
<p>In such cases, we actually have three options:</p>
<ol type="1">
<li>We can go back to score estimators like we did earlier in the notebook. We will suffer from higher variance updates but it will at least allow us to train the model even under discrete or non-continuous samples.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> This is the approach we will take below.</li>
<li>We can essentially try to “relax” discontinuities by finding a distribution that can approximate the “hard” or discrete objects we wish to study by variants that can be differentiated through. The price we will pay for this is that the optimal solution to the relaxed version of the model will be slightly biased away from the optimal solution to the original hard problem, but often this approximation error and bias is acceptable. Classical examples of these distributions include the Gumbel-Softmax or Concrete Distribution for relaxing Categorical variables, or the Birkhoff Polytope for relaxing Permutation Matrices. These extensions go beyond what I want to cover in this book, but are fascinating topics to read more about.</li>
<li>There are also classes of “Straight-Through Estimators” which essentially detach certain gradients from the computational graph such that one can sample hard/discrete samples, but then access non-hard gradients when calling automatic differentiation. These inherit similar problems to the relaxed distributions mentioned above, which is that the gradients you compute using Straight-Through estimators are only approximating, in a heuristic sense, the true gradients.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;There are some tricks to reduce the variance by subtracting a moving “baseline” average to the advantage term, which is implemented below, but we will not go into the details of this here.</p></div></div><p>We will demonstrate an example below where reparameterization will not be effective by fitting a three member Gaussian Mixture Model as our <span class="math inline">\(q_\phi(\mathbf{w})\)</span> and resorting to score-function estimators. Note that this below example takes a significantly longer time to fit/train than the examples we have used so far.</p>
<div id="f15172b1" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Black-box VI with a Gaussian mixture and REINFORCE gradients</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> scipy.stats <span class="im">import</span> gaussian_kde</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">ImportError</span>:</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    gaussian_kde <span class="op">=</span> <span class="va">None</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_mixture_prob(z: torch.Tensor, locs: torch.Tensor, log_scales: torch.Tensor, logits: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> z.unsqueeze(<span class="dv">1</span>) <span class="op">-</span> locs.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    inv_var <span class="op">=</span> torch.exp(<span class="op">-</span><span class="fl">2.0</span> <span class="op">*</span> log_scales).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    quad <span class="op">=</span> torch.<span class="bu">sum</span>(diff<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> inv_var, dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    log_det <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> log_scales.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    log_weights <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    component_log_prob <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (quad <span class="op">+</span> log_det.unsqueeze(<span class="dv">0</span>) <span class="op">+</span> <span class="fl">2.0</span> <span class="op">*</span> LOG_2PI)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.logsumexp(log_weights.unsqueeze(<span class="dv">0</span>) <span class="op">+</span> component_log_prob, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_mixture(locs: torch.Tensor, log_scales: torch.Tensor, logits: torch.Tensor, num_samples: <span class="bu">int</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>    cat <span class="op">=</span> torch.distributions.Categorical(probs<span class="op">=</span>weights)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> cat.sample((num_samples,))</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> torch.exp(log_scales)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> torch.randn(num_samples, <span class="dv">2</span>)</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> locs[indices] <span class="op">+</span> sigma[indices] <span class="op">*</span> eps</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples.detach()</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> radial_density_estimate(samples: np.ndarray, X: np.ndarray, Y: np.ndarray, bandwidth: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.7</span>, max_centers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">240</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>    rng_local <span class="op">=</span> np.random.default_rng(<span class="dv">0</span>)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(samples) <span class="op">&gt;</span> max_centers:</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> rng_local.choice(<span class="bu">len</span>(samples), max_centers, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>        centers <span class="op">=</span> samples[idx]</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>        centers <span class="op">=</span> samples</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>    diff_x <span class="op">=</span> X[<span class="va">None</span>, :, :] <span class="op">-</span> centers[:, <span class="dv">0</span>][:, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>    diff_y <span class="op">=</span> Y[<span class="va">None</span>, :, :] <span class="op">-</span> centers[:, <span class="dv">1</span>][:, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>    density <span class="op">=</span> np.exp(<span class="op">-</span>(diff_x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> diff_y<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="fl">2.0</span> <span class="op">*</span> bandwidth<span class="op">**</span><span class="dv">2</span>)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>    norm <span class="op">=</span> (<span class="fl">2.0</span> <span class="op">*</span> math.pi <span class="op">*</span> bandwidth<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> centers.shape[<span class="dv">0</span>]</span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> density <span class="op">/</span> norm</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>num_components <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.nn.Parameter(torch.zeros(num_components))</span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>locs <span class="op">=</span> torch.nn.Parameter(torch.tensor([[<span class="fl">6.0</span>, <span class="fl">6.0</span>], [<span class="fl">0.0</span>, <span class="fl">0.0</span>], [<span class="op">-</span><span class="fl">6.0</span>, <span class="fl">6.0</span>]]))</span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>log_scales <span class="op">=</span> torch.nn.Parameter(torch.log(torch.ones(num_components, <span class="dv">2</span>) <span class="op">*</span> <span class="fl">2.5</span>))</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> Adam([logits, locs, log_scales], lr<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> <span class="dv">250</span></span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>eval_samples <span class="op">=</span> <span class="dv">6000</span></span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a>record_every <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a>baseline <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a>baseline_beta <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a>bbvi_history <span class="op">=</span> {</span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">"steps"</span>: [],</span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">"elbo"</span>: [],</span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">"grad_norm"</span>: [],</span>
<span id="cb35-55"><a href="#cb35-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">"baseline"</span>: [],</span>
<span id="cb35-56"><a href="#cb35-56" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mean"</span>: [],</span>
<span id="cb35-57"><a href="#cb35-57" aria-hidden="true" tabindex="-1"></a>    <span class="st">"cov"</span>: [],</span>
<span id="cb35-58"><a href="#cb35-58" aria-hidden="true" tabindex="-1"></a>    <span class="st">"density"</span>: []</span>
<span id="cb35-59"><a href="#cb35-59" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb35-60"><a href="#cb35-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-61"><a href="#cb35-61" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb35-62"><a href="#cb35-62" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb35-63"><a href="#cb35-63" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> torch.exp(log_scales)</span>
<span id="cb35-64"><a href="#cb35-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-65"><a href="#cb35-65" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-66"><a href="#cb35-66" aria-hidden="true" tabindex="-1"></a>    cat <span class="op">=</span> torch.distributions.Categorical(probs<span class="op">=</span>weights)</span>
<span id="cb35-67"><a href="#cb35-67" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> cat.sample((batch_size,))</span>
<span id="cb35-68"><a href="#cb35-68" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> torch.randn(batch_size, <span class="dv">2</span>)</span>
<span id="cb35-69"><a href="#cb35-69" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> (locs[indices] <span class="op">+</span> sigma[indices] <span class="op">*</span> eps).detach()</span>
<span id="cb35-70"><a href="#cb35-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-71"><a href="#cb35-71" aria-hidden="true" tabindex="-1"></a>    log_q <span class="op">=</span> log_mixture_prob(samples, locs, log_scales, logits)</span>
<span id="cb35-72"><a href="#cb35-72" aria-hidden="true" tabindex="-1"></a>    log_p <span class="op">=</span> torch_log_p(samples)</span>
<span id="cb35-73"><a href="#cb35-73" aria-hidden="true" tabindex="-1"></a>    elbo_estimate <span class="op">=</span> (log_p <span class="op">-</span> log_q).mean().item()</span>
<span id="cb35-74"><a href="#cb35-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-75"><a href="#cb35-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb35-76"><a href="#cb35-76" aria-hidden="true" tabindex="-1"></a>        baseline <span class="op">=</span> elbo_estimate</span>
<span id="cb35-77"><a href="#cb35-77" aria-hidden="true" tabindex="-1"></a>    baseline <span class="op">=</span> (<span class="fl">1.0</span> <span class="op">-</span> baseline_beta) <span class="op">*</span> baseline <span class="op">+</span> baseline_beta <span class="op">*</span> elbo_estimate</span>
<span id="cb35-78"><a href="#cb35-78" aria-hidden="true" tabindex="-1"></a>    advantage <span class="op">=</span> (log_p <span class="op">-</span> log_q <span class="op">-</span> baseline).detach()</span>
<span id="cb35-79"><a href="#cb35-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-80"><a href="#cb35-80" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>(advantage <span class="op">*</span> log_q).mean()</span>
<span id="cb35-81"><a href="#cb35-81" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb35-82"><a href="#cb35-82" aria-hidden="true" tabindex="-1"></a>    grad_norm <span class="op">=</span> torch.sqrt(<span class="bu">sum</span>(param.grad.detach().<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>() <span class="cf">for</span> param <span class="kw">in</span> [logits, locs, log_scales])).item()</span>
<span id="cb35-83"><a href="#cb35-83" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb35-84"><a href="#cb35-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-85"><a href="#cb35-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> record_every <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> step <span class="op">==</span> steps <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb35-86"><a href="#cb35-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb35-87"><a href="#cb35-87" aria-hidden="true" tabindex="-1"></a>            eval_samples_tensor <span class="op">=</span> sample_mixture(locs, log_scales, logits, eval_samples)</span>
<span id="cb35-88"><a href="#cb35-88" aria-hidden="true" tabindex="-1"></a>            eval_np <span class="op">=</span> eval_samples_tensor.cpu().numpy()</span>
<span id="cb35-89"><a href="#cb35-89" aria-hidden="true" tabindex="-1"></a>            mean_np <span class="op">=</span> eval_np.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-90"><a href="#cb35-90" aria-hidden="true" tabindex="-1"></a>            cov_np <span class="op">=</span> np.cov(eval_np, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb35-91"><a href="#cb35-91" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> gaussian_kde <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb35-92"><a href="#cb35-92" aria-hidden="true" tabindex="-1"></a>                kde <span class="op">=</span> gaussian_kde(eval_np.T)</span>
<span id="cb35-93"><a href="#cb35-93" aria-hidden="true" tabindex="-1"></a>                density <span class="op">=</span> kde(np.vstack([X_grid.ravel(), Y_grid.ravel()])).reshape(X_grid.shape)</span>
<span id="cb35-94"><a href="#cb35-94" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb35-95"><a href="#cb35-95" aria-hidden="true" tabindex="-1"></a>                density <span class="op">=</span> radial_density_estimate(eval_np, X_grid, Y_grid)</span>
<span id="cb35-96"><a href="#cb35-96" aria-hidden="true" tabindex="-1"></a>        bbvi_history[<span class="st">"steps"</span>].append(step)</span>
<span id="cb35-97"><a href="#cb35-97" aria-hidden="true" tabindex="-1"></a>        bbvi_history[<span class="st">"elbo"</span>].append(elbo_estimate)</span>
<span id="cb35-98"><a href="#cb35-98" aria-hidden="true" tabindex="-1"></a>        bbvi_history[<span class="st">"grad_norm"</span>].append(grad_norm)</span>
<span id="cb35-99"><a href="#cb35-99" aria-hidden="true" tabindex="-1"></a>        bbvi_history[<span class="st">"baseline"</span>].append(baseline)</span>
<span id="cb35-100"><a href="#cb35-100" aria-hidden="true" tabindex="-1"></a>        bbvi_history[<span class="st">"mean"</span>].append(mean_np)</span>
<span id="cb35-101"><a href="#cb35-101" aria-hidden="true" tabindex="-1"></a>        bbvi_history[<span class="st">"cov"</span>].append(cov_np)</span>
<span id="cb35-102"><a href="#cb35-102" aria-hidden="true" tabindex="-1"></a>        bbvi_history[<span class="st">"density"</span>].append(density)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="566cfaf5" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>bbvi_history[<span class="st">"steps"</span>] <span class="op">=</span> np.array(bbvi_history[<span class="st">"steps"</span>])</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>bbvi_history[<span class="st">"elbo"</span>] <span class="op">=</span> np.array(bbvi_history[<span class="st">"elbo"</span>])</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>bbvi_history[<span class="st">"grad_norm"</span>] <span class="op">=</span> np.array(bbvi_history[<span class="st">"grad_norm"</span>])</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>bbvi_history[<span class="st">"baseline"</span>] <span class="op">=</span> np.array(bbvi_history[<span class="st">"baseline"</span>])</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>bbvi_history[<span class="st">"mean"</span>] <span class="op">=</span> np.stack(bbvi_history[<span class="st">"mean"</span>])</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>bbvi_history[<span class="st">"cov"</span>] <span class="op">=</span> np.stack(bbvi_history[<span class="st">"cov"</span>])</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>bbvi_history[<span class="st">"density"</span>] <span class="op">=</span> np.stack(bbvi_history[<span class="st">"density"</span>])</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>final_weights <span class="op">=</span> F.softmax(logits.detach(), dim<span class="op">=</span><span class="dv">0</span>).cpu().numpy()</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>final_locs <span class="op">=</span> locs.detach().cpu().numpy()</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">13</span>, <span class="dv">5</span>))</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>ax.contour(X_grid, Y_grid, target_density_grid, levels<span class="op">=</span><span class="dv">14</span>, colors<span class="op">=</span><span class="st">'#b3b3b3'</span>, linewidths<span class="op">=</span><span class="fl">1.0</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>ax.contourf(X_grid, Y_grid, bbvi_history[<span class="st">"density"</span>][<span class="op">-</span><span class="dv">1</span>], levels<span class="op">=</span><span class="dv">14</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, alpha<span class="op">=</span><span class="fl">0.35</span>)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>ax.contour(X_grid, Y_grid, bbvi_history[<span class="st">"density"</span>][<span class="op">-</span><span class="dv">1</span>], levels<span class="op">=</span><span class="dv">14</span>, colors<span class="op">=</span><span class="st">'#1f77b4'</span>, linewidths<span class="op">=</span><span class="fl">1.1</span>, alpha<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>scatter_sizes <span class="op">=</span> <span class="dv">300</span> <span class="op">*</span> final_weights</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>ax.scatter(final_locs[:, <span class="dv">0</span>], final_locs[:, <span class="dv">1</span>], s<span class="op">=</span>scatter_sizes, c<span class="op">=</span><span class="st">'tab:orange'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, linewidths<span class="op">=</span><span class="fl">1.0</span>, label<span class="op">=</span><span class="st">'Mixture means'</span>)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Gaussian mixture $q_{</span><span class="er">\</span><span class="st">phi}$ (final snapshot)'</span>)</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'upper right'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>ax.plot(bbvi_history[<span class="st">"steps"</span>], bbvi_history[<span class="st">"elbo"</span>], color<span class="op">=</span><span class="st">'tab:blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'ELBO estimate'</span>)</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Score-function optimisation trace'</span>)</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'ELBO'</span>)</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> ax.twinx()</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>ax2.plot(bbvi_history[<span class="st">"steps"</span>], bbvi_history[<span class="st">"grad_norm"</span>], color<span class="op">=</span><span class="st">'tab:red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.6</span>, label<span class="op">=</span><span class="st">'Gradient norm'</span>)</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Gradient norm'</span>)</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>lines, labels <span class="op">=</span> ax.get_legend_handles_labels()</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>lines2, labels2 <span class="op">=</span> ax2.get_legend_handles_labels()</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>ax.legend(lines <span class="op">+</span> lines2, labels <span class="op">+</span> labels2, loc<span class="op">=</span><span class="st">'lower right'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final mixture weights: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(final_weights, <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final mixture means: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(final_locs, <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average ELBO estimate: </span><span class="sc">{</span>bbvi_history[<span class="st">'elbo'</span>]<span class="sc">.</span>mean()<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Final mixture weights: [0.043 0.906 0.051]
Final mixture means: [[ 5.449  3.545]
 [ 0.021  0.365]
 [-5.234  3.275]]
Average ELBO estimate: 2.372</code></pre>
</div>
</div>
<p>With this you now have a high-level overview of some of this major approaches to performing Variational Inference as well as their pros and cons. We can now move on to applying this to a more complex example of the Neural Network from earlier.</p>
</section>
<section id="example-of-using-vi-for-neural-networks" class="level3" data-number="7.5.10">
<h3 data-number="7.5.10" class="anchored" data-anchor-id="example-of-using-vi-for-neural-networks"><span class="header-section-number">7.5.10</span> Example of using VI for Neural Networks</h3>
<p>Since in this case the Neural Network is a continuous and differentiable function, we will leverage the reparameterization trick to allow us to automatically differentiate backwards through the network to perform the VI updates.</p>
<div id="86d17669" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variational Inference for Bayesian Neural Network</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BayesianNNVI(nn.Module):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Bayesian Neural Network with Variational Inference.</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Each weight has a variational posterior q(w) = N(mu, sigma^2).</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_size<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Variational parameters for layer 1: input -&gt; hidden</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1_mu_weight <span class="op">=</span> nn.Parameter(torch.randn(hidden_size, <span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1_mu_bias <span class="op">=</span> nn.Parameter(torch.randn(hidden_size) <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1_logsigma_weight <span class="op">=</span> nn.Parameter(torch.ones(hidden_size, <span class="dv">1</span>) <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>)  <span class="co"># Start with low variance</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1_logsigma_bias <span class="op">=</span> nn.Parameter(torch.ones(hidden_size) <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Variational parameters for layer 2: hidden -&gt; output</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2_mu_weight <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, hidden_size) <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2_mu_bias <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2_logsigma_weight <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>, hidden_size) <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2_logsigma_bias <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>) <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_weights(<span class="va">self</span>):</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sample weights using reparameterization trick."""</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer 1</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>        fc1_weight <span class="op">=</span> <span class="va">self</span>.fc1_mu_weight <span class="op">+</span> torch.exp(<span class="va">self</span>.fc1_logsigma_weight) <span class="op">*</span> torch.randn_like(<span class="va">self</span>.fc1_mu_weight)</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>        fc1_bias <span class="op">=</span> <span class="va">self</span>.fc1_mu_bias <span class="op">+</span> torch.exp(<span class="va">self</span>.fc1_logsigma_bias) <span class="op">*</span> torch.randn_like(<span class="va">self</span>.fc1_mu_bias)</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer 2</span></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>        fc2_weight <span class="op">=</span> <span class="va">self</span>.fc2_mu_weight <span class="op">+</span> torch.exp(<span class="va">self</span>.fc2_logsigma_weight) <span class="op">*</span> torch.randn_like(<span class="va">self</span>.fc2_mu_weight)</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>        fc2_bias <span class="op">=</span> <span class="va">self</span>.fc2_mu_bias <span class="op">+</span> torch.exp(<span class="va">self</span>.fc2_logsigma_bias) <span class="op">*</span> torch.randn_like(<span class="va">self</span>.fc2_mu_bias)</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (fc1_weight, fc1_bias, fc2_weight, fc2_bias)</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, sample<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Forward pass with sampled or mean weights."""</span></span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> sample:</span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a>            fc1_w, fc1_b, fc2_w, fc2_b <span class="op">=</span> <span class="va">self</span>.sample_weights()</span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a>            fc1_w, fc1_b <span class="op">=</span> <span class="va">self</span>.fc1_mu_weight, <span class="va">self</span>.fc1_mu_bias</span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a>            fc2_w, fc2_b <span class="op">=</span> <span class="va">self</span>.fc2_mu_weight, <span class="va">self</span>.fc2_mu_bias</span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward propagation</span></span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> F.relu(F.linear(x, fc1_w, fc1_b))</span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> F.linear(h, fc2_w, fc2_b)</span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y.squeeze()</span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> kl_divergence(<span class="va">self</span>, prior_std<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute KL(q||p) where p is N(0, prior_std^2)."""</span></span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true" tabindex="-1"></a>        kl <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb38-56"><a href="#cb38-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-57"><a href="#cb38-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># KL for each parameter: KL(N(mu, sigma^2) || N(0, prior_std^2))</span></span>
<span id="cb38-58"><a href="#cb38-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> mu_param, logsigma_param <span class="kw">in</span> [</span>
<span id="cb38-59"><a href="#cb38-59" aria-hidden="true" tabindex="-1"></a>            (<span class="va">self</span>.fc1_mu_weight, <span class="va">self</span>.fc1_logsigma_weight),</span>
<span id="cb38-60"><a href="#cb38-60" aria-hidden="true" tabindex="-1"></a>            (<span class="va">self</span>.fc1_mu_bias, <span class="va">self</span>.fc1_logsigma_bias),</span>
<span id="cb38-61"><a href="#cb38-61" aria-hidden="true" tabindex="-1"></a>            (<span class="va">self</span>.fc2_mu_weight, <span class="va">self</span>.fc2_logsigma_weight),</span>
<span id="cb38-62"><a href="#cb38-62" aria-hidden="true" tabindex="-1"></a>            (<span class="va">self</span>.fc2_mu_bias, <span class="va">self</span>.fc2_logsigma_bias)</span>
<span id="cb38-63"><a href="#cb38-63" aria-hidden="true" tabindex="-1"></a>        ]:</span>
<span id="cb38-64"><a href="#cb38-64" aria-hidden="true" tabindex="-1"></a>            sigma <span class="op">=</span> torch.exp(logsigma_param)</span>
<span id="cb38-65"><a href="#cb38-65" aria-hidden="true" tabindex="-1"></a>            kl <span class="op">+=</span> <span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(</span>
<span id="cb38-66"><a href="#cb38-66" aria-hidden="true" tabindex="-1"></a>                (mu_param<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> sigma<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> prior_std<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span> <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>logsigma_param <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>np.log(prior_std)</span>
<span id="cb38-67"><a href="#cb38-67" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb38-68"><a href="#cb38-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-69"><a href="#cb38-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> kl</span>
<span id="cb38-70"><a href="#cb38-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-71"><a href="#cb38-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> elbo(<span class="va">self</span>, x, y, n_samples<span class="op">=</span><span class="dv">5</span>, noise_std<span class="op">=</span><span class="fl">0.1</span>, prior_std<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb38-72"><a href="#cb38-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb38-73"><a href="#cb38-73" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the Evidence Lower Bound.</span></span>
<span id="cb38-74"><a href="#cb38-74" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb38-75"><a href="#cb38-75" aria-hidden="true" tabindex="-1"></a><span class="co">        ELBO = E_q[log p(y|x,w)] - KL(q||p)</span></span>
<span id="cb38-76"><a href="#cb38-76" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb38-77"><a href="#cb38-77" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb38-78"><a href="#cb38-78" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-79"><a href="#cb38-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Expected log-likelihood (Monte Carlo estimate)</span></span>
<span id="cb38-80"><a href="#cb38-80" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb38-81"><a href="#cb38-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb38-82"><a href="#cb38-82" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> <span class="va">self</span>.forward(x, sample<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-83"><a href="#cb38-83" aria-hidden="true" tabindex="-1"></a>            log_lik <span class="op">+=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(((y <span class="op">-</span> y_pred) <span class="op">/</span> noise_std) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb38-84"><a href="#cb38-84" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">=</span> log_lik <span class="op">/</span> n_samples</span>
<span id="cb38-85"><a href="#cb38-85" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">-=</span> <span class="bu">len</span>(y) <span class="op">*</span> np.log(noise_std <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb38-86"><a href="#cb38-86" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-87"><a href="#cb38-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># KL divergence</span></span>
<span id="cb38-88"><a href="#cb38-88" aria-hidden="true" tabindex="-1"></a>        kl <span class="op">=</span> <span class="va">self</span>.kl_divergence(prior_std)</span>
<span id="cb38-89"><a href="#cb38-89" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-90"><a href="#cb38-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ELBO</span></span>
<span id="cb38-91"><a href="#cb38-91" aria-hidden="true" tabindex="-1"></a>        elbo <span class="op">=</span> log_lik <span class="op">-</span> kl</span>
<span id="cb38-92"><a href="#cb38-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-93"><a href="#cb38-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> elbo, log_lik, kl</span>
<span id="cb38-94"><a href="#cb38-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-95"><a href="#cb38-95" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize variational model</span></span>
<span id="cb38-96"><a href="#cb38-96" aria-hidden="true" tabindex="-1"></a>vi_model <span class="op">=</span> BayesianNNVI(hidden_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb38-97"><a href="#cb38-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-98"><a href="#cb38-98" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment below if you would like a listing of all parameters and counts:</span></span>
<span id="cb38-99"><a href="#cb38-99" aria-hidden="true" tabindex="-1"></a><span class="co"># print("Variational parameters:")</span></span>
<span id="cb38-100"><a href="#cb38-100" aria-hidden="true" tabindex="-1"></a><span class="co"># for name, param in vi_model.named_parameters():</span></span>
<span id="cb38-101"><a href="#cb38-101" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(f"  {name}: shape {param.shape}")</span></span>
<span id="cb38-102"><a href="#cb38-102" aria-hidden="true" tabindex="-1"></a><span class="co">#     n_var_params = sum(p.numel() for p in vi_model.parameters())</span></span>
<span id="cb38-103"><a href="#cb38-103" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(f"Total variational parameters: {n_var_params}")</span></span>
<span id="cb38-104"><a href="#cb38-104" aria-hidden="true" tabindex="-1"></a><span class="co">#     print("(2x the number of network weights for mean + variance)")</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="867cb7ea" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train with Variational Inference</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>vi_model <span class="op">=</span> BayesianNNVI(hidden_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(vi_model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>x_train_tensor <span class="op">=</span> torch.FloatTensor(x_train)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>y_train_tensor <span class="op">=</span> torch.FloatTensor(y_train)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">3000</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>n_mc_samples <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Monte Carlo samples per ELBO evaluation</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> {</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'elbo'</span>: [],</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'log_lik'</span>: [],</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'kl'</span>: []</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training Bayesian NN with Variational Inference..."</span>)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Epochs: </span><span class="sc">{</span>n_epochs<span class="sc">}</span><span class="ss">, MC samples: </span><span class="sc">{</span>n_mc_samples<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute ELBO (negative for minimization)</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>    elbo, log_lik, kl <span class="op">=</span> vi_model.elbo(x_train_tensor, y_train_tensor, </span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>                                       n_samples<span class="op">=</span>n_mc_samples, </span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>                                       noise_std<span class="op">=</span>noise_std, </span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>                                       prior_std<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>elbo  <span class="co"># Maximize ELBO = minimize negative ELBO</span></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backprop and update</span></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Track metrics</span></span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'elbo'</span>].append(elbo.item())</span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'log_lik'</span>].append(log_lik.item())</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'kl'</span>].append(kl.item())</span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">500</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:4d}</span><span class="ss"> | ELBO: </span><span class="sc">{</span>elbo<span class="sc">.</span>item()<span class="sc">:8.2f}</span><span class="ss"> | "</span></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"Log-lik: </span><span class="sc">{</span>log_lik<span class="sc">.</span>item()<span class="sc">:8.2f}</span><span class="ss"> | KL: </span><span class="sc">{</span>kl<span class="sc">.</span>item()<span class="sc">:6.2f}</span><span class="ss">"</span>)</span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Training Bayesian NN with Variational Inference...
Epochs: 3000, MC samples: 5
Epoch  500 | ELBO:  -123.28 | Log-lik:   -86.77 | KL:  36.51
Epoch  500 | ELBO:  -123.28 | Log-lik:   -86.77 | KL:  36.51
Epoch 1000 | ELBO:  -113.35 | Log-lik:   -80.70 | KL:  32.65
Epoch 1000 | ELBO:  -113.35 | Log-lik:   -80.70 | KL:  32.65
Epoch 1500 | ELBO:   -35.02 | Log-lik:     7.50 | KL:  42.52
Epoch 1500 | ELBO:   -35.02 | Log-lik:     7.50 | KL:  42.52
Epoch 2000 | ELBO:   -32.42 | Log-lik:    12.57 | KL:  44.99
Epoch 2000 | ELBO:   -32.42 | Log-lik:    12.57 | KL:  44.99
Epoch 2500 | ELBO:   -25.91 | Log-lik:    16.31 | KL:  42.22
Epoch 2500 | ELBO:   -25.91 | Log-lik:    16.31 | KL:  42.22
Epoch 3000 | ELBO:   -27.61 | Log-lik:    14.09 | KL:  41.70
Training complete!
Epoch 3000 | ELBO:   -27.61 | Log-lik:    14.09 | KL:  41.70
Training complete!</code></pre>
</div>
</div>
<div id="630a6aa8" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize VI training progress</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>epochs_arr <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="bu">len</span>(history[<span class="st">'elbo'</span>]) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: ELBO over time</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>ax.plot(epochs_arr, history[<span class="st">'elbo'</span>], linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Epoch'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'ELBO'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'ELBO During VI Training'</span>, fontsize<span class="op">=</span><span class="dv">13</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: ELBO components</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>ax.plot(epochs_arr, history[<span class="st">'log_lik'</span>], linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Expected log-likelihood'</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>ax.plot(epochs_arr, history[<span class="st">'kl'</span>], linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'KL divergence'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Epoch'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Value'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'ELBO Components'</span>, fontsize<span class="op">=</span><span class="dv">13</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final ELBO breakdown:"</span>)</span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  ELBO: </span><span class="sc">{</span>history[<span class="st">'elbo'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Expected log-likelihood: </span><span class="sc">{</span>history[<span class="st">'log_lik'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  KL divergence: </span><span class="sc">{</span>history[<span class="st">'kl'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-29-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Final ELBO breakdown:
  ELBO: -27.61
  Expected log-likelihood: 14.09
  KL divergence: 41.70</code></pre>
</div>
</div>
<div id="88678f57" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate VI posterior predictive samples</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>x_test_tensor <span class="op">=</span> torch.FloatTensor(x_test)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>vi_predictions <span class="op">=</span> []</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> vi_model(x_test_tensor, sample<span class="op">=</span><span class="va">True</span>).numpy()</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        vi_predictions.append(y_pred)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>vi_predictions <span class="op">=</span> np.array(vi_predictions)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>vi_mean <span class="op">=</span> vi_predictions.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>vi_std <span class="op">=</span> vi_predictions.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare VI vs MCMC</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">6</span>))</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: VI Posterior Predictive</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a><span class="co"># VI samples</span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(vi_predictions)):</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, vi_predictions[i], <span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.15</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a><span class="co"># True function and data</span></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True function'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="dv">2</span>, zorder<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a><span class="co"># VI mean</span></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, vi_mean, <span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'VI posterior mean'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Dummy line for legend</span></span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>ax.plot([], [], <span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'VI posterior samples'</span>)</span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Variational Inference: Posterior Predictive'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Direct comparison VI vs MCMC</span></span>
<span id="cb43-44"><a href="#cb43-44" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb43-45"><a href="#cb43-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-46"><a href="#cb43-46" aria-hidden="true" tabindex="-1"></a><span class="co"># MCMC samples (lighter)</span></span>
<span id="cb43-47"><a href="#cb43-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">min</span>(<span class="dv">50</span>, <span class="bu">len</span>(mcmc_predictions))):</span>
<span id="cb43-48"><a href="#cb43-48" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, mcmc_predictions[i], <span class="st">'orange'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb43-49"><a href="#cb43-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-50"><a href="#cb43-50" aria-hidden="true" tabindex="-1"></a><span class="co"># VI samples (darker)</span></span>
<span id="cb43-51"><a href="#cb43-51" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb43-52"><a href="#cb43-52" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, vi_predictions[i], <span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb43-53"><a href="#cb43-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-54"><a href="#cb43-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Means</span></span>
<span id="cb43-55"><a href="#cb43-55" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, mcmc_mean, <span class="st">'orange'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'MCMC mean'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb43-56"><a href="#cb43-56" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, vi_mean, <span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'VI mean'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb43-57"><a href="#cb43-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-58"><a href="#cb43-58" aria-hidden="true" tabindex="-1"></a><span class="co"># True function and data</span></span>
<span id="cb43-59"><a href="#cb43-59" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True function'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb43-60"><a href="#cb43-60" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb43-61"><a href="#cb43-61" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="dv">2</span>, zorder<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb43-62"><a href="#cb43-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-63"><a href="#cb43-63" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb43-64"><a href="#cb43-64" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb43-65"><a href="#cb43-65" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Comparison: VI vs MCMC'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb43-66"><a href="#cb43-66" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb43-67"><a href="#cb43-67" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb43-68"><a href="#cb43-68" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb43-69"><a href="#cb43-69" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb43-70"><a href="#cb43-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-71"><a href="#cb43-71" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb43-72"><a href="#cb43-72" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Note the difference in training times between the MCMC Method and VI Method. The main thing we give up for this increase in speed is the</p>
</section>
</section>
<section id="possible-additional-experiments" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="possible-additional-experiments"><span class="header-section-number">7.6</span> Possible Additional Experiments</h2>
<p>Now that you understand the full pipeline, you can explore below how some of the fundamental parameters of the model might affect the results:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Experiment 1: Vary Hidden Layer Size
</div>
</div>
<div class="callout-body-container callout-body">
<p>Train VI models with <code>hidden_size</code> = 2, 5, 10, 20. Observe: - How does model flexibility change? - Does uncertainty increase or decrease? - Is there a risk of overfitting with larger networks?</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Experiment 2: Effect of Training Data Size
</div>
</div>
<div class="callout-body-container callout-body">
<p>Generate datasets with <code>n_train</code> = 5, 10, 20, 50. For each: - Train the VI model - Plot posterior predictive distributions - Compare the model uncertainty in data-rich vs data-sparse regions What do you notice?</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Experiment 3: Prior Sensitivity
</div>
</div>
<div class="callout-body-container callout-body">
<p>Try different prior standard deviations: <code>prior_std</code> = 0.1, 1.0, 10.0. Observe: - How does this affect the learned posterior? - Does a strong prior (small std) regularize more? - What happens with a weak prior (large std)?</p>
</div>
</div>
<p>We can do Experiment 1 together as an example, and leave the remainder for you to do independently:</p>
<div id="8fb27e53" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Experiment 1: Vary hidden layer size</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>hidden_sizes_to_test <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>]</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>results_by_size <span class="op">=</span> {}</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Experiment 1: Effect of Hidden Layer Size"</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training VI models with different architectures..."</span>)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> h_size <span class="kw">in</span> hidden_sizes_to_test:</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Training with hidden_size = </span><span class="sc">{</span>h_size<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize and train</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    model_exp <span class="op">=</span> BayesianNNVI(hidden_size<span class="op">=</span>h_size)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    optimizer_exp <span class="op">=</span> torch.optim.Adam(model_exp.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>        optimizer_exp.zero_grad()</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>        elbo, _, _ <span class="op">=</span> model_exp.elbo(x_train_tensor, y_train_tensor, </span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>                                     n_samples<span class="op">=</span><span class="dv">5</span>, noise_std<span class="op">=</span>noise_std, prior_std<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>elbo</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>        optimizer_exp.step()</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate predictions</span></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model_exp(x_test_tensor, sample<span class="op">=</span><span class="va">True</span>).numpy()</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>            predictions.append(y_pred)</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> np.array(predictions)</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>    results_by_size[h_size] <span class="op">=</span> {</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">'predictions'</span>: predictions,</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>        <span class="st">'mean'</span>: predictions.mean(axis<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>        <span class="st">'std'</span>: predictions.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Final ELBO: </span><span class="sc">{</span>elbo<span class="sc">.</span>item()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Experiment 1: Effect of Hidden Layer Size
Training VI models with different architectures...
Training with hidden_size = 2...
  Final ELBO: -21.87
Training with hidden_size = 5...
  Final ELBO: -21.87
Training with hidden_size = 5...
  Final ELBO: -30.33
Training with hidden_size = 10...
  Final ELBO: -30.33
Training with hidden_size = 10...
  Final ELBO: -45.41
Training with hidden_size = 20...
  Final ELBO: -45.41
Training with hidden_size = 20...
  Final ELBO: -82.38
Training complete!
  Final ELBO: -82.38
Training complete!</code></pre>
</div>
</div>
<div id="95dd762f" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize results from Experiment 1</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">12</span>))</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, h_size <span class="kw">in</span> <span class="bu">enumerate</span>(hidden_sizes_to_test):</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[idx]</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> results_by_size[h_size]</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot posterior samples</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">min</span>(<span class="dv">50</span>, <span class="bu">len</span>(result[<span class="st">'predictions'</span>]))):</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>        ax.plot(x_test, result[<span class="st">'predictions'</span>][i], <span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot mean and uncertainty bands</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, result[<span class="st">'mean'</span>], <span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'VI posterior mean'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>    ax.fill_between(x_test, </span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>                     result[<span class="st">'mean'</span>] <span class="op">-</span> result[<span class="st">'std'</span>], </span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>                     result[<span class="st">'mean'</span>] <span class="op">+</span> result[<span class="st">'std'</span>],</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>                     alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'±1sigma'</span>)</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># True function and data</span></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True function'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>    ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">80</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>               linewidth<span class="op">=</span><span class="fl">1.5</span>, zorder<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f'Hidden Size = </span><span class="sc">{</span>h_size<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>(h_size<span class="op">+</span><span class="dv">1</span>) <span class="op">+</span> h_size<span class="sc">}</span><span class="ss"> parameters)'</span>, </span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>                 fontsize<span class="op">=</span><span class="dv">13</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>    ax.legend(fontsize<span class="op">=</span><span class="dv">10</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">'Experiment 1: Effect of Network Capacity'</span>, fontsize<span class="op">=</span><span class="dv">15</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, y<span class="op">=</span><span class="fl">0.995</span>)</span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-32-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="summary-and-the-bridge-to-vaes" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="summary-and-the-bridge-to-vaes"><span class="header-section-number">7.7</span> Summary and the bridge to VAEs</h2>
<p>We saw a few key takeaways in this notebook:</p>
<p><strong>Bayesian Inference Quantifies Uncertainty</strong> - Instead of a single “best” model, we maintain a distribution over models - The <strong>posterior predictive distribution</strong> <span class="math inline">\(p(y_* | x_*, X, Y)\)</span> gives us predictions <strong>with uncertainty</strong></p>
<p><strong>Linear Models Have Limitations</strong> - <strong>Bayesian linear regression</strong> has exact, closed-form posteriors (Gaussian) - This allows us to compute predictive mean and variance analytically - However, they cannot capture nonlinear relationships, as we saw in this example</p>
<p><strong>Neural Networks Provide Flexibility at the cost of an intractable posterior</strong> - Even a small 1-hidden-layer network can approximate complex functions - <strong>Prior predictive</strong> shows diverse possible functions before seeing data - <strong>Challenge</strong>: Posterior <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span> is no longer tractable.</p>
<p><strong>MCMC: Accurate but Slow</strong> - Markov Chain Monte Carlo samples from the true posterior - <strong>Pros</strong>: Asymptotically exact, unbiased - <strong>Cons</strong>: Computationally expensive, slow for high-dimensional models - Useful as a <strong>reference</strong> for validation</p>
<p><strong>Variational Inference: Fast Approximation</strong> - Approximate <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span> with a tractable family <span class="math inline">\(q_\phi(\mathbf{w})\)</span> - Optimize <span class="math inline">\(\phi\)</span> to maximize the <strong>ELBO</strong>: <span class="math display">\[\mathcal{L}(\phi) = \mathbb{E}_{q_\phi}\left[\log p(Y | X, \mathbf{w})\right] - \text{KL}(q_\phi(\mathbf{w}) || p(\mathbf{w}))\]</span> - <strong>Reparameterization trick</strong> enables low-variance gradient estimates - <strong>Result</strong>: Posterior approximation in seconds instead of minutes</p>
<p><strong>Function Space vs Weight Space</strong> - We don’t visualize high-dimensional weight distributions directly - Instead, we visualize <strong>functions</strong> sampled from the posterior - <strong>Posterior predictive</strong> is what matters for making predictions!</p>
<section id="looking-forward-to-vaes-variational-autoencoders" class="level3" data-number="7.7.1">
<h3 data-number="7.7.1" class="anchored" data-anchor-id="looking-forward-to-vaes-variational-autoencoders"><span class="header-section-number">7.7.1</span> Looking forward to VAEs (Variational Autoencoders)</h3>
<p>The VI approach you learned here is the <strong>same mathematical foundation</strong> for VAEs. The key differences are:</p>
<ul>
<li><strong>Here (Bayesian NN)</strong>: Posterior over <strong>weights</strong> <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span></li>
<li><strong>VAEs</strong>: Posterior over <strong>latent variables</strong> <span class="math inline">\(p(\mathbf{z} | \mathbf{x})\)</span></li>
</ul>
<p>The ELBO, reparameterization trick, and optimization strategy are identical.</p>
<p>Moreover, in VAEs, instead of optimizing <span class="math inline">\(q_\phi(z)\)</span> separately for each datapoint <span class="math inline">\(x\)</span>, we learn an <strong>encoder network</strong> <span class="math inline">\(q_\phi(z | x)\)</span> that works for <strong>all</strong> <span class="math inline">\(x\)</span>. This is called <strong>amortized inference</strong>.</p>
<p><strong>Questions to Reflect on</strong></p>
<p>Think about these before moving to the next notebook:</p>
<ol type="1">
<li><p><strong>Why does VI underestimate uncertainty?</strong><br>
Hint: Is the Mean-field approximation where we assume independence between weights reasonable? When might it not be?</p></li>
<li><p><strong>When would MCMC be essential despite being slow?</strong></p></li>
<li><p><strong>Could we use a more expressive variational family?</strong><br>
We used a Mean-field Gaussian here, but are we limited to this?</p></li>
</ol>


</section>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part1/distribution_distance.html" class="pagination-link" aria-label="Measuring Distribution Distances">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Measuring Distribution Distances</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part2/part2.html" class="pagination-link" aria-label="Model-Specific Approaches">
        <span class="nav-page-text">Model-Specific Approaches</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Machine Learning for Mechanical Engineers © 2025 by <a href="./index.qmd#sec-contributors">Mark Fuge and IDEAL Lab Contributors</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>