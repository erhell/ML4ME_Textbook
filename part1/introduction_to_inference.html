<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mark Fuge">
<meta name="dcterms.date" content="2025-10-22">

<title>7&nbsp; Introduction to Inference – Machine Learning for Mechanical Engineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part2/part2.html" rel="next">
<link href="../part1/distribution_distance.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-0dd2bd5de344125cf763a379ddc3eb04.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part1/part1.html">Foundational Skills</a></li><li class="breadcrumb-item"><a href="../part1/introduction_to_inference.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning for Mechanical Engineering</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part1/part1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foundational Skills</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/reviewing_supervised_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Reviewing Supervised Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/cross_validation_linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/supervised_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/linear_decompositions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Review of Linear Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/taking_derivatives.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Taking Derivatives with Automatic Differentiation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/distribution_distance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Measuring Distribution Distances</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part1/introduction_to_inference.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Inference</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part2/part2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model-Specific Approaches</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part2/review_neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Review of Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part2/gen_models/intro_to_GANS.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part2/gen_models/GAN_pitfalls.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">GAN Training Pitfalls</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part2/gen_models/OT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Optimal Transport for Generative Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../problems/problems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../problems/ps1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Problem Set 1</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../notebooks/notebooks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">In-Class Notebooks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/california_housing_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Housing Price Data Visualization In-Class Exercise</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/helpful_tooling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Helpful Tooling for Working with and Debugging Machine Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/course_progression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Course Lecture Progression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/review_of_singular_value_decomposition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Review of Matrices and the Singular Value Decomposition</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivation-and-toy-problem-definition" id="toc-motivation-and-toy-problem-definition" class="nav-link active" data-scroll-target="#motivation-and-toy-problem-definition"><span class="header-section-number">7.1</span> Motivation and Toy Problem Definition</a>
  <ul class="collapse">
  <li><a href="#the-dataset-nonlinear-sensor-measurements" id="toc-the-dataset-nonlinear-sensor-measurements" class="nav-link" data-scroll-target="#the-dataset-nonlinear-sensor-measurements"><span class="header-section-number">7.1.1</span> The Dataset: Nonlinear Sensor Measurements</a></li>
  </ul></li>
  <li><a href="#linear-bayesian-regression" id="toc-linear-bayesian-regression" class="nav-link" data-scroll-target="#linear-bayesian-regression"><span class="header-section-number">7.2</span> Linear Bayesian Regression</a>
  <ul class="collapse">
  <li><a href="#the-model" id="toc-the-model" class="nav-link" data-scroll-target="#the-model"><span class="header-section-number">7.2.1</span> The Model</a></li>
  <li><a href="#the-posterior-predictive-distribution" id="toc-the-posterior-predictive-distribution" class="nav-link" data-scroll-target="#the-posterior-predictive-distribution"><span class="header-section-number">7.2.2</span> The Posterior Predictive Distribution</a></li>
  </ul></li>
  <li><a href="#a-small-neural-network-nonlinear-model" id="toc-a-small-neural-network-nonlinear-model" class="nav-link" data-scroll-target="#a-small-neural-network-nonlinear-model"><span class="header-section-number">7.3</span> A Small Neural Network (Nonlinear Model)</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture"><span class="header-section-number">7.3.1</span> Architecture</a></li>
  <li><a href="#the-new-challenge-intractable-posterior" id="toc-the-new-challenge-intractable-posterior" class="nav-link" data-scroll-target="#the-new-challenge-intractable-posterior"><span class="header-section-number">7.3.2</span> The New Challenge: Intractable Posterior</a></li>
  </ul></li>
  <li><a href="#mcmc-approximation" id="toc-mcmc-approximation" class="nav-link" data-scroll-target="#mcmc-approximation"><span class="header-section-number">7.4</span> MCMC Approximation</a>
  <ul class="collapse">
  <li><a href="#what-is-mcmc" id="toc-what-is-mcmc" class="nav-link" data-scroll-target="#what-is-mcmc"><span class="header-section-number">7.4.1</span> What is MCMC?</a></li>
  <li><a href="#note-on-implementation" id="toc-note-on-implementation" class="nav-link" data-scroll-target="#note-on-implementation"><span class="header-section-number">7.4.2</span> Note on Implementation</a></li>
  </ul></li>
  <li><a href="#variational-inference-for-neural-networks" id="toc-variational-inference-for-neural-networks" class="nav-link" data-scroll-target="#variational-inference-for-neural-networks"><span class="header-section-number">7.5</span> Variational Inference for Neural Networks</a>
  <ul class="collapse">
  <li><a href="#chosing-a-tractable-family-q_phimathbfw" id="toc-chosing-a-tractable-family-q_phimathbfw" class="nav-link" data-scroll-target="#chosing-a-tractable-family-q_phimathbfw"><span class="header-section-number">7.5.1</span> Chosing a tractable family <span class="math inline">\(q_\phi(\mathbf{w})\)</span></a></li>
  <li><a href="#marginal-likelihood-evidence" id="toc-marginal-likelihood-evidence" class="nav-link" data-scroll-target="#marginal-likelihood-evidence"><span class="header-section-number">7.5.2</span> 1. Marginal Likelihood (Evidence)</a></li>
  <li><a href="#introducing-a-variational-distribution" id="toc-introducing-a-variational-distribution" class="nav-link" data-scroll-target="#introducing-a-variational-distribution"><span class="header-section-number">7.5.3</span> 2. Introducing a Variational Distribution</a></li>
  <li><a href="#applying-jensens-inequality" id="toc-applying-jensens-inequality" class="nav-link" data-scroll-target="#applying-jensens-inequality"><span class="header-section-number">7.5.4</span> 3. Applying Jensen’s Inequality</a></li>
  <li><a href="#the-elbo-expression" id="toc-the-elbo-expression" class="nav-link" data-scroll-target="#the-elbo-expression"><span class="header-section-number">7.5.5</span> 4. The ELBO Expression</a></li>
  <li><a href="#note-on-the-reparameterization-trick" id="toc-note-on-the-reparameterization-trick" class="nav-link" data-scroll-target="#note-on-the-reparameterization-trick"><span class="header-section-number">7.5.6</span> Note on the Reparameterization Trick</a></li>
  </ul></li>
  <li><a href="#possible-experiments" id="toc-possible-experiments" class="nav-link" data-scroll-target="#possible-experiments"><span class="header-section-number">7.6</span> Possible Experiments</a></li>
  <li><a href="#summary-and-the-bridge-to-vaes" id="toc-summary-and-the-bridge-to-vaes" class="nav-link" data-scroll-target="#summary-and-the-bridge-to-vaes"><span class="header-section-number">7.7</span> Summary and the bridge to VAEs</a>
  <ul class="collapse">
  <li><a href="#bayesian-inference-quantifies-uncertainty" id="toc-bayesian-inference-quantifies-uncertainty" class="nav-link" data-scroll-target="#bayesian-inference-quantifies-uncertainty"><span class="header-section-number">7.7.1</span> 1. <strong>Bayesian Inference Quantifies Uncertainty</strong></a></li>
  <li><a href="#linear-models-have-limitations" id="toc-linear-models-have-limitations" class="nav-link" data-scroll-target="#linear-models-have-limitations"><span class="header-section-number">7.7.2</span> 2. <strong>Linear Models Have Limitations</strong></a></li>
  <li><a href="#neural-networks-provide-flexibility-at-the-cost-of-an-intractable-posterior" id="toc-neural-networks-provide-flexibility-at-the-cost-of-an-intractable-posterior" class="nav-link" data-scroll-target="#neural-networks-provide-flexibility-at-the-cost-of-an-intractable-posterior"><span class="header-section-number">7.7.3</span> 3. <strong>Neural Networks Provide Flexibility at the cost of an intractable posterior</strong></a></li>
  <li><a href="#mcmc-accurate-but-slow" id="toc-mcmc-accurate-but-slow" class="nav-link" data-scroll-target="#mcmc-accurate-but-slow"><span class="header-section-number">7.7.4</span> 4. <strong>MCMC: Accurate but Slow</strong></a></li>
  <li><a href="#variational-inference-fast-approximation" id="toc-variational-inference-fast-approximation" class="nav-link" data-scroll-target="#variational-inference-fast-approximation"><span class="header-section-number">7.7.5</span> 5. <strong>Variational Inference: Fast Approximation</strong></a></li>
  <li><a href="#function-space-vs-weight-space" id="toc-function-space-vs-weight-space" class="nav-link" data-scroll-target="#function-space-vs-weight-space"><span class="header-section-number">7.7.6</span> 6. <strong>Function Space vs Weight Space</strong></a></li>
  <li><a href="#looking-forward-to-vaes-variational-autoencoders" id="toc-looking-forward-to-vaes-variational-autoencoders" class="nav-link" data-scroll-target="#looking-forward-to-vaes-variational-autoencoders"><span class="header-section-number">7.7.7</span> Looking forward to VAEs (Variational Autoencoders)</a></li>
  <li><a href="#stochastic-vi" id="toc-stochastic-vi" class="nav-link" data-scroll-target="#stochastic-vi"><span class="header-section-number">7.7.8</span> Stochastic VI</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part1/part1.html">Foundational Skills</a></li><li class="breadcrumb-item"><a href="../part1/introduction_to_inference.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Inference</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Inference</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mark Fuge </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 22, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><strong>Learning Objectives</strong></p>
<p>By the end of this notebook, you will:</p>
<ol type="1">
<li>Understand <strong>Bayesian inference for regression</strong> and how it quantifies model uncertainty</li>
<li>Visualize the <strong>posterior predictive distribution</strong> for linear models</li>
<li>Recognize the <strong>limitations of linear models</strong> on nonlinear data</li>
<li>Learn how <strong>neural networks</strong> provide flexible nonlinear models</li>
<li>Understand why exact Bayesian inference becomes <strong>intractable</strong> for neural networks</li>
<li>Implement <strong>Variational Inference (VI)</strong> as an approximate solution</li>
<li>Compare VI to MCMC and understand the <strong>speed vs.&nbsp;accuracy tradeoff</strong></li>
<li>Visualize <strong>predictive uncertainty</strong> in function space</li>
</ol>
<p><strong>The Big Picture</strong></p>
<p>Imagine you have sparse sensor measurements from a mechanical system. You want to:</p>
<ol type="1">
<li><strong>Fit a model</strong> that captures the underlying relationship</li>
<li><strong>Quantify uncertainty</strong> in predictions (critical for safety-critical systems)</li>
<li><strong>Make predictions</strong> with confidence intervals</li>
</ol>
<p>This notebook shows you how to do this using <strong>Bayesian inference</strong>, starting with simple linear models and progressing to flexible neural networks where <strong>variational inference</strong> becomes essential.</p>
<section id="motivation-and-toy-problem-definition" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="motivation-and-toy-problem-definition"><span class="header-section-number">7.1</span> Motivation and Toy Problem Definition</h2>
<p>Let’s start with a fundamental question: <strong>when you fit a model to data, how confident should you be in its predictions?</strong></p>
<p>In traditional machine learning, we find a single “best” set of parameters. But in engineering, we often need to know: - How uncertain are we about the model? - Where in the input space are predictions reliable? - What happens if we had slightly different data?</p>
<p><strong>Bayesian inference</strong> answers these questions by maintaining a <strong>distribution over models</strong> rather than picking a single one.</p>
<section id="the-dataset-nonlinear-sensor-measurements" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="the-dataset-nonlinear-sensor-measurements"><span class="header-section-number">7.1.1</span> The Dataset: Nonlinear Sensor Measurements</h3>
<p>We’ll use a simple nonlinear function to represent sensor data: <span class="math display">\[y = \sin(x) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, 0.1^2)\]</span></p>
<p>This represents, for example: - Cyclic behavior (vibrations, temperatures, etc.) - Noisy measurements - Sparse observations (we won’t measure everywhere)</p>
<p>We will be interested in understanding how the model can capture the uncertainty of its prediction.</p>
<div id="2fb1a989" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup and imports</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Tuple</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set style</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.dpi'</span>] <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'savefig.dpi'</span>] <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seeds for reproducibility</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>&lt;torch._C.Generator at 0x1d140a56df0&gt;</code></pre>
</div>
</div>
<div id="502d8bcc" class="cell">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate nonlinear dataset</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_nonlinear_data(n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">20</span>, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                           noise_std: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                           x_range: Tuple[<span class="bu">float</span>, <span class="bu">float</span>] <span class="op">=</span> (<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)) <span class="op">-&gt;</span> Tuple[np.ndarray, np.ndarray]:</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Generate sparse noisy samples from a sinusoidal function.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    This simulates sensor measurements from a cyclic process.</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">        n_samples: Number of observations</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">        noise_std: Standard deviation of measurement noise</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">        x_range: Input domain (min, max)</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">        x: Input locations (n_samples,)</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co">        y: Noisy measurements (n_samples,)</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> x_range</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample input locations (sparse, irregular spacing)</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.random.uniform(x_min, x_max, n_samples)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># True underlying function (unknown to the model)</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> np.sin(x)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add measurement noise</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y_true <span class="op">+</span> noise_std <span class="op">*</span> np.random.randn(n_samples)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort for easier visualization</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    sort_idx <span class="op">=</span> np.argsort(x)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x[sort_idx]</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y[sort_idx]</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x, y</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate training data</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>n_train <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>noise_std <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>x_train, y_train <span class="op">=</span> generate_nonlinear_data(n_train, noise_std)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dense grid for visualization (the "true" function)</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">200</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>y_test_true <span class="op">=</span> np.sin(x_test)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the dataset</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot true function</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True function: $y = </span><span class="ch">\\</span><span class="st">sin(x)$'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot noisy training observations</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">80</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="fl">1.5</span>, zorder<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="ss">f'Training data (n=</span><span class="sc">{</span>n_train<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Shade noise region</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>ax.fill_between(x_test, y_test_true <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>noise_std, y_test_true <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>noise_std,</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'gray'</span>, label<span class="op">=</span><span class="ss">f'±2sigma noise band (sigma=</span><span class="sc">{</span>noise_std<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Nonlinear Sensor Measurements'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="linear-bayesian-regression" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="linear-bayesian-regression"><span class="header-section-number">7.2</span> Linear Bayesian Regression</h2>
<p>Let’s start with the simplest approach: <strong>linear regression with Bayesian inference</strong>.</p>
<section id="the-model" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="the-model"><span class="header-section-number">7.2.1</span> The Model</h3>
<p>We assume a linear relationship: <span class="math display">\[y = w_0 + w_1 x + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)\]</span></p>
<p>In Bayesian inference, we place a <strong>prior</strong> over the weights: <span class="math display">\[p(\mathbf{w}) = \mathcal{N}(\mathbf{0}, \alpha^{-1} \mathbf{I})\]</span></p>
<p>After seeing data <span class="math inline">\((X, Y)\)</span>, the <strong>posterior</strong> over weights is: <span class="math display">\[p(\mathbf{w} | X, Y) = \mathcal{N}(\mathbf{m}_N, \mathbf{S}_N)\]</span></p>
<p>where (for linear-Gaussian models, we can derive it exactly): <span class="math display">\[\mathbf{S}_N = (\alpha \mathbf{I} + \beta \mathbf{\Phi}^T \mathbf{\Phi})^{-1}\]</span> <span class="math display">\[\mathbf{m}_N = \beta \mathbf{S}_N \mathbf{\Phi}^T \mathbf{y}\]</span></p>
<p>where <span class="math inline">\(\beta = 1/\sigma^2\)</span> is the noise precision and <span class="math inline">\(\mathbf{\Phi}\)</span> is the design matrix.</p>
</section>
<section id="the-posterior-predictive-distribution" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="the-posterior-predictive-distribution"><span class="header-section-number">7.2.2</span> The Posterior Predictive Distribution</h3>
<p>For a new input <span class="math inline">\(x_*\)</span>, the <strong>predictive distribution</strong> integrates over all possible weights: <span class="math display">\[p(y_* | x_*, X, Y) = \int p(y_* | x_*, \mathbf{w}) p(\mathbf{w} | X, Y) d\mathbf{w}\]</span></p>
<p>For linear-Gaussian models, this is also Gaussian: <span class="math display">\[p(y_* | x_*, X, Y) = \mathcal{N}(\mathbf{m}_N^T \phi(x_*), \sigma_N^2(x_*))\]</span></p>
<p>The variance <span class="math inline">\(\sigma_N^2(x_*)\)</span> tells us <strong>how uncertain</strong> we are about predictions at <span class="math inline">\(x_*\)</span>.</p>
<p>Let’s implement this and see what happens when applied to our non-linear data.</p>
<div id="79950a28" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BayesianLinearRegression:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Bayesian linear regression with Gaussian prior and likelihood.</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Closed-form posterior over weights for polynomial basis functions.</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, degree: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>, alpha: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>, beta: <span class="bu">float</span> <span class="op">=</span> <span class="fl">100.0</span>):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">            degree: Degree of polynomial basis (1 = linear)</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">            alpha: Prior precision (inverse variance)</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">            beta: Noise precision (1/sigma^2)</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.degree <span class="op">=</span> degree</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> beta</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.m_N <span class="op">=</span> <span class="va">None</span>  <span class="co"># Posterior mean</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.S_N <span class="op">=</span> <span class="va">None</span>  <span class="co"># Posterior covariance</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _design_matrix(<span class="va">self</span>, x: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute polynomial design matrix."""</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        Phi <span class="op">=</span> np.concatenate([x<span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.degree <span class="op">+</span> <span class="dv">1</span>)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Phi</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X: np.ndarray, y: np.ndarray):</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute posterior over weights given training data."""</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        Phi <span class="op">=</span> <span class="va">self</span>._design_matrix(X)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        n_features <span class="op">=</span> Phi.shape[<span class="dv">1</span>]</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Posterior covariance</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.S_N <span class="op">=</span> np.linalg.inv(<span class="va">self</span>.alpha <span class="op">*</span> np.eye(n_features) <span class="op">+</span> </span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>                                  <span class="va">self</span>.beta <span class="op">*</span> Phi.T <span class="op">@</span> Phi)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Posterior mean</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.m_N <span class="op">=</span> <span class="va">self</span>.beta <span class="op">*</span> <span class="va">self</span>.S_N <span class="op">@</span> Phi.T <span class="op">@</span> y</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X_test: np.ndarray, return_std: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="co">        Posterior predictive distribution.</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co">            mean: Predictive mean</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="co">            std: Predictive standard deviation (if return_std=True)</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        Phi_test <span class="op">=</span> <span class="va">self</span>._design_matrix(X_test)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predictive mean</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        y_mean <span class="op">=</span> Phi_test <span class="op">@</span> <span class="va">self</span>.m_N</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_std:</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Predictive variance (includes noise and weight uncertainty)</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>            y_var <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span><span class="va">self</span>.beta <span class="op">+</span> np.<span class="bu">sum</span>(Phi_test <span class="op">@</span> <span class="va">self</span>.S_N <span class="op">*</span> Phi_test, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>            y_std <span class="op">=</span> np.sqrt(y_var)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> y_mean, y_std</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y_mean</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_weights(<span class="va">self</span>, n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sample weight vectors from posterior."""</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.multivariate_normal(<span class="va">self</span>.m_N, <span class="va">self</span>.S_N, size<span class="op">=</span>n_samples)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit linear Bayesian regression</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>blr <span class="op">=</span> BayesianLinearRegression(degree<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">1.0</span><span class="op">/</span>noise_std<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>blr.fit(x_train, y_train)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior predictive distribution</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>y_pred_mean, y_pred_std <span class="op">=</span> blr.predict(x_test)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear Bayesian Regression Fitted!"</span>)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Posterior mean weights: </span><span class="sc">{</span>blr<span class="sc">.</span>m_N<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Posterior std of weights: </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(np.diag(blr.S_N))<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear Bayesian Regression Fitted!
Posterior mean weights: [-0.07553835  0.33272507]
Posterior std of weights: [0.0225762  0.01242588]</code></pre>
</div>
</div>
<div id="24f073d0" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize linear Bayesian regression predictions</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">6</span>))</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: Posterior predictive with uncertainty bands</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># True function</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True: $y = </span><span class="ch">\\</span><span class="st">sin(x)$'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Training data</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">80</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="fl">1.5</span>, zorder<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictive mean</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_pred_mean, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Linear prediction (posterior mean)'</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncertainty bands (±1sigma and ±2sigma)</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>ax.fill_between(x_test, y_pred_mean <span class="op">-</span> y_pred_std, y_pred_mean <span class="op">+</span> y_pred_std,</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'±1sigma predictive'</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>ax.fill_between(x_test, y_pred_mean <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>y_pred_std, y_pred_mean <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>y_pred_std,</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'±2sigma predictive'</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Linear Model: Cannot Capture Nonlinearity'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">10</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>])</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Function samples from posterior</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample weight vectors and plot corresponding functions</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>w_samples <span class="op">=</span> blr.sample_weights(n_samples<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>Phi_test <span class="op">=</span> np.concatenate([x_test.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    y_sample <span class="op">=</span> Phi_test <span class="op">@</span> w_samples[i]</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, y_sample, <span class="st">'b-'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="co"># True function and data</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True function'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">80</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="fl">1.5</span>, zorder<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Posterior Function Samples (50 draws)'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>])</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="a-small-neural-network-nonlinear-model" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="a-small-neural-network-nonlinear-model"><span class="header-section-number">7.3</span> A Small Neural Network (Nonlinear Model)</h2>
<p>The linear model fails because the true function is nonlinear. What if we try to use a simple <strong>neural network</strong> instead?</p>
<section id="architecture" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="architecture"><span class="header-section-number">7.3.1</span> Architecture</h3>
<p>We’ll use a simple 1-hidden-layer network: <span class="math display">\[f(x; \mathbf{w}) = \mathbf{w}_2^T \sigma(\mathbf{w}_1 x + \mathbf{b}_1) + b_2\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is ReLU activation. With just 5 hidden units, this can approximate smooth nonlinear functions.</p>
</section>
<section id="the-new-challenge-intractable-posterior" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="the-new-challenge-intractable-posterior"><span class="header-section-number">7.3.2</span> The New Challenge: Intractable Posterior</h3>
<p>Unlike linear regression, the posterior <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span> is <strong>no longer Gaussian</strong>! We cannot compute it analytically.</p>
<p>However, we can still: 1. Define a <strong>prior</strong> <span class="math inline">\(p(\mathbf{w})\)</span> (e.g., Gaussian on all weights) 2. Sample functions from the prior to see what kinds of functions are admissible under the model 3. Later, we can use <strong>VI or MCMC</strong> to approximate the posterior</p>
<p>Let’s first see what the <strong>prior predictive distribution</strong> looks like.</p>
<div id="7333ddf4" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SmallNN(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simple 1-hidden-layer neural network for regression.</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">1</span>, hidden_size)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_size, <span class="dv">1</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.fc2(h)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y.squeeze()</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_weights_flat(<span class="va">self</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Get all weights as a flat vector."""</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat([p.flatten() <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters()])</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_weights_flat(<span class="va">self</span>, w_flat: torch.Tensor):</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Set all weights from a flat vector."""</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        offset <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters():</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>            n_params <span class="op">=</span> p.numel()</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>            p.data <span class="op">=</span> w_flat[offset:offset<span class="op">+</span>n_params].view(p.shape)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>            offset <span class="op">+=</span> n_params</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Create network and count parameters</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>hidden_size <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SmallNN(hidden_size<span class="op">=</span>hidden_size)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>n_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Neural Network Architecture:"</span>)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Input → </span><span class="sc">{</span>hidden_size<span class="sc">}</span><span class="ss"> hidden (ReLU) → 1 output"</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Total parameters: </span><span class="sc">{</span>n_params<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Parameter breakdown:"</span>)</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: shape </span><span class="sc">{</span>param<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, count </span><span class="sc">{</span>param<span class="sc">.</span>numel()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Neural Network Architecture:
  Input → 5 hidden (ReLU) → 1 output
  Total parameters: 16
Parameter breakdown:
  fc1.weight: shape torch.Size([5, 1]), count 5
  fc1.bias: shape torch.Size([5]), count 5
  fc2.weight: shape torch.Size([1, 5]), count 5
  fc2.bias: shape torch.Size([1]), count 1</code></pre>
</div>
</div>
<div id="7cdc97d5" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample functions from the PRIOR (before seeing any data)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_prior_functions(model, x_test, n_samples<span class="op">=</span><span class="dv">20</span>, prior_std<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Sample function predictions from prior p(w) = N(0, prior_std^2 * I).</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    x_test_tensor <span class="op">=</span> torch.FloatTensor(x_test)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Sample weights from prior</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>                param.data.normal_(<span class="dv">0</span>, prior_std)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(x_test_tensor).numpy()</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            predictions.append(y_pred)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(predictions)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate prior predictive samples</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>prior_std <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>n_prior_samples <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>prior_predictions <span class="op">=</span> sample_prior_functions(model, x_test, n_prior_samples, prior_std)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize prior predictive distribution</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">7</span>))</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot prior function samples</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_prior_samples):</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, prior_predictions[i], <span class="st">'purple'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot true function and data</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True function: $y = </span><span class="ch">\\</span><span class="st">sin(x)$'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="dv">2</span>, zorder<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Training data (not yet used!)'</span>)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Add dummy line for legend</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>ax.plot([], [], <span class="st">'purple'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="ss">f'Prior samples (n=</span><span class="sc">{</span>n_prior_samples<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'Prior Predictive: Neural Network Before Training</span><span class="ch">\n</span><span class="ss">(Prior: $w </span><span class="ch">\\</span><span class="ss">sim </span><span class="ch">\\</span><span class="ss">mathcal</span><span class="ch">{{</span><span class="ss">N</span><span class="ch">}}</span><span class="ss">(0, </span><span class="sc">{</span>prior_std<span class="sc">}</span><span class="ss">^2 I)$)'</span>, </span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>])</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see that just sampling functions from the prior, the possible Neural Network functions are a lot more diverse that those we could sample from the linear model. But how do we find out which of these weights are (probabilistically) likely?</p>
</section>
</section>
<section id="mcmc-approximation" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="mcmc-approximation"><span class="header-section-number">7.4</span> MCMC Approximation</h2>
<p>The neural network posterior <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span> is intractable, but we can <strong>sample</strong> from it using <strong>Markov Chain Monte Carlo (MCMC)</strong>.</p>
<section id="what-is-mcmc" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="what-is-mcmc"><span class="header-section-number">7.4.1</span> What is MCMC?</h3>
<p>MCMC methods (like Hamiltonian Monte Carlo) generate samples <span class="math inline">\(\mathbf{w}^{(1)}, \mathbf{w}^{(2)}, \ldots\)</span> from the posterior by: 1. Starting at a random initial weight vector 2. Proposing updated samples that respect the posterior density 3. Eventually converging to samples from <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span></p>
<p>So, in a nutshell, it allows you to generate plausible samples from <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span> without ever having to strictly compute the posterior.</p>
<p><strong>Pros</strong>: Asymptotically exact (given infinite samples)<br>
<strong>Cons</strong>: Slow, especially for high-dimensional models</p>
<p>For our small network, MCMC is feasible. We will use it as a reference to compare against variational inference at the end of this notebook.</p>
</section>
<section id="note-on-implementation" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="note-on-implementation"><span class="header-section-number">7.4.2</span> Note on Implementation</h3>
<p>For this notebook, we’ll use a simple <strong>Metropolis-Hastings</strong> sampler. In practice, you’d use more sophisticated methods like: - Hamiltonian Monte Carlo (HMC) via PyTorch or PyMC - No-U-Turn Sampler (NUTS) - Stochastic Gradient Langevin Dynamics (SGLD)</p>
<p>which we may cover in later notebooks. For an interactive example of samplers, I highly recommend checking out the <a href="https://chi-feng.github.io/mcmc-demo/app.html">MCMC Interactive Gallery</a></p>
<section id="the-metropolis-hastings-algorithm" class="level4" data-number="7.4.2.1">
<h4 data-number="7.4.2.1" class="anchored" data-anchor-id="the-metropolis-hastings-algorithm"><span class="header-section-number">7.4.2.1</span> The Metropolis-Hastings Algorithm</h4>
<p>The algorithm works as follows:</p>
<ol type="1">
<li><strong>Initialize</strong>: Start with random weights <span class="math inline">\(\mathbf{w}^{(0)}\)</span></li>
<li><strong>Propose</strong>: Generate a candidate <span class="math inline">\(\mathbf{w}^* \sim q(\mathbf{w}^* | \mathbf{w}^{(t)})\)</span> (typically a Gaussian centered at current weights)</li>
<li><strong>Accept/Reject</strong>: Compute the acceptance probability: <span class="math display">\[\alpha = \min\left(1, \frac{p(\mathbf{w}^* | X, Y)}{p(\mathbf{w}^{(t)} | X, Y)}\right) = \min\left(1, \frac{p(Y | X, \mathbf{w}^*) p(\mathbf{w}^*)}{p(Y | X, \mathbf{w}^{(t)}) p(\mathbf{w}^{(t)})}\right)\]</span> Accept <span class="math inline">\(\mathbf{w}^{(t+1)} = \mathbf{w}^*\)</span> with probability <span class="math inline">\(\alpha\)</span>, otherwise keep <span class="math inline">\(\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)}\)</span></li>
<li><strong>Repeat</strong>: Continue for many iterations until the chain converges</li>
</ol>
<p>The useful thing about this sampler (and indeed MCMC approaches in general), is that we only ever need to evaluate the <strong>unnormalized</strong> posterior (likelihood × prior), and thus can avoid computing the intractable normalizing constant.</p>
<p><strong>Key parameters to tune</strong>: - <strong>Proposal standard deviation</strong>: Too large → low acceptance rate; too small → slow exploration - <strong>Burn-in</strong>: Discard early samples before convergence - <strong>Thinning</strong>: Keep every <span class="math inline">\(k\)</span>-th sample to reduce autocorrelation</p>
<p>For our 16-parameter network, this is tractable. For modern deep networks with millions of parameters, MCMC becomes prohibitively slow and this is where variational inference can come to the rescue, as we will see later.</p>
<div id="3417dac5" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple MCMC sampler for neural network weights</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(model, x, y, noise_std<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute log p(y|x,w) for current weights."""</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        x_tensor <span class="op">=</span> torch.FloatTensor(x)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(x_tensor).numpy()</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(((y <span class="op">-</span> y_pred) <span class="op">/</span> noise_std) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">-=</span> <span class="bu">len</span>(y) <span class="op">*</span> np.log(noise_std <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_lik</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_prior(model, prior_std<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute log p(w) for current weights."""</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    log_p <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        log_p <span class="op">-=</span> <span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>((param <span class="op">/</span> prior_std) <span class="op">**</span> <span class="dv">2</span>).item()</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_p</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metropolis_hastings_nn(x_train, y_train, n_samples<span class="op">=</span><span class="dv">5000</span>, proposal_std<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>                           prior_std<span class="op">=</span><span class="fl">1.0</span>, noise_std<span class="op">=</span><span class="fl">0.1</span>, thin<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Simple Metropolis-Hastings MCMC for neural network weights.</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co">        thin: Only keep every 'thin'-th sample to reduce autocorrelation</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="co">        weight_samples: List of weight vectors</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="co">        acceptance_rate: Fraction of proposals accepted</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> SmallNN(hidden_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize at MAP estimate (rough approximation: just train briefly)</span></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    x_tensor <span class="op">=</span> torch.FloatTensor(x_train).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    y_tensor <span class="op">=</span> torch.FloatTensor(y_train)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Finding initial weights (quick MAP estimate)..."</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.mse_loss(model(x_tensor.squeeze()), y_tensor)</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Starting MCMC sampling (</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss"> iterations)..."</span>)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Current state</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>    current_log_posterior <span class="op">=</span> log_likelihood(model, x_train, y_train, noise_std) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>                            log_prior(model, prior_std)</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    weight_samples <span class="op">=</span> []</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>    n_accepted <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Propose new weights</span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>        proposed_model <span class="op">=</span> SmallNN(hidden_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> p_curr, p_prop <span class="kw">in</span> <span class="bu">zip</span>(model.parameters(), proposed_model.parameters()):</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>                p_prop.data <span class="op">=</span> p_curr.data <span class="op">+</span> proposal_std <span class="op">*</span> torch.randn_like(p_curr)</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute proposed log posterior</span></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>        proposed_log_posterior <span class="op">=</span> log_likelihood(proposed_model, x_train, y_train, noise_std) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>                                  log_prior(proposed_model, prior_std)</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accept/reject</span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>        log_alpha <span class="op">=</span> proposed_log_posterior <span class="op">-</span> current_log_posterior</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.log(np.random.rand()) <span class="op">&lt;</span> log_alpha:</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Accept</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> proposed_model</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>            current_log_posterior <span class="op">=</span> proposed_log_posterior</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>            n_accepted <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store sample (with thinning)</span></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> thin <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>            weight_samples.append(model.get_weights_flat().clone().detach().numpy())</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">2000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>            acc_rate <span class="op">=</span> n_accepted <span class="op">/</span> (i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Iteration </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">, acceptance rate: </span><span class="sc">{</span>acc_rate<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>    acceptance_rate <span class="op">=</span> n_accepted <span class="op">/</span> n_samples</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"MCMC complete! Final acceptance rate: </span><span class="sc">{</span>acceptance_rate<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weight_samples, acceptance_rate</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Run MCMC (this will take a minute or two)</span></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Running MCMC to sample from posterior p(w|X,Y)..."</span>)</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This may take 1-2 minutes for accurate results."</span>)</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>mcmc_samples, acc_rate <span class="op">=</span> metropolis_hastings_nn(</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>    x_train, y_train, </span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">10000</span>, </span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>    proposal_std<span class="op">=</span><span class="fl">0.010</span>,</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>    prior_std<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>    noise_std<span class="op">=</span>noise_std,</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>    thin<span class="op">=</span><span class="dv">10</span></span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Collected </span><span class="sc">{</span><span class="bu">len</span>(mcmc_samples)<span class="sc">}</span><span class="ss"> posterior samples (after thinning)"</span>)</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Acceptance rate: </span><span class="sc">{</span>acc_rate<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Running MCMC to sample from posterior p(w|X,Y)...
This may take 1-2 minutes for accurate results.
Finding initial weights (quick MAP estimate)...
Starting MCMC sampling (10000 iterations)...
Finding initial weights (quick MAP estimate)...
Starting MCMC sampling (10000 iterations)...
  Iteration 2000/10000, acceptance rate: 0.460
  Iteration 2000/10000, acceptance rate: 0.460
  Iteration 4000/10000, acceptance rate: 0.458
  Iteration 4000/10000, acceptance rate: 0.458
  Iteration 6000/10000, acceptance rate: 0.462
  Iteration 6000/10000, acceptance rate: 0.462
  Iteration 8000/10000, acceptance rate: 0.466
  Iteration 8000/10000, acceptance rate: 0.466
  Iteration 10000/10000, acceptance rate: 0.455
MCMC complete! Final acceptance rate: 0.455
Collected 1000 posterior samples (after thinning)
Acceptance rate: 45.5%
  Iteration 10000/10000, acceptance rate: 0.455
MCMC complete! Final acceptance rate: 0.455
Collected 1000 posterior samples (after thinning)
Acceptance rate: 45.5%</code></pre>
</div>
</div>
<div id="a4e7a3ea" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize MCMC posterior predictive</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_with_weight_samples(weight_samples, x_test, n_samples<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generate predictions using sampled weights."""</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n_samples <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> <span class="bu">len</span>(weight_samples)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    model_temp <span class="op">=</span> SmallNN(hidden_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    x_test_tensor <span class="op">=</span> torch.FloatTensor(x_test)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Randomly select samples to plot</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    sample_indices <span class="op">=</span> np.random.choice(<span class="bu">len</span>(weight_samples), size<span class="op">=</span><span class="bu">min</span>(n_samples, <span class="bu">len</span>(weight_samples)), replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx <span class="kw">in</span> sample_indices:</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>            model_temp.set_weights_flat(torch.FloatTensor(weight_samples[idx]))</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model_temp(x_test_tensor).numpy()</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>            predictions.append(y_pred)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(predictions)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate MCMC posterior predictions</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>mcmc_predictions <span class="op">=</span> predict_with_weight_samples(mcmc_samples, x_test, n_samples<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">7</span>))</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot MCMC posterior samples</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mcmc_predictions)):</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, mcmc_predictions[i], <span class="st">'orange'</span>, alpha<span class="op">=</span><span class="fl">0.15</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="co"># True function and data</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True function'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="dv">2</span>, zorder<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior mean</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>mcmc_mean <span class="op">=</span> mcmc_predictions.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, mcmc_mean, <span class="st">'orange'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'MCMC posterior mean'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Dummy line for legend</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>ax.plot([], [], <span class="st">'orange'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'MCMC posterior samples'</span>)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'MCMC Posterior Predictive Distribution</span><span class="ch">\n</span><span class="st">(Reference "Ground Truth")'</span>, </span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="variational-inference-for-neural-networks" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="variational-inference-for-neural-networks"><span class="header-section-number">7.5</span> Variational Inference for Neural Networks</h2>
<p>MCMC works but is slow, since it essentially has to generate thousands of good samples. In constrast, Variational Inference methods turn the <strong>integration</strong> problem into an <strong>optimization</strong> problem. Instead of sampling from <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span>, we:</p>
<ol type="1">
<li>Choose a tractable family <span class="math inline">\(q_\phi(\mathbf{w})\)</span> (e.g., Gaussian)</li>
<li>Find <span class="math inline">\(\phi\)</span> that makes <span class="math inline">\(q_\phi\)</span> close to the true posterior</li>
<li>Measure “closeness” using the <strong>ELBO</strong> (Evidence Lower Bound)</li>
</ol>
<section id="chosing-a-tractable-family-q_phimathbfw" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="chosing-a-tractable-family-q_phimathbfw"><span class="header-section-number">7.5.1</span> Chosing a tractable family <span class="math inline">\(q_\phi(\mathbf{w})\)</span></h3>
<p>There are many options for this, as we will see in later more advanced notebooks, but for now let’s use something simple: a <strong>mean-field Gaussian</strong> approximation: <span class="math display">\[q_\phi(\mathbf{w}) = \mathcal{N}(\boldsymbol{\mu}_\phi, \text{diag}(\boldsymbol{\sigma}_\phi^2))\]</span></p>
<p>Here, each weight in the network has its own mean and variance, and for our network with <code>n_params</code> parameters, we would then optimize both for all weights, leading to <span class="math inline">\(2 \times {n_{params}}\)</span> variational parameters.</p>
<p>But what should we be optimizing, exactly?</p>
</section>
<section id="marginal-likelihood-evidence" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="marginal-likelihood-evidence"><span class="header-section-number">7.5.2</span> 1. Marginal Likelihood (Evidence)</h3>
<p>The goal in Bayesian inference is to compute the <strong>marginal likelihood</strong> (also called the evidence):</p>
<p><span class="math display">\[
p(Y|X) = \int p(Y|X, \mathbf{w})\, p(\mathbf{w})\, d\mathbf{w}
\]</span></p>
<p>Which sounds fine in theory, but this integral is usually <strong>intractable</strong> for neural networks or other complex models we might be interested in. How can we solve this?</p>
</section>
<section id="introducing-a-variational-distribution" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="introducing-a-variational-distribution"><span class="header-section-number">7.5.3</span> 2. Introducing a Variational Distribution</h3>
<p>We introduce a tractable distribution <span class="math inline">\(q_\phi(\mathbf{w})\)</span> to approximate the true posterior <span class="math inline">\(p(\mathbf{w}|X,Y)\)</span>.</p>
<p>We can rewrite the log evidence using <span class="math inline">\(q_\phi\)</span>:</p>
<p><span class="math display">\[
\log p(Y|X) = \log \int p(Y|X, \mathbf{w})\, p(\mathbf{w})\, d\mathbf{w}
\]</span></p>
<p><span class="math display">\[
= \log \int q_\phi(\mathbf{w})\, \frac{p(Y|X, \mathbf{w})\, p(\mathbf{w})}{q_\phi(\mathbf{w})}\, d\mathbf{w}
\]</span></p>
<p>So far, this mathematical maneuver hasn’t really bought us much, but we can apply an approximation to help us separate out some terms.</p>
</section>
<section id="applying-jensens-inequality" class="level3" data-number="7.5.4">
<h3 data-number="7.5.4" class="anchored" data-anchor-id="applying-jensens-inequality"><span class="header-section-number">7.5.4</span> 3. Applying Jensen’s Inequality</h3>
<p>Jensen’s Inequality states that, for a convex function <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
f(\mathbb{E}[x]) \le \mathbb{E}[f(x)]
\]</span></p>
<p>and if we apply Jensen’s inequality to the above assuming <span class="math inline">\(f=\log\)</span> and <span class="math inline">\(\mathbb{E}=\int\)</span> (note we flip the sign of the inequality since <span class="math inline">\(\log\)</span> is concave, not convex), we get:</p>
<p><span class="math display">\[
\log \mathbb{E}_{q_\phi}\left[\frac{p(Y|X, \mathbf{w})\, p(\mathbf{w})}{q_\phi(\mathbf{w})}\right]
\geq \mathbb{E}_{q_\phi}\left[\log \frac{p(Y|X, \mathbf{w})\, p(\mathbf{w})}{q_\phi(\mathbf{w})}\right]
\]</span></p>
<p>Now we can bring the log inside to help break out some of the terms and rearrange them.</p>
</section>
<section id="the-elbo-expression" class="level3" data-number="7.5.5">
<h3 data-number="7.5.5" class="anchored" data-anchor-id="the-elbo-expression"><span class="header-section-number">7.5.5</span> 4. The ELBO Expression</h3>
<p>This gives us the <strong>Evidence Lower Bound (ELBO)</strong>:</p>
<p><span class="math display">\[
\log p(Y|X) \geq \mathbb{E}_{q_\phi}\left[\log p(Y|X, \mathbf{w})\right]
+ \mathbb{E}_{q_\phi}\left[\log p(\mathbf{w})\right]
- \mathbb{E}_{q_\phi}\left[\log q_\phi(\mathbf{w})\right]
\]</span></p>
<p>Or, by grouping <span class="math inline">\(\mathbb{E}_{q_\phi}\left[\log p(\mathbf{w})\right]\)</span> and <span class="math inline">\(\mathbb{E}_{q_\phi}\left[\log q_\phi(\mathbf{w})\right]\)</span> into the (negative) KL Divergence:</p>
<p><span class="math display">\[
\mathcal{L}(\phi) = \mathbb{E}_{q_\phi}\left[\log p(Y|X, \mathbf{w})\right]
- \mathrm{KL}\left(q_\phi(\mathbf{w})\,\|\,p(\mathbf{w})\right)
\]</span></p>
<p>We arrive at the common form of the ELBO, where:</p>
<ul>
<li>The <strong>first term</strong> encourages <span class="math inline">\(q_\phi\)</span> to place mass on weights that explain the data well. This is the <strong>Expected log-likelihood</strong> and measures How well <span class="math inline">\(q_\phi\)</span> explains the data.</li>
<li>The <strong>second term</strong> (the <strong>KL divergence</strong>) regularizes <span class="math inline">\(q_\phi\)</span> to stay close to the prior.</li>
</ul>
<p>This is the objective we will then optimize when we perform variational inference.</p>
</section>
<section id="note-on-the-reparameterization-trick" class="level3" data-number="7.5.6">
<h3 data-number="7.5.6" class="anchored" data-anchor-id="note-on-the-reparameterization-trick"><span class="header-section-number">7.5.6</span> Note on the Reparameterization Trick</h3>
<p>While in principle we should be all good to go with optimizing the ELBO above, you will see that the gradients will end up having to differentiate backward through <span class="math inline">\(\mathbb{E}_{q_\phi}\)</span> which is a stochastic function. This can induce high variance in the gradient updates, and so in practice, we use something called the Reparameterization Trick to reduce variance in the gradients:</p>
<p><span class="math display">\[
\mathbf{w} = \boldsymbol{\mu}_\phi + \boldsymbol{\sigma}_\phi \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
\]</span></p>
<p>This decouples the stochasticity needed for the sampling of <span class="math inline">\(\mathbb{E}_{q_\phi}\)</span> from the gradient pathway backward through <span class="math inline">\(\mu_\phi\)</span> and <span class="math inline">\(\sigma_\phi\)</span>.</p>
<div id="86d17669" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variational Inference for Bayesian Neural Network</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BayesianNNVI(nn.Module):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Bayesian Neural Network with Variational Inference.</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Each weight has a variational posterior q(w) = N(mu, sigma^2).</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_size<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Variational parameters for layer 1: input -&gt; hidden</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1_mu_weight <span class="op">=</span> nn.Parameter(torch.randn(hidden_size, <span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1_mu_bias <span class="op">=</span> nn.Parameter(torch.randn(hidden_size) <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1_logsigma_weight <span class="op">=</span> nn.Parameter(torch.ones(hidden_size, <span class="dv">1</span>) <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>)  <span class="co"># Start with low variance</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1_logsigma_bias <span class="op">=</span> nn.Parameter(torch.ones(hidden_size) <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Variational parameters for layer 2: hidden -&gt; output</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2_mu_weight <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, hidden_size) <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2_mu_bias <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2_logsigma_weight <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>, hidden_size) <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2_logsigma_bias <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>) <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_weights(<span class="va">self</span>):</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sample weights using reparameterization trick."""</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer 1</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        fc1_weight <span class="op">=</span> <span class="va">self</span>.fc1_mu_weight <span class="op">+</span> torch.exp(<span class="va">self</span>.fc1_logsigma_weight) <span class="op">*</span> torch.randn_like(<span class="va">self</span>.fc1_mu_weight)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        fc1_bias <span class="op">=</span> <span class="va">self</span>.fc1_mu_bias <span class="op">+</span> torch.exp(<span class="va">self</span>.fc1_logsigma_bias) <span class="op">*</span> torch.randn_like(<span class="va">self</span>.fc1_mu_bias)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer 2</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        fc2_weight <span class="op">=</span> <span class="va">self</span>.fc2_mu_weight <span class="op">+</span> torch.exp(<span class="va">self</span>.fc2_logsigma_weight) <span class="op">*</span> torch.randn_like(<span class="va">self</span>.fc2_mu_weight)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        fc2_bias <span class="op">=</span> <span class="va">self</span>.fc2_mu_bias <span class="op">+</span> torch.exp(<span class="va">self</span>.fc2_logsigma_bias) <span class="op">*</span> torch.randn_like(<span class="va">self</span>.fc2_mu_bias)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (fc1_weight, fc1_bias, fc2_weight, fc2_bias)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, sample<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Forward pass with sampled or mean weights."""</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> sample:</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>            fc1_w, fc1_b, fc2_w, fc2_b <span class="op">=</span> <span class="va">self</span>.sample_weights()</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>            fc1_w, fc1_b <span class="op">=</span> <span class="va">self</span>.fc1_mu_weight, <span class="va">self</span>.fc1_mu_bias</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>            fc2_w, fc2_b <span class="op">=</span> <span class="va">self</span>.fc2_mu_weight, <span class="va">self</span>.fc2_mu_bias</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward propagation</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> F.relu(F.linear(x, fc1_w, fc1_b))</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> F.linear(h, fc2_w, fc2_b)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y.squeeze()</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> kl_divergence(<span class="va">self</span>, prior_std<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute KL(q||p) where p is N(0, prior_std^2)."""</span></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>        kl <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># KL for each parameter: KL(N(mu, sigma^2) || N(0, prior_std^2))</span></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> mu_param, logsigma_param <span class="kw">in</span> [</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>            (<span class="va">self</span>.fc1_mu_weight, <span class="va">self</span>.fc1_logsigma_weight),</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>            (<span class="va">self</span>.fc1_mu_bias, <span class="va">self</span>.fc1_logsigma_bias),</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>            (<span class="va">self</span>.fc2_mu_weight, <span class="va">self</span>.fc2_logsigma_weight),</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>            (<span class="va">self</span>.fc2_mu_bias, <span class="va">self</span>.fc2_logsigma_bias)</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>        ]:</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>            sigma <span class="op">=</span> torch.exp(logsigma_param)</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>            kl <span class="op">+=</span> <span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>                (mu_param<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> sigma<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> prior_std<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span> <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>logsigma_param <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>np.log(prior_std)</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> kl</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> elbo(<span class="va">self</span>, x, y, n_samples<span class="op">=</span><span class="dv">5</span>, noise_std<span class="op">=</span><span class="fl">0.1</span>, prior_std<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the Evidence Lower Bound.</span></span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a><span class="co">        ELBO = E_q[log p(y|x,w)] - KL(q||p)</span></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Expected log-likelihood (Monte Carlo estimate)</span></span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> <span class="va">self</span>.forward(x, sample<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>            log_lik <span class="op">+=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(((y <span class="op">-</span> y_pred) <span class="op">/</span> noise_std) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">=</span> log_lik <span class="op">/</span> n_samples</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">-=</span> <span class="bu">len</span>(y) <span class="op">*</span> np.log(noise_std <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># KL divergence</span></span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>        kl <span class="op">=</span> <span class="va">self</span>.kl_divergence(prior_std)</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ELBO</span></span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>        elbo <span class="op">=</span> log_lik <span class="op">-</span> kl</span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> elbo, log_lik, kl</span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize variational model</span></span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>vi_model <span class="op">=</span> BayesianNNVI(hidden_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bayesian NN with Variational Inference initialized"</span>)</span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment below if you would like a listing of all parameters and counts:</span></span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a><span class="co"># print("Variational parameters:")</span></span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a><span class="co"># for name, param in vi_model.named_parameters():</span></span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(f"  {name}: shape {param.shape}")</span></span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a><span class="co">#     n_var_params = sum(p.numel() for p in vi_model.parameters())</span></span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(f"Total variational parameters: {n_var_params}")</span></span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a><span class="co">#     print("(2x the number of network weights for mean + variance)")</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Bayesian NN with Variational Inference initialized</code></pre>
</div>
</div>
<div id="867cb7ea" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train with Variational Inference</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>vi_model <span class="op">=</span> BayesianNNVI(hidden_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(vi_model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>x_train_tensor <span class="op">=</span> torch.FloatTensor(x_train)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>y_train_tensor <span class="op">=</span> torch.FloatTensor(y_train)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">3000</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>n_mc_samples <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Monte Carlo samples per ELBO evaluation</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> {</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'elbo'</span>: [],</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'log_lik'</span>: [],</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'kl'</span>: []</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training Bayesian NN with Variational Inference..."</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Epochs: </span><span class="sc">{</span>n_epochs<span class="sc">}</span><span class="ss">, MC samples: </span><span class="sc">{</span>n_mc_samples<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute ELBO (negative for minimization)</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    elbo, log_lik, kl <span class="op">=</span> vi_model.elbo(x_train_tensor, y_train_tensor, </span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>                                       n_samples<span class="op">=</span>n_mc_samples, </span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>                                       noise_std<span class="op">=</span>noise_std, </span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>                                       prior_std<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>elbo  <span class="co"># Maximize ELBO = minimize negative ELBO</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backprop and update</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Track metrics</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'elbo'</span>].append(elbo.item())</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'log_lik'</span>].append(log_lik.item())</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'kl'</span>].append(kl.item())</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">500</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:4d}</span><span class="ss"> | ELBO: </span><span class="sc">{</span>elbo<span class="sc">.</span>item()<span class="sc">:8.2f}</span><span class="ss"> | "</span></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>              <span class="ss">f"Log-lik: </span><span class="sc">{</span>log_lik<span class="sc">.</span>item()<span class="sc">:8.2f}</span><span class="ss"> | KL: </span><span class="sc">{</span>kl<span class="sc">.</span>item()<span class="sc">:6.2f}</span><span class="ss">"</span>)</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Training Bayesian NN with Variational Inference...
Epochs: 3000, MC samples: 5
Epoch  500 | ELBO:   -51.02 | Log-lik:    -6.08 | KL:  44.94
Epoch  500 | ELBO:   -51.02 | Log-lik:    -6.08 | KL:  44.94
Epoch 1000 | ELBO:   -31.99 | Log-lik:    13.25 | KL:  45.25
Epoch 1000 | ELBO:   -31.99 | Log-lik:    13.25 | KL:  45.25
Epoch 1500 | ELBO:   -31.65 | Log-lik:    14.56 | KL:  46.21
Epoch 1500 | ELBO:   -31.65 | Log-lik:    14.56 | KL:  46.21
Epoch 2000 | ELBO:   -26.31 | Log-lik:    20.06 | KL:  46.36
Epoch 2000 | ELBO:   -26.31 | Log-lik:    20.06 | KL:  46.36
Epoch 2500 | ELBO:   -31.36 | Log-lik:    14.67 | KL:  46.03
Epoch 2500 | ELBO:   -31.36 | Log-lik:    14.67 | KL:  46.03
Epoch 3000 | ELBO:   -27.71 | Log-lik:    18.08 | KL:  45.78
Training complete!
Epoch 3000 | ELBO:   -27.71 | Log-lik:    18.08 | KL:  45.78
Training complete!</code></pre>
</div>
</div>
<div id="630a6aa8" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize VI training progress</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>epochs_arr <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="bu">len</span>(history[<span class="st">'elbo'</span>]) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: ELBO over time</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>ax.plot(epochs_arr, history[<span class="st">'elbo'</span>], linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Epoch'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'ELBO'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'ELBO During VI Training'</span>, fontsize<span class="op">=</span><span class="dv">13</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: ELBO components</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>ax.plot(epochs_arr, history[<span class="st">'log_lik'</span>], linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Expected log-likelihood'</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>ax.plot(epochs_arr, history[<span class="st">'kl'</span>], linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'KL divergence'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Epoch'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Value'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'ELBO Components'</span>, fontsize<span class="op">=</span><span class="dv">13</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final ELBO breakdown:"</span>)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  ELBO: </span><span class="sc">{</span>history[<span class="st">'elbo'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Expected log-likelihood: </span><span class="sc">{</span>history[<span class="st">'log_lik'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  KL divergence: </span><span class="sc">{</span>history[<span class="st">'kl'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Final ELBO breakdown:
  ELBO: -27.71
  Expected log-likelihood: 18.08
  KL divergence: 45.78</code></pre>
</div>
</div>
<div id="88678f57" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate VI posterior predictive samples</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>x_test_tensor <span class="op">=</span> torch.FloatTensor(x_test)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>vi_predictions <span class="op">=</span> []</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> vi_model(x_test_tensor, sample<span class="op">=</span><span class="va">True</span>).numpy()</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        vi_predictions.append(y_pred)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>vi_predictions <span class="op">=</span> np.array(vi_predictions)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>vi_mean <span class="op">=</span> vi_predictions.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>vi_std <span class="op">=</span> vi_predictions.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare VI vs MCMC</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">6</span>))</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: VI Posterior Predictive</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co"># VI samples</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(vi_predictions)):</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, vi_predictions[i], <span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.15</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="co"># True function and data</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True function'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="dv">2</span>, zorder<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="co"># VI mean</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, vi_mean, <span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'VI posterior mean'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Dummy line for legend</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>ax.plot([], [], <span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'VI posterior samples'</span>)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Variational Inference: Posterior Predictive'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Direct comparison VI vs MCMC</span></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a><span class="co"># MCMC samples (lighter)</span></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">min</span>(<span class="dv">50</span>, <span class="bu">len</span>(mcmc_predictions))):</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, mcmc_predictions[i], <span class="st">'orange'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a><span class="co"># VI samples (darker)</span></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, vi_predictions[i], <span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Means</span></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, mcmc_mean, <span class="st">'orange'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'MCMC mean'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, vi_mean, <span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'VI mean'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a><span class="co"># True function and data</span></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True function'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">100</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="dv">2</span>, zorder<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Comparison: VI vs MCMC'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Note the difference in training times between the MCMC Method and VI Method. The main thing we give up for this increase in speed is the</p>
</section>
</section>
<section id="possible-experiments" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="possible-experiments"><span class="header-section-number">7.6</span> Possible Experiments</h2>
<p>Now that you understand the full pipeline, you can explore below how some of the fundamental parameters of the model might affect the results:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Experiment 1: Vary Hidden Layer Size
</div>
</div>
<div class="callout-body-container callout-body">
<p>Train VI models with <code>hidden_size</code> = 2, 5, 10, 20. Observe: - How does model flexibility change? - Does uncertainty increase or decrease? - Is there a risk of overfitting with larger networks?</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Experiment 2: Effect of Training Data Size
</div>
</div>
<div class="callout-body-container callout-body">
<p>Generate datasets with <code>n_train</code> = 5, 10, 20, 50. For each: - Train the VI model - Plot posterior predictive distributions - Compare the model uncertainty in data-rich vs data-sparse regions What do you notice?</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Experiment 3: Prior Sensitivity
</div>
</div>
<div class="callout-body-container callout-body">
<p>Try different prior standard deviations: <code>prior_std</code> = 0.1, 1.0, 10.0. Observe: - How does this affect the learned posterior? - Does a strong prior (small std) regularize more? - What happens with a weak prior (large std)?</p>
</div>
</div>
<p>We can do Experiment 1 together as an example, and leave the remainder for you to do independently:</p>
<div id="8fb27e53" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Experiment 1: Vary hidden layer size</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>hidden_sizes_to_test <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>]</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>results_by_size <span class="op">=</span> {}</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Experiment 1: Effect of Hidden Layer Size"</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training VI models with different architectures..."</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> h_size <span class="kw">in</span> hidden_sizes_to_test:</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Training with hidden_size = </span><span class="sc">{</span>h_size<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize and train</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    model_exp <span class="op">=</span> BayesianNNVI(hidden_size<span class="op">=</span>h_size)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    optimizer_exp <span class="op">=</span> torch.optim.Adam(model_exp.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        optimizer_exp.zero_grad()</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        elbo, _, _ <span class="op">=</span> model_exp.elbo(x_train_tensor, y_train_tensor, </span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>                                     n_samples<span class="op">=</span><span class="dv">5</span>, noise_std<span class="op">=</span>noise_std, prior_std<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>elbo</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        optimizer_exp.step()</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate predictions</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model_exp(x_test_tensor, sample<span class="op">=</span><span class="va">True</span>).numpy()</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>            predictions.append(y_pred)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> np.array(predictions)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    results_by_size[h_size] <span class="op">=</span> {</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">'predictions'</span>: predictions,</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>        <span class="st">'mean'</span>: predictions.mean(axis<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        <span class="st">'std'</span>: predictions.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Final ELBO: </span><span class="sc">{</span>elbo<span class="sc">.</span>item()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Experiment 1: Effect of Hidden Layer Size
Training VI models with different architectures...
Training with hidden_size = 2...
  Final ELBO: -104.90
Training with hidden_size = 5...
  Final ELBO: -104.90
Training with hidden_size = 5...
  Final ELBO: -32.13
Training with hidden_size = 10...
  Final ELBO: -32.13
Training with hidden_size = 10...
  Final ELBO: -45.72
Training with hidden_size = 20...
  Final ELBO: -45.72
Training with hidden_size = 20...
  Final ELBO: -76.40
Training complete!
  Final ELBO: -76.40
Training complete!</code></pre>
</div>
</div>
<div id="95dd762f" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize results from Experiment 1</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">12</span>))</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, h_size <span class="kw">in</span> <span class="bu">enumerate</span>(hidden_sizes_to_test):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[idx]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> results_by_size[h_size]</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot posterior samples</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">min</span>(<span class="dv">50</span>, <span class="bu">len</span>(result[<span class="st">'predictions'</span>]))):</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        ax.plot(x_test, result[<span class="st">'predictions'</span>][i], <span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot mean and uncertainty bands</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, result[<span class="st">'mean'</span>], <span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'VI posterior mean'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    ax.fill_between(x_test, </span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>                     result[<span class="st">'mean'</span>] <span class="op">-</span> result[<span class="st">'std'</span>], </span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>                     result[<span class="st">'mean'</span>] <span class="op">+</span> result[<span class="st">'std'</span>],</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>                     alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'±1sigma'</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># True function and data</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_test, y_test_true, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'True function'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    ax.scatter(x_train, y_train, s<span class="op">=</span><span class="dv">80</span>, c<span class="op">=</span><span class="st">'red'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, </span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>               linewidth<span class="op">=</span><span class="fl">1.5</span>, zorder<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Input $x$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Output $y$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f'Hidden Size = </span><span class="sc">{</span>h_size<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>(h_size<span class="op">+</span><span class="dv">1</span>) <span class="op">+</span> h_size<span class="sc">}</span><span class="ss"> parameters)'</span>, </span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>                 fontsize<span class="op">=</span><span class="dv">13</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>    ax.legend(fontsize<span class="op">=</span><span class="dv">10</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim([<span class="op">-</span><span class="fl">3.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">'Experiment 1: Effect of Network Capacity'</span>, fontsize<span class="op">=</span><span class="dv">15</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, y<span class="op">=</span><span class="fl">0.995</span>)</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="introduction_to_inference_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="summary-and-the-bridge-to-vaes" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="summary-and-the-bridge-to-vaes"><span class="header-section-number">7.7</span> Summary and the bridge to VAEs</h2>
<p>We saw a few key takeaways in this notebook:</p>
<section id="bayesian-inference-quantifies-uncertainty" class="level3" data-number="7.7.1">
<h3 data-number="7.7.1" class="anchored" data-anchor-id="bayesian-inference-quantifies-uncertainty"><span class="header-section-number">7.7.1</span> 1. <strong>Bayesian Inference Quantifies Uncertainty</strong></h3>
<ul>
<li>Instead of a single “best” model, we maintain a distribution over models</li>
<li>The <strong>posterior predictive distribution</strong> <span class="math inline">\(p(y_* | x_*, X, Y)\)</span> gives us predictions <strong>with uncertainty</strong></li>
</ul>
</section>
<section id="linear-models-have-limitations" class="level3" data-number="7.7.2">
<h3 data-number="7.7.2" class="anchored" data-anchor-id="linear-models-have-limitations"><span class="header-section-number">7.7.2</span> 2. <strong>Linear Models Have Limitations</strong></h3>
<ul>
<li><strong>Bayesian linear regression</strong> has exact, closed-form posteriors (Gaussian)</li>
<li>This allows us to compute predictive mean and variance analytically</li>
<li>However, they cannot capture nonlinear relationships, as we saw in this example</li>
</ul>
</section>
<section id="neural-networks-provide-flexibility-at-the-cost-of-an-intractable-posterior" class="level3" data-number="7.7.3">
<h3 data-number="7.7.3" class="anchored" data-anchor-id="neural-networks-provide-flexibility-at-the-cost-of-an-intractable-posterior"><span class="header-section-number">7.7.3</span> 3. <strong>Neural Networks Provide Flexibility at the cost of an intractable posterior</strong></h3>
<ul>
<li>Even a small 1-hidden-layer network can approximate complex functions</li>
<li><strong>Prior predictive</strong> shows diverse possible functions before seeing data</li>
<li><strong>Challenge</strong>: Posterior <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span> is no longer tractable.</li>
</ul>
</section>
<section id="mcmc-accurate-but-slow" class="level3" data-number="7.7.4">
<h3 data-number="7.7.4" class="anchored" data-anchor-id="mcmc-accurate-but-slow"><span class="header-section-number">7.7.4</span> 4. <strong>MCMC: Accurate but Slow</strong></h3>
<ul>
<li>Markov Chain Monte Carlo samples from the true posterior</li>
<li><strong>Pros</strong>: Asymptotically exact, unbiased</li>
<li><strong>Cons</strong>: Computationally expensive, slow for high-dimensional models</li>
<li>Useful as a <strong>reference</strong> for validation</li>
</ul>
</section>
<section id="variational-inference-fast-approximation" class="level3" data-number="7.7.5">
<h3 data-number="7.7.5" class="anchored" data-anchor-id="variational-inference-fast-approximation"><span class="header-section-number">7.7.5</span> 5. <strong>Variational Inference: Fast Approximation</strong></h3>
<ul>
<li>Approximate <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span> with a tractable family <span class="math inline">\(q_\phi(\mathbf{w})\)</span></li>
<li>Optimize <span class="math inline">\(\phi\)</span> to maximize the <strong>ELBO</strong>: <span class="math display">\[\mathcal{L}(\phi) = \mathbb{E}_{q_\phi}\left[\log p(Y | X, \mathbf{w})\right] - \text{KL}(q_\phi(\mathbf{w}) || p(\mathbf{w}))\]</span></li>
<li><strong>Reparameterization trick</strong> enables low-variance gradient estimates</li>
<li><strong>Result</strong>: Posterior approximation in seconds instead of minutes</li>
</ul>
</section>
<section id="function-space-vs-weight-space" class="level3" data-number="7.7.6">
<h3 data-number="7.7.6" class="anchored" data-anchor-id="function-space-vs-weight-space"><span class="header-section-number">7.7.6</span> 6. <strong>Function Space vs Weight Space</strong></h3>
<ul>
<li>We don’t visualize high-dimensional weight distributions directly</li>
<li>Instead, we visualize <strong>functions</strong> sampled from the posterior</li>
<li><strong>Posterior predictive</strong> is what matters for making predictions!</li>
</ul>
</section>
<section id="looking-forward-to-vaes-variational-autoencoders" class="level3" data-number="7.7.7">
<h3 data-number="7.7.7" class="anchored" data-anchor-id="looking-forward-to-vaes-variational-autoencoders"><span class="header-section-number">7.7.7</span> Looking forward to VAEs (Variational Autoencoders)</h3>
<p>The VI approach you learned here is the <strong>same mathematical foundation</strong> for VAEs. The key differences are:</p>
<ul>
<li><strong>Here (Bayesian NN)</strong>: Posterior over <strong>weights</strong> <span class="math inline">\(p(\mathbf{w} | X, Y)\)</span></li>
<li><strong>VAEs</strong>: Posterior over <strong>latent variables</strong> <span class="math inline">\(p(\mathbf{z} | \mathbf{x})\)</span></li>
</ul>
<p>The ELBO, reparameterization trick, and optimization strategy are identical.</p>
<p>Moreover, in VAEs, instead of optimizing <span class="math inline">\(q_\phi(z)\)</span> separately for each datapoint <span class="math inline">\(x\)</span>, we learn an <strong>encoder network</strong> <span class="math inline">\(q_\phi(z | x)\)</span> that works for <strong>all</strong> <span class="math inline">\(x\)</span>. This is called <strong>amortized inference</strong>.</p>
</section>
<section id="stochastic-vi" class="level3" data-number="7.7.8">
<h3 data-number="7.7.8" class="anchored" data-anchor-id="stochastic-vi"><span class="header-section-number">7.7.8</span> Stochastic VI</h3>
<p>For very large datasets, we can use <strong>mini-batches</strong> and stochastic optimization, much like we did for gradient descent, to scale VI to millions of datapoints.</p>
<p><strong>Questions to Reflect on</strong></p>
<p>Think about these before moving to the next notebook:</p>
<ol type="1">
<li><p><strong>Why does VI underestimate uncertainty?</strong><br>
Hint: Is the Mean-field approximation where we assume independence between weights reasonable? When might it not be?</p></li>
<li><p><strong>When would MCMC be essential despite being slow?</strong></p></li>
<li><p><strong>Could we use a more expressive variational family?</strong><br>
We used a Mean-field Gaussian here, but are we limited to this?</p></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part1/distribution_distance.html" class="pagination-link" aria-label="Measuring Distribution Distances">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Measuring Distribution Distances</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part2/part2.html" class="pagination-link" aria-label="Model-Specific Approaches">
        <span class="nav-page-text">Model-Specific Approaches</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Machine Learning for Mechanical Engineers © 2025 by <a href="./index.qmd#sec-contributors">Mark Fuge and IDEAL Lab Contributors</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>