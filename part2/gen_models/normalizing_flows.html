<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mark Fuge">
<meta name="dcterms.date" content="2025-10-19">

<title>13&nbsp; Normalizing Flows – Machine Learning for Mechanical Engineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../references.html" rel="next">
<link href="../../part2/gen_models/VAEs.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-0dd2bd5de344125cf763a379ddc3eb04.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../part2/part2.html">Model-Specific Approaches</a></li><li class="breadcrumb-item"><a href="../../part2/gen_models/normalizing_flows.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Normalizing Flows</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Machine Learning for Mechanical Engineering</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../part1/part1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foundational Skills</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part1/reviewing_supervised_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Reviewing Supervised Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notebooks/cross_validation_linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Evaluating Machine Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notebooks/supervised_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part1/linear_decompositions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Review of Linear Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part1/taking_derivatives.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Taking Derivatives with Automatic Differentiation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part1/distribution_distance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Measuring Distribution Distances</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part1/introduction_to_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to Inference</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../part2/part2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model-Specific Approaches</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part2/review_neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Review of Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part2/gen_models/intro_to_GANS.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part2/gen_models/GAN_pitfalls.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">GAN Training Pitfalls</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part2/gen_models/OT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Optimal Transport for Generative Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part2/gen_models/VAEs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Variational Autoencoders (VAEs)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../part2/gen_models/normalizing_flows.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Normalizing Flows</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../problems/problems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../problems/ps1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Problem Set 1</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../notebooks/notebooks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">In-Class Notebooks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notebooks/california_housing_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Housing Price Data Visualization In-Class Exercise</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/helpful_tooling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Helpful Tooling for Working with and Debugging Machine Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/course_progression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Course Lecture Progression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/review_of_singular_value_decomposition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Review of Matrices and the Singular Value Decomposition</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/review_of_math_and_computing_foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Reviewing Mathematical and Computational Foundations for Machine Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">13.1</span> Learning Objectives</a></li>
  <li><a href="#vaes-vs.-normalizing-flows" id="toc-vaes-vs.-normalizing-flows" class="nav-link" data-scroll-target="#vaes-vs.-normalizing-flows"><span class="header-section-number">13.2</span> VAEs vs.&nbsp;Normalizing Flows</a></li>
  <li><a href="#key-idea-the-change-of-variables-formula" id="toc-key-idea-the-change-of-variables-formula" class="nav-link" data-scroll-target="#key-idea-the-change-of-variables-formula"><span class="header-section-number">13.3</span> Key Idea: The Change-of-Variables Formula</a>
  <ul class="collapse">
  <li><a href="#building-intuition-through-simple-transforms" id="toc-building-intuition-through-simple-transforms" class="nav-link" data-scroll-target="#building-intuition-through-simple-transforms"><span class="header-section-number">13.3.1</span> Building Intuition Through Simple Transforms</a></li>
  <li><a href="#more-complex-transforms" id="toc-more-complex-transforms" class="nav-link" data-scroll-target="#more-complex-transforms"><span class="header-section-number">13.3.2</span> More Complex Transforms</a></li>
  <li><a href="#from-transforms-to-complex-flows-and-probabilities" id="toc-from-transforms-to-complex-flows-and-probabilities" class="nav-link" data-scroll-target="#from-transforms-to-complex-flows-and-probabilities"><span class="header-section-number">13.3.3</span> From Transforms to Complex Flows and Probabilities</a></li>
  </ul></li>
  <li><a href="#constructing-a-flow-with-coupling-layers-using-realnvp" id="toc-constructing-a-flow-with-coupling-layers-using-realnvp" class="nav-link" data-scroll-target="#constructing-a-flow-with-coupling-layers-using-realnvp"><span class="header-section-number">13.4</span> Constructing a Flow with Coupling Layers using RealNVP</a>
  <ul class="collapse">
  <li><a href="#training-a-flow-on-the-ring-of-gaussians-dataset" id="toc-training-a-flow-on-the-ring-of-gaussians-dataset" class="nav-link" data-scroll-target="#training-a-flow-on-the-ring-of-gaussians-dataset"><span class="header-section-number">13.4.1</span> Training a Flow on the Ring-of-Gaussians Dataset</a></li>
  <li><a href="#monitoring-training-dynamics" id="toc-monitoring-training-dynamics" class="nav-link" data-scroll-target="#monitoring-training-dynamics"><span class="header-section-number">13.4.2</span> Monitoring Training Dynamics</a></li>
  <li><a href="#inspecting-the-learned-mapping" id="toc-inspecting-the-learned-mapping" class="nav-link" data-scroll-target="#inspecting-the-learned-mapping"><span class="header-section-number">13.4.3</span> Inspecting the Learned Mapping</a></li>
  <li><a href="#layer-by-layer-transformation" id="toc-layer-by-layer-transformation" class="nav-link" data-scroll-target="#layer-by-layer-transformation"><span class="header-section-number">13.4.4</span> Layer-by-Layer Transformation</a></li>
  </ul></li>
  <li><a href="#advantages-and-limitations-of-normalizing-flows" id="toc-advantages-and-limitations-of-normalizing-flows" class="nav-link" data-scroll-target="#advantages-and-limitations-of-normalizing-flows"><span class="header-section-number">13.5</span> Advantages and Limitations of Normalizing Flows</a></li>
  <li><a href="#summary-and-looking-ahead" id="toc-summary-and-looking-ahead" class="nav-link" data-scroll-target="#summary-and-looking-ahead"><span class="header-section-number">13.6</span> Summary and Looking Ahead</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../part2/part2.html">Model-Specific Approaches</a></li><li class="breadcrumb-item"><a href="../../part2/gen_models/normalizing_flows.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Normalizing Flows</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Normalizing Flows</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mark Fuge </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 19, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>In the previous chapter we trained a Variational Autoencoder (VAE) to learn a latent representation that could reconstruct and generate samples. VAEs rely on a probabilistic encoder-decoder pair and optimize the Evidence Lower Bound (ELBO). In this notebook we take a different perspective: we build models that exactly transform a simple base distribution (e.g., a Gaussian) into our target data distribution using a sequence of invertible mappings known as <em>normalizing flows</em>.</p>
<p>We will reuse the same 2D ring-of-Gaussians dataset that we have been using in prior notebooks so far, so that we can compare models directly and focus on the geometric intuition behind these transformations.</p>
<section id="learning-objectives" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">13.1</span> Learning Objectives</h2>
<ul>
<li>Understand how normalizing flows differ from VAEs in the way they model probability densities</li>
<li>Apply the change-of-variables formula to relate base and transformed distributions</li>
<li>Experiment with invertible transformations in 1D and 2D to build intuition</li>
<li>Implement and train a RealNVP-style normalizing flow on the ring dataset</li>
<li>Summarize the advantages and limitations of flows and motivate continuous or stochastic extensions</li>
</ul>
<div id="73ed26e9" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup and Imports</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributions <span class="im">import</span> MultivariateNormal, Normal</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> ipywidgets <span class="im">import</span> interact, FloatSlider, IntSlider</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    widgets_available <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    interact <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    FloatSlider <span class="op">=</span> IntSlider <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    widgets_available <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gen_models_utilities <span class="im">import</span> (</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    device,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    create_ring_gaussians,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    make_loader,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    compute_diversity_metric</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'seaborn-v0_8-muted'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>sns.set_context(<span class="st">'talk'</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed_all(<span class="dv">42</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Using device: cuda</code></pre>
</div>
</div>
</section>
<section id="vaes-vs.-normalizing-flows" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="vaes-vs.-normalizing-flows"><span class="header-section-number">13.2</span> VAEs vs.&nbsp;Normalizing Flows</h2>
<p>VAEs <em>approximate</em> the data likelihood by maximizing an evidence lower bound, balancing a reconstruction term with a KL regularizer on the latent posterior. Normalizing flows instead learn an <em>exactly</em> normalizable density by composing invertible transformations that map a simple base distribution to the target data distribution using the change-of-variables formula.</p>
<p>The key trade-off is between tractable posterior inference (VAE) and exact likelihood modeling with potentially sharper samples (flow). Both rely on simple base distributions, but flows <em>must</em> ensure every transformation remains invertible with a tractable Jacobian determinant.</p>
</section>
<section id="key-idea-the-change-of-variables-formula" class="level2 page-columns page-full" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="key-idea-the-change-of-variables-formula"><span class="header-section-number">13.3</span> Key Idea: The Change-of-Variables Formula</h2>
<p>Suppose we want to transform a simple base random variable <span class="math inline">\(z\)</span> with known density <span class="math inline">\(p_Z(z)\)</span> into a new variable <span class="math inline">\(x = f(z)\)</span>. If <span class="math inline">\(f\)</span> is invertible and differentiable, then the density of <span class="math inline">\(x\)</span> is given by the Change-of-Variables formula:</p>
<p><span class="math display">\[
p_X(x) = p_Z(f^{-1}(x)) \left\lvert \det \frac{\partial f^{-1}(x)}{\partial x} \right\rvert = p_Z(z) \left\lvert \det \frac{\partial f(z)}{\partial z} \right\rvert^{-1}.
\]</span></p>
<p>This formula is for a single function <span class="math inline">\(f\)</span>. In Normalizing Flows, the idea is to instead construct <span class="math inline">\(f\)</span> as a <strong>composition</strong> of many simple, easily invertible transformations, each of which can be computed using the Change-of-Variables formula above. If we can do this, then computing the log-density reduces to <strong>summing</strong> the log-determinants of the Jacobians of those transformations. Let’s ground this idea in an accessible one-dimensional example before moving to higher dimensions.</p>
<p>Specifically, we will consider first a simple tanh-based transformation, which is both invertible and differentiable. The forward function <span class="math inline">\(f\)</span> and its inverse <span class="math inline">\(f^{-1}\)</span> are given by: <span class="math display">\[
f(z) = \tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}, \quad f^{-1}(x) = \tanh^{-1}(x) = \frac{1}{2} \ln\left(\frac{1+x}{1-x}\right).
\]</span></p>
<p>Let’s see what this transformation does to a simple base distribution, such as a standard normal.</p>
<div id="7ebe4167" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1D change-of-variables example with a tanh transform</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">10_000</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>base_normal <span class="op">=</span> Normal(loc<span class="op">=</span>torch.tensor(<span class="fl">0.0</span>, device<span class="op">=</span>device), scale<span class="op">=</span>torch.tensor(<span class="fl">1.0</span>, device<span class="op">=</span>device))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># We can compute all of the below functions analytically for a given simple transformation:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_transform(u: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Smooth, monotonic transform mapping R -&gt; (-1, 1)."""</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tanh(u)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inverse_transform(y: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    clipped <span class="op">=</span> torch.clamp(y, <span class="bu">min</span><span class="op">=-</span><span class="fl">0.999_999</span>, <span class="bu">max</span><span class="op">=</span><span class="fl">0.999_999</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> torch.log((<span class="dv">1</span> <span class="op">+</span> clipped) <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> clipped))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_abs_det_jacobian(y: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>torch.log1p(<span class="op">-</span>y.<span class="bu">pow</span>(<span class="dv">2</span>))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    z_samples <span class="op">=</span> base_normal.sample((num_samples,))</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    x_samples <span class="op">=</span> forward_transform(z_samples)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    x_grid <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="fl">0.999</span>, <span class="fl">0.999</span>, <span class="dv">500</span>, device<span class="op">=</span>device)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    z_grid <span class="op">=</span> inverse_transform(x_grid)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    log_px <span class="op">=</span> base_normal.log_prob(z_grid) <span class="op">+</span> log_abs_det_jacobian(x_grid)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="7b8f6190" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">13</span>, <span class="dv">4</span>))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].hist(x_samples.cpu().numpy(), bins<span class="op">=</span><span class="dv">80</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.65</span>, color<span class="op">=</span><span class="st">'tab:blue'</span>, label<span class="op">=</span><span class="st">'Samples'</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(x_grid.cpu().numpy(), torch.exp(log_px).cpu().numpy(), color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Analytic density'</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Density after tanh transform'</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'p(x)'</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(z_samples.cpu().numpy(), x_samples.cpu().numpy(), <span class="st">'.'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, markersize<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'tab:orange'</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Mapping z → x'</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'z ~ N(0,1)'</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'x = tanh(z)'</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="normalizing_flows_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In this simple transformation, we can see how it modifies the density of a standard normal distribution, and also how the tranformation is invertible, since we can go both ways. We can draw a sample from <span class="math inline">\(z\)</span> and pass it through the forward transformation to get <span class="math inline">\(x\)</span>, and we can also take a value of <span class="math inline">\(x\)</span> and map it back to <span class="math inline">\(z\)</span> using the inverse transformation. We can see from the mapping that the transformation is indeed invertible.</p>
<section id="building-intuition-through-simple-transforms" class="level3" data-number="13.3.1">
<h3 data-number="13.3.1" class="anchored" data-anchor-id="building-intuition-through-simple-transforms"><span class="header-section-number">13.3.1</span> Building Intuition Through Simple Transforms</h3>
<p>Before we move onto more complex normalizing flows, it will help us to build geometric intuition for individual transformations. We will:</p>
<ul>
<li>Look at several common 1D functions that appear in flow architectures to understand how they bend the real line.</li>
<li>Apply those transforms to a base Gaussian and inspect how the probability density changes.</li>
<li>Explore higher-dimensional analogues (rotations, couplings, planar flows) to see how they reshape contours while keeping the transformation invertible.</li>
</ul>
<p>Keep the change-of-variables formula in mind: steep regions of a transform squeeze probability mass, while flat regions stretch it.</p>
<div id="e4a520a0" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizing common 1D flow transformations before touching the density</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>z_plot <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="fl">3.0</span>, <span class="fl">3.0</span>, <span class="dv">400</span>, device<span class="op">=</span>device)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>transform_fns <span class="op">=</span> {</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Identity"</span>: <span class="kw">lambda</span> z: z,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Affine (scale=2.5, shift=1)"</span>: <span class="kw">lambda</span> z: <span class="fl">2.5</span> <span class="op">*</span> z <span class="op">+</span> <span class="fl">1.0</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sigmoid"</span>: torch.sigmoid,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Tanh"</span>: torch.tanh,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Leaky ReLU (α=0.1)"</span>: <span class="kw">lambda</span> z: torch.where(z <span class="op">&gt;=</span> <span class="dv">0</span>, z, <span class="fl">0.1</span> <span class="op">*</span> z),</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Softplus shift"</span>: <span class="kw">lambda</span> z: F.softplus(z) <span class="op">-</span> np.log(<span class="fl">2.0</span>),</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="b904be00" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">5</span>))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, fn <span class="kw">in</span> transform_fns.items():</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    ax.plot(z_plot.cpu().numpy(), fn(z_plot).cpu().numpy(), linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span>label)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'How candidate flow transforms warp the real line'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Input z'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Transformed output f(z)'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'upper left'</span>, frameon<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="normalizing_flows_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Note how in the above all warpings are monotonic functions, ensuring invertibility. Now let’s see how they modify a simple Gaussian base density.</p>
<div id="a2b4842a" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying 1D transforms to a Gaussian base distribution</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>samples_1d <span class="op">=</span> <span class="dv">6_000</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>base_1d <span class="op">=</span> Normal(loc<span class="op">=</span>torch.tensor(<span class="fl">0.0</span>, device<span class="op">=</span>device), scale<span class="op">=</span>torch.tensor(<span class="fl">1.0</span>, device<span class="op">=</span>device))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> derivative(label: <span class="bu">str</span>, z: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> label <span class="op">==</span> <span class="st">"Identity"</span>:</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.ones_like(z)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> label.startswith(<span class="st">"Affine"</span>):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.full_like(z, <span class="fl">2.5</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> label <span class="op">==</span> <span class="st">"Sigmoid"</span>:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> torch.sigmoid(z)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> s <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> s)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> label <span class="op">==</span> <span class="st">"Tanh"</span>:</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> torch.tanh(z)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">-</span> t.<span class="bu">pow</span>(<span class="dv">2</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> label.startswith(<span class="st">"Leaky"</span>):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.where(z <span class="op">&gt;=</span> <span class="dv">0</span>, torch.ones_like(z), torch.full_like(z, <span class="fl">0.1</span>))</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> label <span class="op">==</span> <span class="st">"Softplus shift"</span>:</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(z)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Unknown transform: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    z_samples <span class="op">=</span> base_1d.sample((samples_1d,))</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    transformed_samples <span class="op">=</span> {}</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    log_abs_det <span class="op">=</span> {}</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label, fn <span class="kw">in</span> transform_fns.items():</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> fn(z_samples)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        transformed_samples[label] <span class="op">=</span> y.cpu().numpy()</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        jac <span class="op">=</span> derivative(label, z_samples)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        log_abs_det[label] <span class="op">=</span> torch.log(jac.<span class="bu">abs</span>() <span class="op">+</span> <span class="fl">1e-8</span>).cpu().numpy()</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">8</span>))</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flat</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, (label, values) <span class="kw">in</span> <span class="bu">zip</span>(axes, transformed_samples.items()):</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    ax.hist(values, bins<span class="op">=</span><span class="dv">80</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'tab:blue'</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ch">\n</span><span class="ss">⟨log|f'|⟩ = </span><span class="sc">{</span>log_abs_det[label]<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Transformed value'</span>)</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="normalizing_flows_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="more-complex-transforms" class="level3 page-columns page-full" data-number="13.3.2">
<h3 data-number="13.3.2" class="anchored" data-anchor-id="more-complex-transforms"><span class="header-section-number">13.3.2</span> More Complex Transforms</h3>
<p>OK, so far so good, and the 1D change of variables is fairly intuitive. But how do we extend this to higher dimensions? For higher dimensions, the change-of-variables formula generalizes to: <span class="math display">\[
p_X(\mathbf{x}) = p_Z(f^{-1}(\mathbf{x})) \left\lvert \det \frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}} \right\rvert = p_Z(\mathbf{z}) \left\lvert \det \frac{\partial f(\mathbf{z})}{\partial \mathbf{z}} \right\rvert^{-1}.
\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}, \mathbf{z} \in \mathbb{R}^D\)</span> are now <span class="math inline">\(D\)</span>-dimensional vectors, and the Jacobian (J) <span class="math inline">\(\frac{\partial f(\mathbf{z})}{\partial \mathbf{z}}\)</span> is a <span class="math inline">\(D \times D\)</span> matrix of partial derivatives: <span class="math display">\[
\frac{\partial f(\mathbf{z})}{\partial \mathbf{z}} = \begin{bmatrix}
\frac{\partial f_1(\mathbf{z})}{\partial z_1} &amp; \frac{\partial f_1(\mathbf{z})}{\partial z_2} &amp; \cdots &amp; \frac{\partial f_1(\mathbf{z})}{\partial z_D} \\
\frac{\partial f_2(\mathbf{z})}{\partial z_1} &amp; \frac{\partial f_2(\mathbf{z})}{\partial z_2} &amp; \cdots &amp; \frac{\partial f_2(\mathbf{z})}{\partial z_D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_D(\mathbf{z})}{\partial z_1} &amp; \frac{\partial f_D(\mathbf{z})}{\partial z_2} &amp; \cdots &amp; \frac{\partial f_D(\mathbf{z})}{\partial z_D}
\end{bmatrix}
\]</span></p>
<p>So, if we unpack the new formula, we see that the density at a point <span class="math inline">\(\mathbf{x}\)</span> depends on the density at the corresponding point <span class="math inline">\(\mathbf{z} = f^{-1}(\mathbf{x})\)</span> in the base distribution, scaled by the volume change induced by the transformation <span class="math inline">\(f\)</span> at that point, as captured by the <strong>determinant of the Jacobian</strong>. As such, in normalizing flows, most of the effort goes into designing transformations <span class="math inline">\(f\)</span> (or compositions of transformations) that are:</p>
<ol type="1">
<li><strong>Invertible</strong>: So we can compute <span class="math inline">\(f^{-1}(\mathbf{x})\)</span>.</li>
<li><strong>Tractable Jacobian Determinant</strong>: So we can efficiently compute <span class="math inline">\(\log|\det \frac{\partial f(\mathbf{z})}{\partial \mathbf{z}}|\)</span>.</li>
<li><strong>Expressive</strong>: So that the composition of multiple such transformations can model complex target distributions.</li>
</ol>
<section id="a-refresher-on-determinants" class="level4 page-columns page-full" data-number="13.3.2.1">
<h4 data-number="13.3.2.1" class="anchored" data-anchor-id="a-refresher-on-determinants"><span class="header-section-number">13.3.2.1</span> A Refresher on Determinants</h4>
<p>Before going into specific transformations or architectures, let’s briefly refresh our understanding of determinants, in general.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> We will see that many of the common transformations in Normalizing Flows are designed to take advantage of one or more of the following properties of determinants:</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;For some common matrix identities, I can recommend reviewing the <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">Matrix Cookbook</a></p></div></div><ul>
<li><strong>Product Rule</strong>: For two square matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> of the same size, <span class="math inline">\(\det(\mathbf{A}\mathbf{B}) = \det(\mathbf{A}) \cdot \det(\mathbf{B})\)</span>. This means that for a composition of functions <span class="math inline">\(f = f_2 \circ f_1\)</span>, the Jacobian determinant is the product of the individual determinants: <span class="math inline">\(\det(J_f) = \det(J_{f_2}) \cdot \det(J_{f_1})\)</span>.</li>
<li><strong>Matrix Inversion</strong>: The determinant of the inverse of a matrix is the reciprocal of the determinant: <span class="math inline">\(\det(\mathbf{A}^{-1}) = 1/\det(\mathbf{A})\)</span>. This is useful when computing the density of the transformed variable using the inverse transformation.</li>
<li><strong>Eigenvalues</strong>: The determinant of a matrix can also be computed as the product of its eigenvalues. This property is sometimes used in flow architectures that involve spectral methods or transformations based on eigen-decompositions.</li>
<li><strong>Triangular Matrices</strong>: The determinant of a triangular matrix (upper or lower) is simply the product of its diagonal elements: <span class="math inline">\(\det J = \prod_{i=1}^d \frac{\partial f_i}{\partial x_i}\)</span> and the log-determinant is just the sum of those diagonal terms. This property is often exploited in flow architectures to design transformations with triangular Jacobians, making the determinant computation efficient. This is because, for a matrix of size <span class="math inline">\(D \times D\)</span>, computing the determinant directly is <span class="math inline">\(O(D^3)\)</span>, while for triangular matrices this reduces to <span class="math inline">\(O(D)\)</span>. We can see that a diagonal matrix is a special case of a triangular matrix, as thus has the same formula for the determinant.</li>
<li><strong>Block Matrices</strong>: For block diagonal matrices, the determinant is the product of the determinants of the blocks. This allows for designing transformations that operate on subsets of dimensions independently, simplifying the Jacobian structure.</li>
<li><strong>Orthogonal Matrices</strong>: For orthogonal matrices (where <span class="math inline">\(\mathbf{A}^T\mathbf{A} = \mathbf{I}\)</span>), the determinant is either +1 or -1. This property is useful in designing volume-preserving transformations, since we can make sure that the determinant of the Jacobian is one, and thus simplify density computations or ensure that volume doesn’t change. This is also useful in special matrices like Permutation matrices, which are orthogonal, and we will see later are useful for shuffling dimensions in coupling layers.</li>
<li><strong>Volume Preserving Matrices</strong>: Some transformations are designed to be volume-preserving, meaning that they do not change the volume of regions in space. For such transformations, the determinant of the Jacobian is one. These will have important analogues to continuous flows which we will see in later noterbooks, where this property corresponds to divergence-free vector fields (<span class="math inline">\(\nabla \cdot f = 0\)</span>).</li>
<li><strong>Low-Rank Updates</strong>: For matrices that can be expressed as a low-rank update to another matrix, the determinant can be computed using the <a href="https://en.wikipedia.org/wiki/Matrix_determinant_lemma">matrix determinant lemma</a>. This is particularly useful in certain flow architectures that involve low-rank transformations, such as planar flows. Specifically, if <span class="math inline">\(\mathbf{A}\)</span> is an invertible matrix and <span class="math inline">\(\mathbf{u}, \mathbf{v}\)</span> are column vectors, then: $ ( + ^T) = () (1 + ^T ^{-1} )$. In the special case where <span class="math inline">\(\mathbf{A}\)</span> is the identity matrix, this simplifies to: <span class="math inline">\(\det(\mathbf{I} + \mathbf{u}\mathbf{v}^T) = 1 + \mathbf{v}^T \mathbf{u}\)</span>.</li>
</ul>
<p>With these properties in mind, we can now explore some common transformations used in normalizing flows.</p>
</section>
<section id="affine-transforms" class="level4" data-number="13.3.2.2">
<h4 data-number="13.3.2.2" class="anchored" data-anchor-id="affine-transforms"><span class="header-section-number">13.3.2.2</span> Affine Transforms</h4>
<p>These are the simplest layers, applying a linear transformation followed by a shift: <span class="math inline">\(f(\mathbf{z}) = \mathbf{A}\mathbf{z} + \mathbf{b}\)</span>. The Jacobian is simply <span class="math inline">\(\mathbf{A}\)</span>, so its log-determinant is <span class="math inline">\(\log|\det \mathbf{A}|\)</span>. If <span class="math inline">\(\mathbf{A}\)</span> is a rotation matrix, the transform is volume-preserving (<span class="math inline">\(\log|\det|=0\)</span>). If <span class="math inline">\(\mathbf{A}\)</span> is diagonal, it applies axis-aligned scaling.</p>
</section>
<section id="planar-flows" class="level4" data-number="13.3.2.3">
<h4 data-number="13.3.2.3" class="anchored" data-anchor-id="planar-flows"><span class="header-section-number">13.3.2.3</span> Planar Flows</h4>
<p>These layers stretch and compress the space along a specific direction (a hyperplane). The transformation is <span class="math inline">\(f(\mathbf{z}) = \mathbf{z} + \mathbf{u}h(\mathbf{w}^T\mathbf{z} + b)\)</span>, where <span class="math inline">\(\mathbf{u}, \mathbf{w}\)</span> are vectors, <span class="math inline">\(b\)</span> is a scalar, and <span class="math inline">\(h\)</span> is a smooth nonlinearity like <span class="math inline">\(\tanh\)</span>. This creates contractions or expansions perpendicular to the hyperplane <span class="math inline">\(\mathbf{w}^T\mathbf{z} + b=0\)</span>. Because this is a low-rank update, the log-determinant can be efficiently computed as <span class="math inline">\(\log|1 + \mathbf{u}^T\mathbf{v}(\mathbf{z})|\)</span>, where <span class="math inline">\(\mathbf{v}(\mathbf{z}) = h'(\mathbf{w}^T\mathbf{z}+b)\mathbf{w}\)</span>.</p>
</section>
<section id="radial-flows" class="level4" data-number="13.3.2.4">
<h4 data-number="13.3.2.4" class="anchored" data-anchor-id="radial-flows"><span class="header-section-number">13.3.2.4</span> Radial Flows</h4>
<p>These layers modify the space around a reference point <span class="math inline">\(\mathbf{z}_0\)</span>: <span class="math inline">\(f(\mathbf{z}) = \mathbf{z} + \beta \frac{\mathbf{z}-\mathbf{z}_0}{\alpha+r}\)</span>, where <span class="math inline">\(r=||\mathbf{z}-\mathbf{z}_0||\)</span> is a radius from the reference point and <span class="math inline">\(\alpha &gt; 0, \beta \in \R\)</span> are fixed or learned scalars parameterizing the function. This can create or fill holes in the distribution around <span class="math inline">\(\mathbf{z}_0\)</span> by pushing or pulling points in <span class="math inline">\(\mathbf{z}\)</span> toward/away from <span class="math inline">\(\mathbf{z}_0\)</span>. Its Jacobian is given by: <span class="math display">\[
\frac{\partial f(\mathbf{z})}{\partial \mathbf{z}} = \mathbf{I} + h(r)\mathbf{I} + h'(r) \frac{(\mathbf{z}-\mathbf{z}_0)(\mathbf{z}-\mathbf{z}_0)^T}{r}
\]</span></p>
<p>where <span class="math inline">\(h(r) = \frac{\beta}{\alpha + r}\)</span> and <span class="math inline">\(h'(r) = -\frac{\beta}{(\alpha + r)^2}\)</span>. As in planar flows, this is a low-rank update, so we can use the matrix determinant lemma to compute the determinant efficiently: <span class="math display">\[
\det \frac{\partial f(\mathbf{z})}{\partial \mathbf{z}} = (1 + h(r))^{D-1} (1 + h(r) + h'(r) r) = \left(1 + \frac{\beta}{\alpha + r}\right)^{D-1} \left(1 + \frac{\beta \alpha}{(\alpha + r)^2}\right)
\]</span></p>
</section>
<section id="affine-coupling-layers" class="level4" data-number="13.3.2.5">
<h4 data-number="13.3.2.5" class="anchored" data-anchor-id="affine-coupling-layers"><span class="header-section-number">13.3.2.5</span> Affine Coupling Layers</h4>
<p>We are not only restricted to transforming the entire base distribution in one function. We can also split the base distribution into two parts, <span class="math inline">\(\mathbf{z}_1\)</span> and <span class="math inline">\(\mathbf{z}_2\)</span>. One part is used to parameterize an affine transformation on the other:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y}_1 &amp;= \mathbf{z}_1 \\
\mathbf{y}_2 &amp;= \mathbf{z}_2 \odot \exp(s(\mathbf{z}_1)) + t(\mathbf{z}_1)
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(s\)</span> (scale) and <span class="math inline">\(t\)</span> (translate) are neural networks. Let’s assume for this simple example that <span class="math inline">\(\mathbf{z}_1\)</span> and <span class="math inline">\(\mathbf{z}_2\)</span> are each half of the dimensions of <span class="math inline">\(\mathbf{z}\)</span>. The inverse transformation is straightforward: <span class="math display">\[
\begin{aligned}
\mathbf{z}_1 &amp;= \mathbf{y}_1 \\
\mathbf{z}_2 &amp;= (\mathbf{y}_2 - t(\mathbf{y}_1)) \odot \exp(-s(\mathbf{y}_1))
\end{aligned}
\]</span></p>
<p>We can also see that this approach endowed the Jacobian with a triangular structure: <span class="math display">\[
\frac{\partial \mathbf{y}}{\partial \mathbf{z}} = \begin{bmatrix}
\mathbf{I} &amp; \mathbf{0} \\
\frac{\partial \mathbf{y}_2}{\partial \mathbf{z}_1} &amp; \text{diag}(\exp(s(\mathbf{z}_1)))
\end{bmatrix}
\]</span></p>
<p>Since the Jacobian is triangular, this makes its determinant just the product of the diagonal elements: <span class="math inline">\(\sum_j s(\mathbf{z}_1)_j\)</span>. This is expressive, efficient, and easily invertible. Of course the cost of doing this is that we can only transform half of the dimensions in one layer, leaving the rest unchanged. To address this, we can stack multiple coupling layers, alternating which half of the dimensions are transformed in each layer. Additionally, we can add permutation layers (which are orthogonal and volume-preserving) between coupling layers to shuffle the dimensions, ensuring that all dimensions get transformed over multiple layers.</p>
<div id="5d314cd1" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exploring 2D invertible transforms that inspire flow layers</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>samples_2d <span class="op">=</span> <span class="dv">4_000</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>base_2d <span class="op">=</span> MultivariateNormal(loc<span class="op">=</span>torch.zeros(<span class="dv">2</span>), covariance_matrix<span class="op">=</span>torch.eye(<span class="dv">2</span>))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    z2 <span class="op">=</span> base_2d.sample((samples_2d,))</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    rotation <span class="op">=</span> torch.tensor([[np.cos(np.deg2rad(<span class="fl">45.0</span>)), <span class="op">-</span>np.sin(np.deg2rad(<span class="fl">45.0</span>))],</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>                               [np.sin(np.deg2rad(<span class="fl">45.0</span>)), np.cos(np.deg2rad(<span class="fl">45.0</span>))]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> torch.tensor([[<span class="fl">0.35</span>, <span class="fl">0.0</span>],[<span class="fl">0.0</span>, <span class="fl">2.0</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    shear <span class="op">=</span> torch.tensor([[<span class="fl">1.0</span>, <span class="fl">0.6</span>],[<span class="fl">0.0</span>, <span class="fl">1.0</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> torch.tensor([<span class="fl">0.6</span>, <span class="op">-</span><span class="fl">0.4</span>])</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    u <span class="op">=</span> torch.tensor([<span class="fl">0.4</span>, <span class="fl">0.5</span>])</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> torch.tensor(<span class="fl">0.1</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    z0 <span class="op">=</span> torch.tensor([<span class="fl">1.2</span>, <span class="op">-</span><span class="fl">0.8</span>])</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> torch.tensor(<span class="fl">1.0</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> torch.tensor(<span class="fl">0.8</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> planar_flow(x: torch.Tensor) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        linear <span class="op">=</span> x <span class="op">@</span> w <span class="op">+</span> b</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> torch.tanh(linear)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        psi <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> torch.tanh(linear).<span class="bu">pow</span>(<span class="dv">2</span>)).unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> w</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        det_term <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> (psi <span class="op">*</span> u).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> x <span class="op">+</span> u <span class="op">*</span> h.unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y, det_term</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> radial_flow(x: torch.Tensor) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        diff <span class="op">=</span> x <span class="op">-</span> z0</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> diff.norm(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> <span class="fl">1e-6</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (alpha <span class="op">+</span> r)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        h_prime <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">/</span> (alpha <span class="op">+</span> r).<span class="bu">pow</span>(<span class="dv">2</span>)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> x <span class="op">+</span> beta <span class="op">*</span> h <span class="op">*</span> diff</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        det_term <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> beta <span class="op">*</span> h.squeeze()) <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> beta <span class="op">*</span> h.squeeze() <span class="op">+</span> beta <span class="op">*</span> h_prime.squeeze() <span class="op">*</span> r.squeeze())</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y, det_term</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> affine_coupling(x: torch.Tensor) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        x1 <span class="op">=</span> x[:, :<span class="dv">1</span>]</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        x2 <span class="op">=</span> x[:, <span class="dv">1</span>:]</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> <span class="fl">0.75</span> <span class="op">*</span> torch.tanh(x1)</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> torch.sin(x1)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        y1 <span class="op">=</span> x1</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        y2 <span class="op">=</span> x2 <span class="op">*</span> torch.exp(s) <span class="op">+</span> t</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> torch.cat([y1, y2], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>        det_term <span class="op">=</span> torch.exp(s.squeeze())</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y, det_term</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    transforms_2d <span class="op">=</span> {}</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    log_abs_det_2d <span class="op">=</span> {}</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>    y_rot <span class="op">=</span> z2 <span class="op">@</span> rotation.T</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    transforms_2d[<span class="st">"Rotation 45°"</span>] <span class="op">=</span> y_rot.numpy()</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    log_abs_det_2d[<span class="st">"Rotation 45°"</span>] <span class="op">=</span> np.zeros(samples_2d)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>    y_scale <span class="op">=</span> z2 <span class="op">@</span> scale.T</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    transforms_2d[<span class="st">"Anisotropic scaling"</span>] <span class="op">=</span> y_scale.numpy()</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>    log_abs_det_2d[<span class="st">"Anisotropic scaling"</span>] <span class="op">=</span> np.full(samples_2d, np.log(<span class="bu">abs</span>(torch.det(scale).item())))</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>    y_shear <span class="op">=</span> z2 <span class="op">@</span> shear.T</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>    transforms_2d[<span class="st">"Shear"</span>] <span class="op">=</span> y_shear.numpy()</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>    log_abs_det_2d[<span class="st">"Shear"</span>] <span class="op">=</span> np.zeros(samples_2d)</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>    y_planar, det_planar <span class="op">=</span> planar_flow(z2)</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>    transforms_2d[<span class="st">"Planar flow"</span>] <span class="op">=</span> y_planar.numpy()</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>    log_abs_det_2d[<span class="st">"Planar flow"</span>] <span class="op">=</span> torch.log(det_planar.<span class="bu">abs</span>() <span class="op">+</span> <span class="fl">1e-8</span>).numpy()</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>    y_radial, det_radial <span class="op">=</span> radial_flow(z2)</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>    transforms_2d[<span class="st">"Radial flow"</span>] <span class="op">=</span> y_radial.numpy()</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>    log_abs_det_2d[<span class="st">"Radial flow"</span>] <span class="op">=</span> torch.log(det_radial.<span class="bu">abs</span>() <span class="op">+</span> <span class="fl">1e-8</span>).numpy()</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>    y_coupling, det_coupling <span class="op">=</span> affine_coupling(z2)</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>    transforms_2d[<span class="st">"Affine coupling"</span>] <span class="op">=</span> y_coupling.numpy()</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>    log_abs_det_2d[<span class="st">"Affine coupling"</span>] <span class="op">=</span> torch.log(det_coupling.<span class="bu">abs</span>() <span class="op">+</span> <span class="fl">1e-8</span>).numpy()</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">10</span>))</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flat</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, (label, points) <span class="kw">in</span> <span class="bu">zip</span>(axes, transforms_2d.items()):</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>    ax.scatter(points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">6</span>, alpha<span class="op">=</span><span class="fl">0.35</span>)</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ch">\n</span><span class="ss">⟨log|det J|⟩ = </span><span class="sc">{</span>log_abs_det_2d[label]<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'$x_1$'</span>)</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'$x_2$'</span>)</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="normalizing_flows_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="from-transforms-to-complex-flows-and-probabilities" class="level3" data-number="13.3.3">
<h3 data-number="13.3.3" class="anchored" data-anchor-id="from-transforms-to-complex-flows-and-probabilities"><span class="header-section-number">13.3.3</span> From Transforms to Complex Flows and Probabilities</h3>
<p>In practice, we can stack many such layers of different types to build a deep normalizing flow that can model complex target distributions. Each layer contributes to the overall transformation, and we can compute the log-density of the transformed variable by summing the log-determinants of each layer’s Jacobian. Each transform above comes with a Jacobian determinant that tells us how probability mass re-scales. In one dimension, the recipe is <span class="math display">\[
\log p_Y(y) = \log p_Z(f^{-1}(y)) + \log\left|\frac{\mathrm{d} f^{-1}(y)}{\mathrm{d} y}\right| = \log p_Z(z) - \log\left|f'(z)\right|.
\]</span> For multivariate flows with <span class="math inline">\(x = f(z)\)</span>, we track the log-determinant at every layer: <span class="math display">\[
\log p_X(x) = \log p_Z(z_0) + \sum_{\ell=1}^K \log\left|\det J_{f_\ell}(z_{\ell-1})\right|,
\]</span> these terms is why we prefer architectures (like affine coupling, planar, or radial flows) with Jacobians that are either triangular or have closed-form determinants, so that we can avoid very expensive determinant calculations. When training a flow we maximise this exact log-likelihood, and during sampling we simply draw <span class="math inline">\(z \sim p_Z\)</span> and apply the inverses <span class="math inline">\(f_\ell^{-1}\)</span> in reverse order.</p>
</section>
</section>
<section id="constructing-a-flow-with-coupling-layers-using-realnvp" class="level2" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="constructing-a-flow-with-coupling-layers-using-realnvp"><span class="header-section-number">13.4</span> Constructing a Flow with Coupling Layers using RealNVP</h2>
<p>One of the first models for building tractable flows was the RealNVP (Real-valued Non-Volume Preserving) architecture. It splits the vector into two parts, using a coupling layer as we saw previously. This leaves one part unchanged, and uses it to condition an affine transformation on the other part. Because the transformation is triangular, the Jacobian determinant is the product of diagonal entries and is therefore easy to compute. Below implements a RealNVP flow using coupling layers where we swap in each layer which one of the two input dimensions is fixed. For a larger or more complex example with more inputs, these could be selected randomly using permutations, but our simple approach below is just to illustrate the concept.</p>
<div id="f2c7c67a" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AffineCoupling(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""RealNVP-style affine coupling layer for 2D inputs."""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim: <span class="bu">int</span>, hidden_dim: <span class="bu">int</span>, mask: torch.Tensor):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dim <span class="op">=</span> dim</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'mask'</span>, mask)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale_net <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, hidden_dim),</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_dim, hidden_dim),</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_dim, dim)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.translate_net <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, hidden_dim),</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_dim, hidden_dim),</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_dim, dim)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        nn.init.zeros_(<span class="va">self</span>.scale_net[<span class="op">-</span><span class="dv">1</span>].weight)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        nn.init.zeros_(<span class="va">self</span>.scale_net[<span class="op">-</span><span class="dv">1</span>].bias)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        nn.init.zeros_(<span class="va">self</span>.translate_net[<span class="op">-</span><span class="dv">1</span>].weight)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        nn.init.zeros_(<span class="va">self</span>.translate_net[<span class="op">-</span><span class="dv">1</span>].bias)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        x_masked <span class="op">=</span> x <span class="op">*</span> <span class="va">self</span>.mask</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> <span class="va">self</span>.scale_net(x_masked) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> <span class="va">self</span>.translate_net(x_masked) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> torch.tanh(s)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> x_masked <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask) <span class="op">*</span> (x <span class="op">*</span> torch.exp(s) <span class="op">+</span> t)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        log_det <span class="op">=</span> ((<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask) <span class="op">*</span> s).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y, log_det</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> inverse(<span class="va">self</span>, y: torch.Tensor) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        y_masked <span class="op">=</span> y <span class="op">*</span> <span class="va">self</span>.mask</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> <span class="va">self</span>.scale_net(y_masked) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> <span class="va">self</span>.translate_net(y_masked) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> torch.tanh(s)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> y_masked <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask) <span class="op">*</span> ((y <span class="op">-</span> t) <span class="op">*</span> torch.exp(<span class="op">-</span>s))</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>        log_det <span class="op">=</span> <span class="op">-</span>((<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask) <span class="op">*</span> s).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, log_det</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RealNVP(nn.Module):</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Stack of affine coupling layers implementing a normalizing flow."""</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>, hidden_dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">128</span>, n_layers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">6</span>):</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>        masks: List[torch.Tensor] <span class="op">=</span> []</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_layers):</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>                mask <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>                mask <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>            masks.append(mask)</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>            AffineCoupling(dim<span class="op">=</span>dim, hidden_dim<span class="op">=</span>hidden_dim, mask<span class="op">=</span>mask)</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> mask <span class="kw">in</span> masks</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>        log_det_total <span class="op">=</span> torch.zeros(x.shape[<span class="dv">0</span>], device<span class="op">=</span>x.device)</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> x</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>            z, log_det <span class="op">=</span> layer(z)</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Note here how we can just accumulate log-determinants forward:</span></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>            log_det_total <span class="op">=</span> log_det_total <span class="op">+</span> log_det</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, log_det_total</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> inverse(<span class="va">self</span>, z: torch.Tensor) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>        log_det_total <span class="op">=</span> torch.zeros(z.shape[<span class="dv">0</span>], device<span class="op">=</span>z.device)</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> z</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">reversed</span>(<span class="va">self</span>.layers):</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>            x, log_det <span class="op">=</span> layer.inverse(x)</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Likewise, here we can accumulate log-determinants backward:</span></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>            log_det_total <span class="op">=</span> log_det_total <span class="op">+</span> log_det</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, log_det_total</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> inverse_with_intermediates(<span class="va">self</span>, z: torch.Tensor) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Inverse transformation that also returns intermediate layer outputs."""</span></span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>        log_det_total <span class="op">=</span> torch.zeros(z.shape[<span class="dv">0</span>], device<span class="op">=</span>z.device)</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> z</span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>        intermediates <span class="op">=</span> [z.clone()]</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">reversed</span>(<span class="va">self</span>.layers):</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a>            x, log_det <span class="op">=</span> layer.inverse(x)</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>            log_det_total <span class="op">=</span> log_det_total <span class="op">+</span> log_det</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a>            intermediates.append(x.clone())</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, log_det_total, intermediates</span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_prob(<span class="va">self</span>, x: torch.Tensor, base_dist: MultivariateNormal) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a>        z, log_det <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a>        log_prob_base <span class="op">=</span> base_dist.log_prob(z)</span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Now we can just combine base log-probability and log-determinant, which we accumulated earlier</span></span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> log_prob_base <span class="op">+</span> log_det</span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, num_samples: <span class="bu">int</span>, base_dist: MultivariateNormal) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> base_dist.sample((num_samples,))</span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a>            x, _ <span class="op">=</span> <span class="va">self</span>.inverse(z)</span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="training-a-flow-on-the-ring-of-gaussians-dataset" class="level3" data-number="13.4.1">
<h3 data-number="13.4.1" class="anchored" data-anchor-id="training-a-flow-on-the-ring-of-gaussians-dataset"><span class="header-section-number">13.4.1</span> Training a Flow on the Ring-of-Gaussians Dataset</h3>
<p>We now train a small RealNVP model on the same ring dataset used in the VAE notebook. Unlike the VAE, the flow optimizes the exact log-likelihood of the data, so we do not need a separate encoder or a variational bound. The key ingredients are:</p>
<ol type="1">
<li>A base density <span class="math inline">\(p_0(z)\)</span> (we use a standard 2D Gaussian).</li>
<li>A flow <span class="math inline">\(f_{\theta}\)</span> composed of coupling layers.</li>
<li>An optimizer to maximize <span class="math inline">\(\log p_X(x) = \log p_0(f(x)) + \log \lvert \det J_f(x) \rvert\)</span>.</li>
</ol>
<div id="66c3f953" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FlowHistory:</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    loss: List[<span class="bu">float</span>]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    log_prob: List[<span class="bu">float</span>]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    diversity: List[<span class="bu">float</span>]</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_flow_model(</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    data: np.ndarray,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    flow: RealNVP,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    base_dist: MultivariateNormal,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    epochs: <span class="bu">int</span> <span class="op">=</span> <span class="dv">300</span>,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">256</span>,</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    lr: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-3</span>,</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    print_every: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tuple[RealNVP, FlowHistory]:</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    flow.train()</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    loader <span class="op">=</span> make_loader(data, batch_size)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.Adam(flow.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> FlowHistory(loss<span class="op">=</span>[], log_prob<span class="op">=</span>[], diversity<span class="op">=</span>[])</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        batch_losses <span class="op">=</span> []</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        batch_log_probs <span class="op">=</span> []</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (batch,) <span class="kw">in</span> loader:</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>            batch <span class="op">=</span> batch.<span class="bu">float</span>().to(device)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>            log_prob <span class="op">=</span> flow.log_prob(batch, base_dist)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="op">-</span>log_prob.mean()</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>            batch_losses.append(loss.item())</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>            batch_log_probs.append(log_prob.mean().item())</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>        mean_loss <span class="op">=</span> <span class="bu">float</span>(np.mean(batch_losses))</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>        mean_log_prob <span class="op">=</span> <span class="bu">float</span>(np.mean(batch_log_probs))</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>            samples <span class="op">=</span> flow.sample(<span class="dv">2048</span>, base_dist)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>            diversity <span class="op">=</span> compute_diversity_metric(samples)</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>        history.loss.append(mean_loss)</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>        history.log_prob.append(mean_log_prob)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>        history.diversity.append(diversity)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> print_every <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> epoch <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">:03d}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss"> | Loss: </span><span class="sc">{</span>mean_loss<span class="sc">:.3f}</span><span class="ss"> | Log p(x): </span><span class="sc">{</span>mean_log_prob<span class="sc">:.3f}</span><span class="ss"> | Div: </span><span class="sc">{</span>diversity<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    flow.<span class="bu">eval</span>()</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> flow, history</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>X_ring, y_ring <span class="op">=</span> create_ring_gaussians(n_samples<span class="op">=</span><span class="dv">2000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>You can modify the below RealNVP parameters (number of layers, hidden units) and retrain the flow to see how it affects expressiveness and training time, as we mention for the experiment at the end of the notebook.</p>
<div id="2ec028c4" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the flow on the ring dataset</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>flow <span class="op">=</span> RealNVP(dim<span class="op">=</span><span class="dv">2</span>, hidden_dim<span class="op">=</span><span class="dv">128</span>, n_layers<span class="op">=</span><span class="dv">6</span>).to(device)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>base_dist <span class="op">=</span> MultivariateNormal(</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    loc<span class="op">=</span>torch.zeros(<span class="dv">2</span>, device<span class="op">=</span>device),</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    covariance_matrix<span class="op">=</span>torch.eye(<span class="dv">2</span>, device<span class="op">=</span>device)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>flow, flow_history <span class="op">=</span> train_flow_model(</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>X_ring,</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    flow<span class="op">=</span>flow,</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    base_dist<span class="op">=</span>base_dist,</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">300</span>,</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">1e-3</span>,</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    print_every<span class="op">=</span><span class="dv">50</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 002/300 | Loss: 4.634 | Log p(x): -4.634 | Div: 15.833
Epoch 050/300 | Loss: 2.800 | Log p(x): -2.800 | Div: 4.141
Epoch 100/300 | Loss: 2.591 | Log p(x): -2.591 | Div: 4.106
Epoch 150/300 | Loss: 2.430 | Log p(x): -2.430 | Div: 4.275
Epoch 200/300 | Loss: 2.490 | Log p(x): -2.490 | Div: 4.298
Epoch 250/300 | Loss: 2.383 | Log p(x): -2.383 | Div: 4.308
Epoch 300/300 | Loss: 2.401 | Log p(x): -2.401 | Div: 4.274</code></pre>
</div>
</div>
</section>
<section id="monitoring-training-dynamics" class="level3" data-number="13.4.2">
<h3 data-number="13.4.2" class="anchored" data-anchor-id="monitoring-training-dynamics"><span class="header-section-number">13.4.2</span> Monitoring Training Dynamics</h3>
<p>The total loss is the negative log-likelihood (NLL). Because we optimize the exact density, improvements in the loss correspond directly to better calibrated probabilities. We also track a simple diversity statistic computed from generated samples to monitor coverage.</p>
<div id="27c39e9d" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>epochs_axis <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="bu">len</span>(flow_history.loss) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(epochs_axis, flow_history.loss, color<span class="op">=</span><span class="st">'tab:blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Negative log-likelihood'</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yscale(<span class="st">'log'</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(epochs_axis, flow_history.log_prob, color<span class="op">=</span><span class="st">'tab:green'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Mean log p(x)'</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Log probability'</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].plot(epochs_axis, flow_history.diversity, color<span class="op">=</span><span class="st">'tab:purple'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'Sample diversity metric'</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'Variance proxy'</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="normalizing_flows_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="inspecting-the-learned-mapping" class="level3" data-number="13.4.3">
<h3 data-number="13.4.3" class="anchored" data-anchor-id="inspecting-the-learned-mapping"><span class="header-section-number">13.4.3</span> Inspecting the Learned Mapping</h3>
<p>With a trained flow we can examine three complementary views: the data space, the latent space obtained by applying the forward transformation <span class="math inline">\(f(x)\)</span>, and samples generated by pushing base noise through the inverse transformation <span class="math inline">\(f^{-1}(z)\)</span>. A well-trained flow should map the multimodal ring distribution onto a near-Gaussian latent distribution while recovering the ring geometry when sampling.</p>
<div id="23e50d40" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>flow.<span class="bu">eval</span>()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    X_tensor <span class="op">=</span> torch.from_numpy(X_ring).<span class="bu">float</span>().to(device)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    z_encoded, _ <span class="op">=</span> flow.forward(X_tensor)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    z_encoded <span class="op">=</span> z_encoded.cpu().numpy()</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    generated <span class="op">=</span> flow.sample(<span class="dv">4000</span>, base_dist).cpu().numpy()</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">5</span>))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>sc0 <span class="op">=</span> axes[<span class="dv">0</span>].scatter(X_ring[:, <span class="dv">0</span>], X_ring[:, <span class="dv">1</span>], c<span class="op">=</span>y_ring, cmap<span class="op">=</span><span class="st">'tab10'</span>, s<span class="op">=</span><span class="dv">15</span>, alpha<span class="op">=</span><span class="fl">0.65</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Original data space'</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'$x_1$'</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'$x_2$'</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axis(<span class="st">'equal'</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>plt.colorbar(sc0, ax<span class="op">=</span>axes[<span class="dv">0</span>], label<span class="op">=</span><span class="st">'Mode index'</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(z_encoded[:, <span class="dv">0</span>], z_encoded[:, <span class="dv">1</span>], c<span class="op">=</span>y_ring, cmap<span class="op">=</span><span class="st">'tab10'</span>, s<span class="op">=</span><span class="dv">15</span>, alpha<span class="op">=</span><span class="fl">0.65</span>)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Latent space after flow'</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'$z_1$'</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'$z_2$'</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axis(<span class="st">'equal'</span>)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].scatter(X_ring[:, <span class="dv">0</span>], X_ring[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'lightgray'</span>, s<span class="op">=</span><span class="dv">10</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="st">'Real'</span>)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].scatter(generated[:, <span class="dv">0</span>], generated[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'tab:red'</span>, s<span class="op">=</span><span class="dv">15</span>, alpha<span class="op">=</span><span class="fl">0.55</span>, label<span class="op">=</span><span class="st">'Generated'</span>)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'Generated samples'</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="st">'$x_1$'</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'$x_2$'</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].axis(<span class="st">'equal'</span>)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].legend()</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="normalizing_flows_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="layer-by-layer-transformation" class="level3" data-number="13.4.4">
<h3 data-number="13.4.4" class="anchored" data-anchor-id="layer-by-layer-transformation"><span class="header-section-number">13.4.4</span> Layer-by-Layer Transformation</h3>
<p>To better understand how the normalizing flow works, let’s visualize how each coupling layer progressively transforms the base distribution into the target distribution. We’ll sample from the base Gaussian and then apply each layer’s inverse transformation one at a time, coloring the points by their eventual mode assignment (determined by which ring Gaussian they end up closest to).</p>
<p>This visualization shows the geometric action of each layer, illustrating how the flow gradually warps the simple base distribution into the complex ring structure. Note in particular how the RealNVP coupling layers show up here: in the first layer only one of the dimensions is changed, while the other remains fixed. In the next layer, the roles are swapped, and so on.</p>
<div id="e952792d" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Show Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize how each layer transforms the distribution</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>flow.<span class="bu">eval</span>()</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample from base distribution</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    z_base <span class="op">=</span> base_dist.sample((<span class="dv">2000</span>,))</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get intermediate transformations</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    x_final, _, intermediates <span class="op">=</span> flow.inverse_with_intermediates(z_base)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Also encode the real data to get mode labels for coloring</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    X_tensor <span class="op">=</span> torch.from_numpy(X_ring).<span class="bu">float</span>().to(device)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    z_real, _ <span class="op">=</span> flow.forward(X_tensor)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For each base sample, find the closest real data point to determine its "mode"</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This is a simple nearest-neighbor approach for visualization</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cdist</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    distances <span class="op">=</span> cdist(z_base.cpu().numpy(), z_real.cpu().numpy())</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    nearest_indices <span class="op">=</span> distances.argmin(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> y_ring[nearest_indices]</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a grid of subplots</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>n_layers <span class="op">=</span> <span class="bu">len</span>(intermediates)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>n_cols <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>n_rows <span class="op">=</span> (n_layers <span class="op">+</span> n_cols <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> n_cols</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(n_rows, n_cols, figsize<span class="op">=</span>(<span class="dv">4</span><span class="op">*</span>n_cols, <span class="dv">4</span><span class="op">*</span>n_rows))</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten() <span class="cf">if</span> n_rows <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> [axes] <span class="cf">if</span> n_cols <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> axes</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, intermediate <span class="kw">in</span> <span class="bu">enumerate</span>(intermediates):</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[i]</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> intermediate.cpu().numpy()</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    ax.scatter(points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>], c<span class="op">=</span>colors, cmap<span class="op">=</span><span class="st">'tab10'</span>, s<span class="op">=</span><span class="dv">8</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="st">'Base Distribution</span><span class="ch">\n</span><span class="st">(Latent Space)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="bu">len</span>(intermediates) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f'After Layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ch">\n</span><span class="ss">(Final Data Space)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f'After Layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'$x_1$'</span>)</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'$x_2$'</span>)</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Hide any unused subplots</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_layers, <span class="bu">len</span>(axes)):</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>    axes[i].axis(<span class="st">'off'</span>)</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">'Progressive Transformation Through RealNVP Layers'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, y<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="normalizing_flows_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Experiment: Effect of Flow Depth and Width
</div>
</div>
<div class="callout-body-container callout-body">
<p>Go back up to where we defined the RealNVP architecture and try it again with the following changes and note the behavior that you see:</p>
<ul>
<li>Vary the number of coupling layers (e.g., 2, 4, 8, 16). What happens to the quality of the learned distribution and the training time?</li>
<li>What does varying the number of hidden units in the scale and translate networks do to the expressiveness and stability of training?</li>
<li>As you make the number of coupling layers extremely large (50,100), what do you notice about the training dynamics as well as the effect of individual layers on the movement of the distribution?</li>
</ul>
</div>
</div>
</section>
</section>
<section id="advantages-and-limitations-of-normalizing-flows" class="level2" data-number="13.5">
<h2 data-number="13.5" class="anchored" data-anchor-id="advantages-and-limitations-of-normalizing-flows"><span class="header-section-number">13.5</span> Advantages and Limitations of Normalizing Flows</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 45%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th>Strengths</th>
<th>Limitations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Exact log-likelihood evaluation enables principled model comparison</td>
<td>Computing Jacobians can be expensive for high-dimensional data</td>
</tr>
<tr class="even">
<td>Invertible architecture provides bidirectional mapping without a separate encoder</td>
<td>Designing expressive yet tractable transforms is challenging</td>
</tr>
<tr class="odd">
<td>Amenable to gradient-based training with standard optimizers</td>
<td>Coupling layers may struggle with highly non-local dependencies</td>
</tr>
<tr class="even">
<td>Samples are sharp because no explicit reconstruction loss is used</td>
<td>Memory footprint grows with the number of layers</td>
</tr>
</tbody>
</table>
<p>Normalizing flows shine when calibrated densities and invertible mappings are required, such as uncertainty-aware control or anomaly detection in sensor networks. However, they can become cumbersome in very high dimensions or when local affine transformations are insufficient.</p>
</section>
<section id="summary-and-looking-ahead" class="level2" data-number="13.6">
<h2 data-number="13.6" class="anchored" data-anchor-id="summary-and-looking-ahead"><span class="header-section-number">13.6</span> Summary and Looking Ahead</h2>
<ul>
<li>Normalizing flows learn an exact, differentiable transformation between a base distribution and the data, turning density modeling into a sequence of Jacobian adjustments.</li>
<li>The change-of-variables formula provides the theoretical backbone, allowing us to evaluate log-likelihoods exactly.</li>
<li>Coupling layers (as in RealNVP) offer an efficient way to construct expressive flows with tractable inverses and determinants.</li>
<li>On the ring dataset, flows match the multimodal structure while keeping a simple latent Gaussian distribution.</li>
</ul>
<p>If you ran the last experiment, you may have noticed that as we increase the number of layers, the individual transformations become smaller and more incremental. What would happen if we set the number of layers to infinity? In this case, each individual transformation would become an almost infinitesimal transformation from one probability distribution to another, and the integral over all of those many transformations would get us the flow from <span class="math inline">\(z\)</span> to <span class="math inline">\(x\)</span>. This is exactly the idea behind <em>continuous normalizing flows</em> or neural ODE-based flows, where the model evolves samples through a learned vector field. Introducing stochastic dynamics yields <em>stochastic flows</em> that blend diffusion models with flow-based ideas. These extensions retain the change-of-variables logic while trading discrete layers for continuous-time dynamics, setting the stage for the next chapter.</p>


</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../part2/gen_models/VAEs.html" class="pagination-link" aria-label="Variational Autoencoders (VAEs)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Variational Autoencoders (VAEs)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Machine Learning for Mechanical Engineers © 2025 by <a href="./index.qmd#sec-contributors">Mark Fuge and IDEAL Lab Contributors</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>