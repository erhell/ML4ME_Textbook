[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Mechanical Engineering",
    "section": "",
    "text": "Preface\nThis is an open textbook to accompany my course notes for “Machine Learning for Mechanical Engineering” at ETH Zürich in the Department of Mechanical and Process Engineering (D-MAVT).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#runnable-and-interactive-code-elements",
    "href": "index.html#runnable-and-interactive-code-elements",
    "title": "Machine Learning for Mechanical Engineering",
    "section": "Runnable and Interactive Code Elements",
    "text": "Runnable and Interactive Code Elements\nMany of the elements of this book involve runnable code elements in Python, or interactive experiments that you can conduct or visualize while you are reading. These will appear largely static in the rendered book, since some may have long run-times, but you can download and run the corresponding notebook in a browser-based environment (e.g., CoLab) to be able to run the experiments. Most experiments in the book are designed to be run in class in the span of a few minutes, so browser-based environments should work well for this.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#callouts-with-notes-and-experiments",
    "href": "index.html#callouts-with-notes-and-experiments",
    "title": "Machine Learning for Mechanical Engineering",
    "section": "Callouts with Notes and Experiments",
    "text": "Callouts with Notes and Experiments\nThrough the book, I will occasionally use callouts like the one demonstrated below for a couple of use cases:\n\n\n\n\n\n\nNoteExample Collapsed Note, e.g., for a Derivation\n\n\n\n\n\nHere is what a note with a collapsable block structure will look like. We will also use this type of structure to hide details of long derivations, either so you can work them out on your own first, or because the details are not immediately central to following what comes next in the chapter or lecture.\n\n\n\n\n\n\n\n\n\nTipExample Experiment\n\n\n\nHere is an example where we might pose an experiment or task for you to do on your own or in class. Often wrestling with these experiments is a good way to build intuitive understanding and integrate some of the material into practice. When you see this type of callout, you should pause and do the exercise at this point in the chapter, rather than reading ahead, as it may build intuition which is useful later in the chapter.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part1/part1.html",
    "href": "part1/part1.html",
    "title": "Foundational Skills",
    "section": "",
    "text": "This part of the book covers useful foundational skills that should serve you well regardless of which model is State-of-the-Art at the time. We will use them throughout the book, but it is helpful to have some material and exercises together in a cohesive whole.",
    "crumbs": [
      "Foundational Skills"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html",
    "href": "part1/reviewing_supervised_linear_models.html",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "",
    "text": "1.1 What is a Linear Model?\nWe will start by reviewing Linear Models, since they are likely already familiar to you from prior coursework, are widely used, and will serve as a useful (if simple) launching off point for later discussions of more advanced techniques. So, while some of what we will explore in this section might seem pretty basic at first glance, do not let it’s simplicity fool you, as we will revisit similar concepts throughout the rest of the notes, as these concepts will help you form a strong foundation that will serve us well once things get more complex.\nThis chapter will do this in three parts:\nWith this as a baseline model, the next chapter will review the concept of Cross-Validation and how we evaluate whether an Machine Learning model is “good”, and then the subsequent chapter will review (Stochastic) Gradient Descent, in the Linear Model context.\nLet’s start by trying to fit a model to the (admittedly simple) dataset below, where I have just sampled some (noisy) points from a periodic function:\nCode\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import SGDClassifier\nfrom ipywidgets import interact,interact_manual, FloatSlider\nfrom sklearn.datasets import make_regression\nimport seaborn as sns\nsns.set_context('poster')\nnp.random.seed(1)\n\n# Number of data points\nn_samples = 30\n\n# True Function we want to estimate\ntrue_fun = lambda X: np.cos(1.5 * np.pi * X)\n\n# Noisy Samples from the true function\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(10,10))\n# Plot the true function:\nX_plot = np.linspace(0, 1, 100)\nplt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n# Plot the data samples\nplt.scatter(X,y, label=\"Samples\")\nplt.legend(loc=\"best\")\nplt.show()\nIf we wanted to fit a line to this data, we would use what is called a linear model:\n\\[\ny = w_0+w_1\\cdot x\n\\]\nwhere \\(w_0\\) is the intercept and \\(w_1\\) is the slope of the line. We can write this more compactly using vector notation as: \\[\ny = \\mathbf{w}^T \\mathbf{x}\n\\] where w is the weight vector [\\(w_0\\), \\(w_1\\)] and \\(x\\) is the feature vector [1, x]. We can see here that taking the dot product between \\(w\\) and \\(x\\) is equivalent to the equation above. Importantly, even though the above equation represents a straight line with respect to x, we are not limited to using linear models only for this. For example, we could make:\n\\[\n\\mathbf{w} = [w_0, w_1, w_2, w_3];\\quad \\mathbf{x} = [1, x, x^2, x^3]\n\\]\nand in this way, we can model y as a cubic function of x, while \\(y = \\mathbf{w}^T \\mathbf{x}\\) remains a “linear model”, since it is still linear with respect to the weights \\(w\\). This is quite powerful, since by adding features (i.e., additional concatentated entries) to \\(\\mathbf{x}\\), we can fit functions that are apparently non-linear with respect to the original input variable \\(x\\), but will possess many useful properties of linear models that we will discuss later (e.g., convexity with respect to \\(w\\)).\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\n\n# Here is a list of different degree polynomials to try out\ndegrees = [1,2,3,5,10,15,20,30]\n\n# Generate samples of the true function + noise\nX = np.sort(np.random.rand(n_samples))\nnoise_amount = 0.1\ny = true_fun(X) + np.random.randn(n_samples) * noise_amount\n\n# For each of the different polynomial degrees we listed above\nfor d in degrees:\n    plt.figure(figsize=(7, 7)) # Make a new figure\n    # Construct the polynomial features\n    polynomial_features = PolynomialFeatures(degree=d,\n                                             include_bias=False)\n    # Construct linear regression model\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    # Now fit the data first through the \n    # polynomial basis, then do regression\n    pipeline.fit(X[:, np.newaxis], y)\n    \n    # Get the accuracy score of the trained model\n    # on the original training data\n    score = pipeline.score(X[:, np.newaxis],y)\n\n    # Plot the results\n    X_plot = np.linspace(0, 1, 100)\n    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n    plt.scatter(X, y, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    \n    # Print the polynomial degree and the training\n    # accuracy in the title of the graph\n    plt.title(\"Degree {}\\nTrain score = {:.3f}\".format(\n        d, score))\n    plt.show()\nWe can see that even though we are fitting a linear model every time, the behavior with respect to x is markedly non-linear. Moreover, we see some strange behavior as we increase the polynomial degree. What is going on here and why is it behaving in this way? To build some intuition, we can take a look at the learned weight coefficients, by accessing the coef_ attribute of the fitted model:\nCode\nlinear_regression.coef_\n\n\narray([ 4.29e+04, -6.41e+05,  4.46e+06, -5.47e+06, -1.61e+08,  1.50e+09,\n       -7.03e+09,  2.01e+10, -3.46e+10,  2.83e+10,  9.52e+09, -3.50e+10,\n        5.36e+08,  3.61e+10,  3.50e+09, -3.63e+10, -1.74e+10,  2.94e+10,\n        3.42e+10, -1.02e+10, -4.27e+10, -1.74e+10,  3.46e+10,  4.11e+10,\n       -1.48e+10, -5.44e+10,  1.25e+09,  6.96e+10, -5.11e+10,  1.14e+10])\nWhat is going on here? To understand this, we need to understand something about how the model is optimizing error and how we might control this behavior.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#what-is-a-linear-model",
    "href": "part1/reviewing_supervised_linear_models.html#what-is-a-linear-model",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "",
    "text": "TipExperiment: Effect of Polynomial Degree on Model Fit\n\n\n\nWe can see this in the below experiment, where we fit progressively higher order polynomial functions to the above training data. In each case we are fitting a linear model, by virtue of appending additional polynomial features to x. When you do this experiment, ask yourself:\n\nHow does the model fit change as we increase the polynomial degree?\nWhat happens to the training score (\\(R^2\\)) as we increase the polynomial degree?\nDoes this increase in the training score reflect what you intuitively expect? Why or why not?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#regularization-and-controlling-model-complexity",
    "href": "part1/reviewing_supervised_linear_models.html#regularization-and-controlling-model-complexity",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.2 Regularization and Controlling Model Complexity",
    "text": "1.2 Regularization and Controlling Model Complexity\nIn the base case above, all the model is trying to do is minimize the Mean Squared Error (MSE) with respect to the training error, and with a sufficient number of parameters (polynomial features, in this case), it becomes possible to always achieve perfect accuracy (i.e., fit every point) in the training data. Unfortunately, as we can see, fitting the training data perfectly does not necessarily lead to a model that generalizes well to new data. To help us control this behavior, we can introduce the concept of Regularization, which is a way of penalizing overly complex models. Specifically, we can modify our cost function to be something like:\n\\[\nCost = Loss(w,D) + \\Omega(w)\n\\]\nwhere \\(\\Omega(w)\\) represents what we call a “Regularization” of the function or a “Penalty Term” The purpose of \\(\\Omega(w)\\) is to help us prevent the (otherwise complex) model from being overly complicated, by penalizing this complexity. There are many ways to do this that we will see later on, but one common way to do this for linear models is to penalize the total weight that you allow all of the \\(w_i\\) to have. Specifically how one calculates this total weight turns out to matter a lot, and we shall see it return in later chapters and sections. But to get us started in un-packing how to do this, we first need to talk about what a Norm is, how it relates to Linear Regression weights, and how it helps us perform Regularization.\n\n1.2.1 Norms and their relationship to Regularization\nA Norm is a concept in mathematics that allows us to essentially measure length or size, typically of vectors. Any time you have tried to compute the distance between two points in space (say, by using the Pythagorean Theorem), or the magnitude of an applied Force vector, you have been using a Norm — most likely the Euclidean Norm or Euclidean Distance. For example, for a vector \\(\\mathbf{x}\\) with \\(n\\) dimensions, the Euclidean Norm looks like this: \\[\n||\\mathbf{x}||_2 = \\sqrt{ x_1^2 + x_2^2 + \\cdots x_n^2 }\n\\] If you have ever had to compute the total Force Magnitude given its x and y components (for example, in Statics class), you have used the Euclidean Norm to do so. In that context, it served to take multiple components of a Force aggregate them in such a way as to tell you something about the total force – by analogy, we will do the same thing here with linear regression, where each weight is like a component and we can use the Euclidean Norm to compute the total weight.\nWhile Euclidean Norms may be quite useful or familiar to Engineers, it turns out that they are a special case of a much wider family of Norms called p-norms, which are defined as: \\[\n||\\mathbf{x}||_p = \\left(|x_1|^p + |x_2|^p+\\cdots + |x_n|^p\\right)^{1/p} = \\left(\\sum_{i=1}^n \\left| x_i \\right|^p \\right)^p\n\\]\nSpecifically, the Euclidean Norm is called the L2-norm, or sometimes just the 2-norm. To see why this is, just set \\(p=2\\) in the above, and note how it corresponds to the Euclidean Norm that we all know and love. So, by setting \\(p\\) to a number between \\(0\\) and \\(\\infty\\), we can modify what the total weight means, and setting \\(p=2\\) is the setting which we are all most familiar with. To get a visual sense of how norms vary, see below, which visualizes a line of “circle” of radius 1, but where the length of the line is determined by the p-norm. You will see that when p=2 this corresponds to what we are familiar with, but when p goes up or down things change.\n\n\nCode\nfrom ipywidgets import interact, FloatSlider\n\n# Define a function to compute and plot the p-norm unit ball in 2D\ndef plot_p_norm(p=2.0):\n    # Avoid invalid p values\n    if p &lt;= 0:\n        print(\"p must be &gt; 0\")\n        return\n    \n    theta = np.linspace(0, 2*np.pi, 400)\n    # Parametric form of p-norm unit circle\n    x = np.cos(theta)\n    y = np.sin(theta)\n    \n    # Normalize to p-norm = 1\n    denom = (np.abs(x)**p + np.abs(y)**p)**(1/p)\n    x_unit = x / denom\n    y_unit = y / denom\n    \n    plt.figure(figsize=(5,5))\n    plt.plot(x_unit, y_unit, label=f'p = {p:.2f}')\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.title(f'Unit Ball in p-norm (p={p:.2f})')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Interactive slider for p\ninteract(\n    plot_p_norm,\n    p=FloatSlider(value=2.0, min=0.1, max=10.0, step=0.1, description='p')\n)\n\n\n\n\n\n&lt;function __main__.plot_p_norm(p=2.0)&gt;\n\n\nFor today, we will just focus on the L2-Norm, however we will revist norms again later where we will see how changing the one we are using can have positive or negative effects in certain circumstances.\nWe will use the L2-Norm to help us penalize having linear regression models with really large weights, by essentially putting a cost on the total weight of the weight vector, where the total is measured by the L2-Norm. That is: \\[\nCost = \\sum_{n=1}^{N}||y-w\\cdot x||^2 + \\alpha \\cdot ||w||^2\n\\] Where \\(\\alpha\\) is the price we pay for including more weight in the linear model. If it reduces our error cost enough to offset the cost of the increased weight, then that may be worth it to us. Otherwise, we would err on the side of using less weight overall.\nThis Regularization (i.e., increasing \\(\\alpha\\)) essentially allows you to trade off bias and variance, as we will see below.\n\n\n\n\n\n\nTipExperiment: Effect of Increasing Regularization Strength on Polynomial Fit\n\n\n\nIn the below experiment, we will increase the regularization strength (\\(\\alpha\\)) for a fixed 15-degree polynomial, and observe its effect on the overfitting problem before. Consider the following questions as you observe the experiment below:\n\nWhat happens when we set \\(\\alpha\\) to a low (close to zero) value?\nWhat happens when we set \\(\\alpha\\) to a high value?\n\n\n\n\n\nCode\n# Import Ridge Regression\nfrom sklearn.linear_model import Ridge\n\n# alpha determines how much of \n# a penalty the weights incur\nalphas = [0, 1e-20, 1e-10, 1e-7, 1e-5, 1, 10, 100]\n\n# For the below example, let's\n# just consider a 15-degree polynomial\nd=15\nnp.random.seed(100)\n\nfor a in alphas:\n    plt.figure(figsize=(7, 7))\n    polynomial_features = PolynomialFeatures(degree=d,\n                                             include_bias=False)\n    #linear_regression = LinearRegression() #&lt;- Note difference with next line\n    linear_regression = Ridge(alpha=a)\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    # Fit model\n    pipeline.fit(X[:, np.newaxis], y)\n    # Get Training Accuracy\n    score = pipeline.score(X[:, np.newaxis],y)\n\n    # Plot things\n    X_plot = np.linspace(0, 1, 100)\n    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n    plt.scatter(X, y, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    \n    plt.title(\"Degree={}, $\\\\alpha$={}\\nTrain score = {:.3f}\".format(\n        d, a, score))\n    plt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#examples-of-other-commonly-used-loss-functions-in-linear-models",
    "href": "part1/reviewing_supervised_linear_models.html#examples-of-other-commonly-used-loss-functions-in-linear-models",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.3 Examples of Other Commonly used Loss Functions in Linear Models",
    "text": "1.3 Examples of Other Commonly used Loss Functions in Linear Models\nThus far we have been discussing Linear Models in their most familiar context — minimizing the Mean Squared Error (MSE) with respect to the training data, optionally with an L2 regularization on the weight vector:\n\\[\n\\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 + \\alpha ||\\mathbf{w}||_2^2\n\\]\nFor now, let us ignore the regularization term, and just focus on the Loss term. Why should we minimize the squared error?1 Why not the absolute error or other possible loss functions? Let’s explore a few of those options and then see, in practice, how they affect the learned linear model.\n1 It turns out that there are good theoretical reasons for this, for example, that a Linear Model trained via an L2/MSE Loss is the Best Linear Unbiased Estimate (BLUE) of the Linear Model, according to the Gauss-Markov Theorem, but, as we will see, there are other reasons to forgo these advantages.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#loss-functions-for-regression",
    "href": "part1/reviewing_supervised_linear_models.html#loss-functions-for-regression",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.4 Loss Functions for Regression",
    "text": "1.4 Loss Functions for Regression\nBeyond classical MSE, there are two main types of variants that are commonly used for regression problems:\n\nRobust Loss Functions, that minimize the quadratic effect of the MSE loss for very large errors. These are typically used to make the trained model less sensitive to outliers in the training data.\nEpsilon-Insensitive Loss functions, that ignore errors that are sufficiently small (within an \\(\\epsilon\\) margin). These are typically used to incur some advantages in terms of sparsity in the learned model (for example, in Support Vector Methods, which we will see later).\n\nIn reality, these two variants can be combined in different ways, which we will see reflected below, but as a summary, these are:\n\nAbsolute Loss: \\(|y - \\hat{y}|\\)\nHuber Loss: A squared loss for small errors, and then transitioning to an absolute loss for large errors.\nEpsilon-Insensitive Loss: A loss that is zero for errors up to \\(\\epsilon\\) and then uses absolute loss for larger errors.\nSquared Epsilon-Insensitive Loss: Similar to Epsilon-Insensitive Loss, but uses squared loss for errors larger than \\(\\epsilon\\).\n\nOf course, you could imagine more complex variants and combinations of these properties, but these capture the main properties and benefits that we will see below.\n\n\nCode\ndef modified_huber_loss(y):\n    if(abs(y)&lt;1):\n        return y**2\n    else:\n        return 2*abs(y)-1\nmhuber = np.vectorize(modified_huber_loss)\n\neps = 0.7\ndef sq_esp_insensitive(y):\n    if(abs(y)&lt;eps):\n        return 0\n    else:\n        return (abs(y)-eps)**2\nsq_eps_ins = np.vectorize(sq_esp_insensitive)\n\n\n\n\nCode\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\n\nplt.figure(figsize=(10,5))\nplt.plot(xx, xx**2, 'g-',\n         label=\"Squared Loss\")\nplt.plot(xx, abs(xx), 'g--',\n         label=\"Absolute Loss\")\n\nplt.plot(xx, abs(xx)-eps, 'b--',\n         label=\"Epsilon-Insensitive Loss\")\n\nplt.plot(xx, sq_eps_ins(xx), 'b-',\n         label=\"Sq-Epsilon-Insensitive Loss\")\nplt.plot(xx, mhuber(xx), 'r-',\n         label=\"Modified-Huber Loss\")\n\nplt.ylim((0, 8))\nplt.legend(loc=\"upper center\")\nplt.xlabel(\"Error\")\nplt.ylabel(\"$L(y, f(x))$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.4.1 Handling Outliers using Robust Loss Functions\n\n\nCode\n# Create some data with Outliers\nn_samples = 1000\nn_outliers = 50\n\nXr, yr, coef = make_regression(n_samples=n_samples, n_features=1,\n                              n_informative=1, noise=10,\n                              coef=True, random_state=0)\n# Add outlier data\nnp.random.seed(0)\nXr[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\nyr[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\nyr/=10\nyr += 10\n\nline_X = np.arange(-5, 5)\nfigure = plt.figure(figsize=(9, 9))\n\nplt.scatter(Xr, yr,facecolors='None',edgecolors='k',alpha=0.5)\n\n# Loss Options: huber, squared_error, epsilon_insensitive, squared_epsilon_insensitive\nlosses = ['huber', 'squared_error']\nfor loss in losses:\n    model = SGDRegressor(loss=loss, fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\n    model.fit(Xr, yr)\n\n    # Predict data of estimated models\n    line_y = model.predict(line_X[:, np.newaxis])\n    plt.plot(line_X, line_y, '-', label=loss,alpha=1)\n\nplt.axis('tight')\nplt.legend(loc='best')\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#loss-functions-for-linear-classification",
    "href": "part1/reviewing_supervised_linear_models.html#loss-functions-for-linear-classification",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.5 Loss Functions for Linear Classification",
    "text": "1.5 Loss Functions for Linear Classification\nThus far we have only discussed Regression problems, where we are modeling a continuous output (e.g., \\(y=w^T\\cdot x\\)). However, Linear Models can also be used for Classification problems, where the output is discrete (e.g., \\(y \\in \\{0,1\\}\\) or \\(y \\in \\{-1,1\\}\\)). A naive approach to handling this, would be to just train a regression model as per before, but then just threshold the output at some value to derive the class label. For example, we can take the below binary data:\n\n\nCode\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_features=1, n_redundant=0, \n                           n_informative=1,\n                           random_state=1,\n                           n_clusters_per_class=1,\n                           flip_y=0.0, class_sep=1)\nplt.figure()\nplt.xlabel('x')\nplt.ylabel('True or False')\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\nplt.show()\n\n\n\n\n\n\n\n\n\nand then fit a regular linear model to this:\n\n\nCode\nmodel = SGDRegressor(loss='squared_error', fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\nmodel.fit(X, y)\nXp = np.linspace(X.min(),X.max(),100)\nXp = Xp[:, np.newaxis]\nplt.figure()\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\nplt.plot(Xp,model.predict(Xp))\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from the above that this is not entirely what we want – for example, just setting the class cutoff at 0.5 would not produce a classification boundary that would be optimal. In contrast, we can modify the loss function to more accurately project the \\(w^T\\cdot x\\) linear model into a classification context.\nWe do this by modifying the loss function from Mean Squared Error to something like: \\[\ny_i\\cdot(w\\cdot x_i)\n\\] where \\(y_i = \\pm 1\\) such that if \\(y_i\\) and \\(w\\cdot x_i\\) point have similar signs, then the decision function is positive, otherwise it is negative.\nWith this change, we are now interested primarily with how the behavior of the loss function when we are in the negative regime (i.e., misclassified points):\n\n\nCode\n# From: http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html\ndef modified_huber_loss(y_true, y_pred):\n    z = y_pred * y_true\n    loss = -4 * z\n    loss[z &gt;= -1] = (1 - z[z &gt;= -1]) ** 2\n    loss[z &gt;= 1.] = 0\n    return loss\n\n\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\nlw = 2\nfig = plt.figure(figsize=(15,8))\nplt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color='gold', lw=lw,\n         label=\"Zero-one loss\")\nplt.plot(xx, -np.minimum(xx, 0), color='yellowgreen', lw=lw,\n         label=\"Perceptron loss\")\nplt.plot(xx, modified_huber_loss(xx, 1), color='darkorchid', lw=lw,\n         linestyle='--', label=\"Modified Huber loss\")\nplt.plot(xx, np.log2(1 + np.exp(-xx)), color='cornflowerblue', lw=lw,\n         label=\"Log loss\")\nplt.plot(xx, np.where(xx &lt; 1, 1 - xx, 0), color='teal', lw=lw,\n         label=\"Hinge loss\")\nplt.plot(xx, np.where(xx &lt; 1, 1 - xx, 0) ** 2, color='orange', lw=lw,\n         label=\"Squared hinge loss\")\nplt.ylim((0, 8))\nplt.legend(loc=\"upper right\")\nplt.xlabel(r\"Decision function $f(x)$\")\nplt.ylabel(\"$L(y, f(x))$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n####### Try Changing the Below ######\n#loss = 'squared_error'\nloss = 'log_loss'\n#loss = 'hinge'\n#####################################\n\nmodel = SGDClassifier(loss=loss, fit_intercept=True, max_iter = 2000,\n                     penalty='l2', alpha=.001, epsilon=1, tol=1e-3)\nmodel.fit(X, y)\nXp = np.linspace(X.min(),X.max(),100)\nXp = Xp[:, np.newaxis]\nplt.figure()\nplt.scatter(X,y,marker='o',facecolors='none',edgecolors='k')\ntry:\n    plt.plot(Xp,model.predict_proba(Xp)[:,1],label='probability')\nexcept:\n    pass\nplt.plot(Xp,model.predict(Xp))\nplt.title(\"Classifier with loss = {}\".format(loss))\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also try this with 2D data to get a better sense of how some of the other loss functions behave:\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\n\n\nX, y = make_classification(n_features=2, n_redundant=0, \n                           n_informative=2,\n                           random_state=1,\n                           n_clusters_per_class=1,\n                           flip_y=0.0, class_sep=0.7)\n\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\n# Change: try 0,1, or 2\nds = datasets[2]\n\nX, y = ds\nX = StandardScaler().fit_transform(X)\n\nplt.figure(figsize=(9, 9))\nplt.scatter(X[y==1,0],X[y==1,1],marker='+')\nplt.scatter(X[y==0,0],X[y==0,1],marker='o',facecolors='none',edgecolors='k')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect of Linear Model Classification Losses\n\n\n\nIn the below experiment, try modifying the different classification loss functions and re-running the model. You will see a dark solid line representing the decision boundary, and dashed lines representing where the decision boundary is +1 or -1. What do you notice?:\n\nFor the perceptron loss, what behavior do you observe if you re-run this model multiple times? Why do you observe this behavior?\nComparing the hinge loss to the perceptron loss, the loss functions look remarkably similar, yet they have very different behavior in the model. Why do you think this is?\nComparing the squared error versus log loss versus hinge, what sorts of differences in behavior do you observe? Thinking about the location of the decision boundary and the shape of the loss functions, why do you think they behave differently?\n\n\n\n\n\nCode\nfrom sklearn.svm import LinearSVC\n\n# Try modifying these:\n#====================\nloss = 'squared_error'\n#loss = 'perceptron'\n#loss = 'log_loss'\n#loss = 'hinge'\n#loss = 'modified_huber'\n#loss = 'squared_hinge'\n\n# Also try the effect of Alpha:\n# e.g., between ranges 1e-20 and 1e0\n#=============================\nalpha=1e-3\n\n# You can also try other models by commenting out the below:\nmodel = SGDClassifier(loss=loss, fit_intercept=True,\n                      max_iter=2000,tol=1e-5, n_iter_no_change =100,\n                      penalty='l2',alpha=alpha) \n# If you would like to compare the SGDClassifier with hinge loss to LinearSVC, you can uncomment the below:\n#model = LinearSVC(loss='hinge',C=1e3)\nmodel.fit(X, y)\n\nplt.figure(figsize=(9, 9))\n\nh=0.01\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nif hasattr(model, \"decision_function\"):\n    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\nelse:\n    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ = Z.reshape(xx.shape)\n\nvmax = max(abs(Z.min()),abs(Z.max()))\ncm = plt.cm.RdBu\nplt.contourf(xx, yy, Z, cmap=cm, alpha=.5, vmax = vmax, vmin = -vmax)\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = ['dashed', 'solid', 'dashed']\ncolors = 'k'\nplt.contour(xx, yy, Z, levels, colors=colors, linestyles=linestyles)\nplt.scatter(X[y==1,0],X[y==1,1],marker='+')\nplt.scatter(X[y==0,0],X[y==0,1],marker='o',facecolors='none',edgecolors='k')\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#penalty-functions-example-of-how-l_p-penalty-changes-loss-optima",
    "href": "part1/reviewing_supervised_linear_models.html#penalty-functions-example-of-how-l_p-penalty-changes-loss-optima",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.6 Penalty Functions: Example of How \\(L_p\\) Penalty Changes Loss Optima",
    "text": "1.6 Penalty Functions: Example of How \\(L_p\\) Penalty Changes Loss Optima\nOK, so we have seen how different loss functions can affect the learned linear model. But what about the penalty function? How does changing the penalty function affect the learned model? Let’s explore this with a simple example by again returning to fitting a line. In this case, we will fit an actual “line” to the data, by which I mean:\n\\[\ny= w_0 + w_1 \\cdot x = \\mathbf{w}^T \\cdot \\mathbf{x}\n\\]\n\n\nCode\n# Generate noisy data from a line\n\n######################\n# Change Me!\nw1_true = 5\nw0_true = 2\nnoise = 0.001\n#################\n\ntrue_func = lambda x: w1_true*x+w0_true\n\nnum_samples = 50\nx = (np.random.rand(num_samples)-0.5)*20\ny = true_func(x)+np.random.normal(scale=noise,size=num_samples)\n\n\n\n\nCode\nplt.figure()\nplt.scatter(x,y)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this case, since we have very little noise (although you can play with this if you would like), you can see that if we put in the true intercept and slope, we get zero (or close to zero) training objective.\n\nfrom numpy.linalg import norm\ndef loss(a,b,alpha,order=2):\n    return np.average((y - (a*x+b))**2) + alpha*norm([a,b],ord=order)\n\n#example\nprint(f\"Objective: {loss(a=5,b=2,alpha=0)}\")\n\nObjective: 1.3389235788421472e-06\n\n\nThis tells us the objective at the true parameters, but now let’s visualize how the total objective (training error + penalty) varies as we change the parameters. To do this, we can compute the total objective over a grid of possible values for \\(w_0\\) and \\(w_1\\), and then plot the contours of this objective function:\n\n\nCode\nA, B = np.meshgrid(np.linspace(-10, 10, 201), np.linspace(-10, 10, 201))\nN,M = A.shape\nfloss = np.vectorize(loss)\n\ndef generate_new_data(a=5,b=5):\n    x = (np.random.rand(num_samples)-0.5)*20\n    #y = true_func(x)+np.random.normal(scale=1,size=num_samples)\n    y = a*x+b+np.random.normal(scale=1,size=num_samples)\n    return x,y\n\ndef generate_z_grid(alpha,order):\n    Z_noalpha = floss(A.flatten(),B.flatten(),0).reshape((N,M))\n    alpha=alpha\n    Z = floss(A.flatten(),B.flatten(),alpha,order)\n    min_ind = np.argmin(Z)\n    Amin = A.flatten()[min_ind]\n    Bmin = B.flatten()[min_ind]\n    Z = Z.reshape((N,M))\n    return Z_noalpha, Z, Amin, Bmin\n\nget_levels = lambda z: [np.percentile(z.flatten(),i) for i in [0,0.5,1,2,5,10,15,30,40,50,75,95,100]]\n#levels = [np.percentile(allz,i) for i in [0,0.5,1,2,5,10,15,30,40,50,75,95,100]]\n#levels = [np.percentile(allz,i) for i in np.logspace(-2,3,10,base=3)]\n\ndef plot_objective(alpha=0,order=2):\n    Z_noalpha, Z, Amin, Bmin = generate_z_grid(alpha,order)\n    plt.figure(figsize=(7,7))\n    plt.vlines(0,-10,10,alpha=0.25,colors='r',linestyles='solid')\n    plt.hlines(0,-10,10,alpha=0.25,colors='r',linestyles='solid')\n    plt.contour(A,B,Z_noalpha,10,levels=get_levels(Z_noalpha),linestyles='solid',cmap='Greys_r',alpha=0.05)\n    plt.contour(A,B,Z,10,levels=get_levels(Z),linestyles='solid',cmap='Greys_r',alpha=0.5)\n    plt.scatter([0],[0],marker='D',s=50,c='r')\n    plt.scatter([w1_true],[w0_true],marker='*',s=400)\n    plt.scatter([Amin],[Bmin])\n    plt.xlabel('a')\n    plt.ylabel('b')\n    plt.title('Optima: a={:.2f}, b={:.2f}'.format(Amin,Bmin))\n    plt.show()\n\n\n\ninteract(plot_objective,\n         alpha=np.logspace(-2,5,8),\n         order=FloatSlider(min=0,max=10,step=0.1,continuous_update=False,value=2.0))\n\n\n\n\n&lt;function __main__.plot_objective(alpha=0, order=2)&gt;\n\n\nHere we can see the true optimal parameters as a blue star, the objective function contours as dark gray lines, and the red diamond shows the point where both weights are zero. The orange circle shows the minimum point of the objective function. If you increase the value of alpha in the drop down, this will increase the penalty weight, and you can see how both the objective and the optimal point change. Moreover, you can change the p-norm order in the penalty to see what effect this has on the total objective function.\n\n\n\n\n\n\nTipExperiment: How do the different penalty terms affect the Objective Function and the optimal weight?\n\n\n\nTry modifying the following:\n\nAs you increase the penalty weight under the L2 norm, how does the objective landscape change?\nIf you use a p-order that is less than 2, how does this alter the objective landscape?\nThe L1 norm is known to induce sparsity in the optimal weights. Do you observe this in the objective landscape? Why or why not?\nDifferent types of regularization within the p-norm family are often referred to as “Shrinkage” operators. Why do you think this is the case, based on what you observe?\nIf my goal is to induce sparsity, it would make sense to consider the L0 norm, which just counts the number of non-zero entries in a vector. Based on the plots below, why won’t this work?\n\n\n\nTo help visualize the experiment above, let’s try plotting, for different norm orders, the path that the coefficients take as we set alpha = 0 (no regularization) to a large number (essentially fully regularized). Now the light green contour represents just the contribution to the objective from the training error (no regularization), and the light gray contour shows the contribution from the penalty term (no training error). The blue line shows how the optimal weight changes as we increase the penalty weight from 0 to a large number.2\n2 Note how I can visually find the optimal weight by fixing a given iso-contour of the regularization term (the gray lines) and then finding the point along that iso-contour where the green contour is minimized.\n\nCode\nfrom scipy.optimize import fmin\nimport warnings\nwarnings.simplefilter('ignore', RuntimeWarning)\n\nalpha_range = np.logspace(-1,5,14)\norder_range = [0,0.25,.5,.75,1,1.5,2,3,5,10,20,100]\n\nAl = len(alpha_range)\nOl = len(order_range)\nresults = np.zeros((Al,Ol,2))\n\nfor j,o in enumerate(order_range):\n    prev_opt = [5,5]\n    for i,a in enumerate(alpha_range):\n        f = lambda x: loss(x[0],x[1],alpha=a,order=o)\n        x_opt = fmin(f,prev_opt,disp=False)\n        results[i,j] = x_opt\n        prev_opt = x_opt\n\n\n\n\nCode\nfor j,o in enumerate(order_range):\n    fig = plt.figure(figsize=(12,7))\n    Z_noalpha, Z, Amin, Bmin = generate_z_grid(10000,o)\n    plt.contour(A,B,Z_noalpha,10,levels=get_levels(Z_noalpha),linestyles='solid',alpha=0.2,colors='g')\n    plt.contour(A,B,Z,10,levels=get_levels(Z),linestyles='solid',alpha=0.2,colors='k')\n    plt.plot(results[:,j,0],results[:,j,1],marker='o',alpha=0.75)\n    plt.title('L-{} Norm'.format(o))\n    plt.axis('equal')\n    plt.xlim([-1,6])\n    plt.ylim([-1,3])\n    plt.show()\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "part1/reviewing_supervised_linear_models.html#summary-of-how-to-select-a-loss-or-penalty-function",
    "href": "part1/reviewing_supervised_linear_models.html#summary-of-how-to-select-a-loss-or-penalty-function",
    "title": "1  Reviewing Supervised Linear Models",
    "section": "1.7 Summary of How to Select a Loss or Penalty function",
    "text": "1.7 Summary of How to Select a Loss or Penalty function\nIn class, we reviewed a couple of different forms of loss functions and penalty functions and talked a bit about the criteria for selecting them. Below is a very short summary of these.\n\n1.7.1 Loss Functions for Regression:\n\nIf you have no prior knowledge of the function or data, then selecting an L2 type loss (like the Mean Squared Error) is reasonable. When data nicely behaves w.r.t. a linear model (e.g., features are uncorrelated, errors in the linear model are uncorrelated, have equal variances, and expected error of zero around the linear model, etc.) then a Linear Model with an L2 Loss is the Best Linear Unbiased Estimate (BLUE) according to the Gauss-Markov Theorem.\nIf you have reason to believe that the data will have outliers or otherwise need to be robust to spurious large samples, then L2 loss will not be robust to this (as we saw in Lecture). For this, moving to an L1 type loss (like an Absolute Loss or Huber loss) will make the model less sensitive to outliers. It is one approach to handling Robust Regression.\nIf you need to have the model’s loss be dominated only by a handful of points/data as opposed to all of the data, then epsilon-insensitive loss is appropriate since many points well-fit by the model will have “zero” loss. For things like Linear Models, this has limited usefulness right now. However, when we “kernalize” Linear models in “Kernels” week, you will see that this is a big deal, and it is what gives rise to the “Support Vector” part of “Support Vector Machines”. Specifically, decreasing epsilon towards zero increases the number of needed Support Vectors (can be a bad thing), and increasing epsilon can decrease the number of needed Support Vectors (can be a good thing). This will make more sense in a few week’s time.\n\n\n\n1.7.2 Loss Functions for Classification:\n\nZero-One loss sounds nice, but is not useful in practice, since it is not differentiable.\nPerceptron loss, while of historical importance, is not terribly useful in practice, since it does not converge to a unique solution and an SVM (i.e., Hinge Loss below) has all of the same benefits.\nIf you need a simple linear model which outputs actual probabilities (like, 95% sure the component has failed), then the log-loss does this, via Logistic Regression and allows you to calculate classification probabilities in closed form.\nIf you want something that maximizes the margin of the classifier, then the Hinge Loss can get close to this. It is the basis of Linear Support Vector Machines\n\n\n\n1.7.3 Penalty Terms (Lp Norms) for Linear Models:\n\n\\(L_2\\) Norm penalties on the weight vector essentially “shrink” the weights towards zero as you increase the penalty weight. Adding this kind of penalty to a linear model has different names, depending on which community of people you are talking with. Some of these other names are: (1) Tikhonov regularization, (2) Ridge Regression, (3) \\(L_2\\) Shrinkage Estimators, or (4) Gaussian Weight Priors. I find looking at the penalty term itself more helpful at understanding the effects rather than memorizing the different names.\n\\(L_0\\) Norm penalties, while conceptually interesting since they essentially “count” entries in the weight vector, are not practically useful since they are not differentiable and are thus difficult to optimize.\n\\(L_1\\) Norm penalties are a compromise between \\(L_2\\) and \\(L_0\\) in that they promote sparsity in the weights (some weights will become zero) but are (largely) differentiable, meaning that you can meaningfully optimize them (unlike \\(L_0\\)). Shrinking certain weights to zero in this way can be useful when (1) you are in the \\(n \\ll p\\) regime (many more features than data points) where the model is underdetermined and (2) you want some degree of model interpretability (it sets many weights to zero). Some of these other names for this kind of linear regression with this penalty are the LASSO (least absolute shrinkage and selection operator). \n\\(L_\\infty\\) (where p is really large) essentially penalize the size of the biggest element of the weight vector (w). While there are some niche instances where this kind of norm is useful for a Loss function (e.g., Chebyshev Regression), I have rarely seen meaningful use cases in practice where this makes sense as a penalty term for the weight vector.\nCombinations of penalties. For example, a common combination is combining both an \\(L_2\\) and \\(L_1\\) penalty on the weights, as in: \\(\\Omega(w) = \\alpha ||w||_2 + \\beta ||w||_1\\). This particular combination is called “Elastic Net” and exhibits some of the good properties of \\(L_2\\) penalities with the sparsity inducing properties of \\(L_1\\) regularization.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reviewing Supervised Linear Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html",
    "href": "notebooks/cross_validation_linear_regression.html",
    "title": "2  Evaluating Machine Learning Models",
    "section": "",
    "text": "2.1 Getting an Unbiased Estimate of Out-of-Sample Performance\nIn the prior chapter, we covered how different loss functions and regularization terms affected a linear model, in terms of the model’s qualitative performance and its affect on the training score. However, as we saw, the training score an be a misleading performance indicator. How might we judge the model’s performance more rigorously? This chapter addresses this by reviewing the why and how of performing Cross Validation, and also what this means regarding optimizing the hyperparameters of a model.\nWhen we train a machine learning model, we are typically interested in how well the model will perform on data it has not seen before. This is often referred to as the model’s generalization performance. However, if we evaluate the model’s performance on the same data it was trained on, we may get an overly optimistic estimate of its true performance. This is because the model may have simply memorized the training data, rather than learning the underlying patterns. One popular way to assess this is through Cross Validation:\n# Now let's split the data into training and test data:\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.2, random_state=0)\nLet’s take a look at what the above has actually done.\nCode\nprint('X_train\\n',X_train,'\\n')\nprint('X_test\\n',X_test,'\\n')\nprint('y_train\\n',y_train,'\\n')\nprint('y_test\\n',y_test,'\\n')\n\nplt.figure(figsize=(8,8))\nplt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n# plot the training and testing points in colors\nplt.scatter(X_train,y_train, label=\"Training data\")\nplt.scatter(X_test,y_test, label=\"Testing data\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\nX_train\n [8.78142503e-01 3.02332573e-01 4.19194514e-01 6.92322616e-01\n 1.40386939e-01 4.17304802e-01 1.86260211e-01 3.96767474e-01\n 7.20324493e-01 6.70467510e-01 2.73875932e-02 9.68261576e-01\n 1.46755891e-01 9.23385948e-02 5.38816734e-01 5.58689828e-01\n 1.98101489e-01 1.69830420e-01 8.76389152e-01 8.50442114e-02\n 1.14374817e-04 6.85219500e-01 4.17022005e-01 3.13424178e-01] \n\nX_test\n [0.03905478 0.89460666 0.34556073 0.20445225 0.87811744 0.80074457] \n\ny_train\n [-0.37395132  0.05199163 -0.47818202 -0.82672018  0.90350849 -0.45417721\n  0.72898424 -0.36366041 -0.89399733 -1.11157064  0.90389734 -0.21270638\n  0.86040424  0.79675107 -0.89105816 -0.87458209  0.52662674  0.74673588\n -0.63887828  0.97904574  0.98275703 -0.97273902 -0.42390528  0.0668933 ] \n\ny_test\n [ 0.98733352 -0.47140604 -0.00455281  0.55839434 -0.61801179 -0.82613321]\nThe key idea in cross validation is to test the model on data that was separate from the data you trained on, therefore establishing two datasets: a training set and a test set. The training set is used to fit the model, while the test set is used to evaluate its performance. This way, we can get a more realistic estimate of how well the model will perform on new, unseen data.\nCode\nalphas = [0, 1e-20, 1e-10, 1e-7, 1e-5, 1,10]\nd=15\nfor a in alphas:\n    plt.figure(figsize=(7, 7))\n    #plt.setp(ax, xticks=(), yticks=())\n    polynomial_features = PolynomialFeatures(degree=d,\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    linear_regression = Ridge(alpha=a)\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    #pipeline.fit(X[:, np.newaxis], y)\n    pipeline.fit(X_train[:, np.newaxis], y_train)\n    # Evaluate the models using crossvalidation\n    #scores = cross_validation.cross_val_score(pipeline,\n    #    X[:, np.newaxis], y, scoring=\"mean_squared_error\", cv=10)\n    \n    testing_score = pipeline.score(X_test[:, np.newaxis],y_test)\n    training_score = pipeline.score(X_train[:, np.newaxis],y_train)\n\n    X_plot = np.linspace(0, 1, 100)\n    plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_plot, true_fun(X_plot), '--',label=\"True function\")\n    plt.scatter(X, y, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    \n    plt.title(\"Degree {}, Alpha {}\\nTest score = {:.3f}\\nTraining score = {:.3f}\".format(\n        d, a, testing_score,training_score))\n    plt.show()\nThis is a simplified type of cross-validation often referred to as “Shuffle Splitting” and is one of the most common, but it is useful to review other types of cross-validation via this nice summary page from the SKLearn library, which covers a variety of important variants including:\nDiscussion Point: Under what conditions or situations would using each type of cross-validation above be appropriate versus inappropriate?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#getting-an-unbiased-estimate-of-out-of-sample-performance",
    "href": "notebooks/cross_validation_linear_regression.html#getting-an-unbiased-estimate-of-out-of-sample-performance",
    "title": "2  Evaluating Machine Learning Models",
    "section": "",
    "text": "K-Fold Cross Validation\nLeave-One-Out Cross Validation\nStratified Cross Validation\nGroup-wise Cross Validation\nTime Series Split Cross Validation",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#finding-the-optimal-hyper-parameters",
    "href": "notebooks/cross_validation_linear_regression.html#finding-the-optimal-hyper-parameters",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.2 Finding the Optimal Hyper-parameters",
    "text": "2.2 Finding the Optimal Hyper-parameters\nNow that we have introduced the usage of hyper-parameters and cross-validation, a natural question arises: How do we choose the hyper-parameters? There are many ways to do this, and this section will describe the most common and basic ones, while leaving more advanced techniques (like Implicit Differentiation) for later. Specifically, this section will: 1. Define the concepts of Grid and Random Hyper-parameter search. 2. Use Grid and Random search to optimize hyper-parameters of a model. 2. Distinguish when Randomized Search is much better than grid search. 3. Describe how Global Optimization procedures such as Bayesian Optimization work. 4. Recognize why none of those at all work in High Dimensions and describe the “Curse of Dimensionality”\nIn future chapters once we cover more advanced derivative methods, we can discuss how to use tools like Implicit Differentiation to directly compute the gradient of the cross-validation score with respect to hyper-parameters, and then use this gradient to optimize the hyper-parameters using standard gradient-based optimization methods. However, for now, let’s focus on more basic derivative-free methods, since they are more widely used and easier to understand.\nLet’s start by returning to our Polynomial example, and this time focus on finding the best combination of degree and penalty weight for a linear model.\n\n\nCode\n# from sklearn.preprocessing import PolynomialFeatures\n# from sklearn.pipeline import Pipeline\n# from sklearn.linear_model import Ridge\n# from sklearn.metrics import mean_squared_error\n\n\n\n\nCode\nfrom sklearn import model_selection\n\n# Let's plot the behavior of a fixed degree polynomial\ndegree = 15\n# (i.e., f(x) = w_1*x + w_2*x^2 + ... + w_15*x^15)\n# but where we change alpha.\nalphas = np.logspace(start=-13,stop=4,num=20)\npolynomial_features = PolynomialFeatures(degree=degree,\n                                         include_bias=False)\nscores = []\nfor a in alphas:\n    linear_regression = Ridge(alpha=a)\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    cv_scores = model_selection.cross_val_score(pipeline,\n        X[:,np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=20)\n    scores.append(cv_scores)\n\nscores = np.array(scores)\n\nplt.figure(figsize=(7,3))\nplt.semilogx(alphas,-np.mean(scores,axis=1),'-')\nplt.ylabel('Test MSE')\nplt.xlabel('Alpha ($\\\\alpha$)')\nsns.despine()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#what-if-we-have-more-than-one-variable",
    "href": "notebooks/cross_validation_linear_regression.html#what-if-we-have-more-than-one-variable",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.3 What if we have more than one variable?",
    "text": "2.3 What if we have more than one variable?\nLet’s look at both polynomial degree and regularization weight\n\n\nCode\nscores = []\nalphas = np.logspace(start=-13, # Start at 1e-13\n                     stop=4,    # Stop at 1e4\n                     num=40)    # Split that into 40 pieces\ndegrees = range(1,16) # This will only go to 15, due to how range works\n\nscores = np.zeros(shape=(len(degrees), # i.e., 15\n                         len(alphas))) # i.e., 20\n\nfor i, degree in enumerate(degrees): # For each degree\n    polynomial_features = PolynomialFeatures(degree=degree,\n                                             include_bias=False)\n    \n    for j,a in enumerate(alphas):    # For each alpha\n        linear_regression = Ridge(alpha=a)\n        pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                             (\"linear_regression\", linear_regression)])\n        cv_scores = model_selection.cross_val_score(pipeline,\n            X[:,np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=20)\n        scores[i][j] = -np.mean(cv_scores)\n\n\n\n\nCode\nfig = plt.figure(figsize=(10,7))\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nax = fig.add_subplot(111, projection='3d')\n\nXs, Ys = np.meshgrid(range(len(degrees)), range(len(alphas)))\nzs = np.array([scores[i,j] for i,j in zip(np.ravel(Xs), np.ravel(Ys))])\nZs = zs.reshape(Xs.shape)\n\nXs, Ys = np.meshgrid(degrees, np.log(alphas))\n\nax.plot_surface(Xs, Ys, Zs, rstride=1, cstride=1, cmap=cm.coolwarm,\n    linewidth=0, antialiased=False)\n\n# Label the Axes\nax.set_xlabel('Degree')\nax.set_ylabel('Regularization')\nax.set_zlabel('MSE')\n\n# Rotate the image\nax.view_init(30, # larger # goes \"higher\"\n             30) # larger # \"circles around\"\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(5,10))\nplt.imshow(Zs,\n           cmap=cm.coolwarm, # Allows you to set the color\n           vmin=Zs.min(), vmax=0.2, # The min and max Z-Values (for coloring purposes)\n           extent=[Xs.min(), Xs.max(),   # How far on X-Axis you want to plot\n                   Ys.min(), Ys.max()],  # How far on Y-Axis\n           interpolation='spline16',      # How do you want to interpolate values between data?\n           origin='lower')\nplt.title('Mean Squared Error')\nplt.xlabel('Degree')\nplt.ylabel('Regularization')\nplt.colorbar()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#optimization",
    "href": "notebooks/cross_validation_linear_regression.html#optimization",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.4 Optimization",
    "text": "2.4 Optimization\nAt the end of the day, all this is doing is optimization/search over different parameters.\nHow should we go about automating this?\nMost common: Grid Search.\n\n\nCode\nprint('parameters we could change:')\nfor k in pipeline.get_params().keys():\n    print(\" \",k)\n\n\nparameters we could change:\n  memory\n  steps\n  transform_input\n  verbose\n  polynomial_features\n  linear_regression\n  polynomial_features__degree\n  polynomial_features__include_bias\n  polynomial_features__interaction_only\n  polynomial_features__order\n  linear_regression__alpha\n  linear_regression__copy_X\n  linear_regression__fit_intercept\n  linear_regression__max_iter\n  linear_regression__positive\n  linear_regression__random_state\n  linear_regression__solver\n  linear_regression__tol\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'polynomial_features__degree': list(range(1,16)), # 15 possible\n              'linear_regression__alpha': np.logspace(start=-13,stop=4,num=10),\n              'polynomial_features__include_bias':[True, False]}\n\n\n\n\nCode\n# How do we want to do cross-validation?\nfrom sklearn import model_selection\nnum_data_points = len(y)\n\n# 4-fold CV\nkfold_cv = model_selection.KFold(n_splits = 4) \n\n# Or maybe you want randomized splits?\nshuffle_cv = model_selection.ShuffleSplit(n_splits = 20,     # How many iterations?\n                                          test_size=0.2    # What % should we keep for test?\n                                         )\n\n\n\n\nCode\nX=X[:,np.newaxis]\ngrid_search = GridSearchCV(pipeline,    # The thing we want to optimize\n                           parameters,  # The parameters we will change\n                           cv=shuffle_cv, # How do you want to cross-validate?\n                           scoring = 'neg_mean_squared_error'\n                          )\ngrid_search.fit(X, y) # This runs the cross-validation\n\n\nGridSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n             estimator=Pipeline(steps=[('polynomial_features',\n                                        PolynomialFeatures(degree=15,\n                                                           include_bias=False)),\n                                       ('linear_regression',\n                                        Ridge(alpha=np.float64(10000.0)))]),\n             param_grid={'linear_regression__alpha': array([1.00000000e-13, 7.74263683e-12, 5.99484250e-10, 4.64158883e-08,\n       3.59381366e-06, 2.78255940e-04, 2.15443469e-02, 1.66810054e+00,\n       1.29154967e+02, 1.00000000e+04]),\n                         'polynomial_features__degree': [1, 2, 3, 4, 5, 6, 7, 8,\n                                                         9, 10, 11, 12, 13, 14,\n                                                         15],\n                         'polynomial_features__include_bias': [True, False]},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n             estimator=Pipeline(steps=[('polynomial_features',\n                                        PolynomialFeatures(degree=15,\n                                                           include_bias=False)),\n                                       ('linear_regression',\n                                        Ridge(alpha=np.float64(10000.0)))]),\n             param_grid={'linear_regression__alpha': array([1.00000000e-13, 7.74263683e-12, 5.99484250e-10, 4.64158883e-08,\n       3.59381366e-06, 2.78255940e-04, 2.15443469e-02, 1.66810054e+00,\n       1.29154967e+02, 1.00000000e+04]),\n                         'polynomial_features__degree': [1, 2, 3, 4, 5, 6, 7, 8,\n                                                         9, 10, 11, 12, 13, 14,\n                                                         15],\n                         'polynomial_features__include_bias': [True, False]},\n             scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('polynomial_features', PolynomialFeatures(degree=4)),\n                ('linear_regression',\n                 Ridge(alpha=np.float64(3.5938136638046257e-06)))]) PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=4) Ridge?Documentation for RidgeRidge(alpha=np.float64(3.5938136638046257e-06)) \n\n\n\n\nCode\ngrid_search.best_params_ # Once finished, you can see what the best parameters are\n\n\n{'linear_regression__alpha': np.float64(3.5938136638046257e-06),\n 'polynomial_features__degree': 4,\n 'polynomial_features__include_bias': True}\n\n\n\n\nCode\nprint(\"Best MSE for Grid Search: {:.2e}\".format(-grid_search.best_score_))\n\n\nBest MSE for Grid Search: 6.93e-03\n\n\n\n\nCode\ngrid_search.predict(X)  # You can also use the best model directly (in sklearn)\n\n\narray([ 0.92597995,  0.97112185,  0.97915236,  0.95343228,  0.94185444,\n        0.8235348 ,  0.80306679,  0.72117767,  0.65636685,  0.60679754,\n        0.57933728,  0.10937232,  0.05389104, -0.10522154, -0.34633312,\n       -0.43501599, -0.43622117, -0.44424987, -0.84518448, -0.88774347,\n       -0.98056731, -0.97399224, -0.96929348, -0.94126767, -0.78277808,\n       -0.54827761, -0.54221084, -0.54212267, -0.48316009, -0.20435414])\n\n\n\n\nCode\nbest_degree = grid_search.best_params_['polynomial_features__degree']\nbest_alpha = grid_search.best_params_['linear_regression__alpha']\nX_plot = X_plot[:,np.newaxis]\nplt.figure(figsize=(7, 7))\nplt.plot(X_plot, grid_search.predict(X_plot),'-',label=\"Model\",alpha=0.5)\nplt.plot(X_plot, true_fun(X_plot), ':',label=\"True function\",alpha=1)\nplt.scatter(X,y, c='Blue', s=20, edgecolors='none')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xlim((0, 1))\nplt.ylim((-2, 2))\nsns.despine()\nplt.title(\"Degree {}, Alpha {:.1e}\".format(best_degree,best_alpha))\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#randomized-search",
    "href": "notebooks/cross_validation_linear_regression.html#randomized-search",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.5 Randomized Search",
    "text": "2.5 Randomized Search\nIn reality, grid search is wasteful and not easy to control. A better (and still easy way) is to randomize the search.\n\n\nCode\n# Now, instead of specifying exact which points to test, we instead\n# have to specify a distribution to sample from.\n# For example, things from http://docs.scipy.org/doc/scipy/reference/stats.html\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import lognorm as sp_lognorm\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nparameters = {'polynomial_features__degree': sp_randint(1,20), # We want an integer\n              'linear_regression__alpha': sp_lognorm(1),\n              'polynomial_features__include_bias':[True, False]} # Selecting from two is fine\n\n\nNeed something whose logarithmic distribution we can control. How about a lognormal? \\[\n\\mathcal{N}(\\ln x;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac {(\\ln x - \\mu)^2} {2\\sigma^2}\\right].\n\\]\n\n\nCode\nsigma=6\nrv = sp_lognorm(sigma,scale=1e-7)\n\nplt.figure()\nplt.hist(rv.rvs(size=1000),bins=np.logspace(-20, 2, 22))\nplt.xscale('log')\nplt.xlabel('x')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nparameters = {'polynomial_features__degree': sp_randint(1,20), # We want an integer\n              'linear_regression__alpha': sp_lognorm(sigma,scale=1e-7),\n              'polynomial_features__include_bias':[True, False]} # Selecting from two is fine\n\n\n\n\nCode\n# Fitting the high degree polynomial makes the linear system almost\n# singular, which makes Numpy issue a Runtime warning.\n# This is not a problem here, except that it pops up the warning box\n# So I will disable it just for pedagogical purposes\nimport warnings\nwarnings.simplefilter('ignore',RuntimeWarning)\n\n# specify parameters and distributions to sample from\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# run randomized search\n#n_iter_search = 300 # How many random parameter settings should we try?\nn_iter_search = len(grid_search.cv_results_['params']) # Give it same # as grid search, to be fair\nrandom_search = RandomizedSearchCV(pipeline,\n                                   param_distributions=parameters,\n                                   n_iter=n_iter_search, \n                                   cv=shuffle_cv, # How do you want to cross-validate?\n                                   scoring = 'neg_mean_squared_error')\nrandom_search.fit(X, y)\n\n\nRandomizedSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n                   estimator=Pipeline(steps=[('polynomial_features',\n                                              PolynomialFeatures(degree=15,\n                                                                 include_bias=False)),\n                                             ('linear_regression',\n                                              Ridge(alpha=np.float64(10000.0)))]),\n                   n_iter=300,\n                   param_distributions={'linear_regression__alpha': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000023215096120&gt;,\n                                        'polynomial_features__degree': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002320FEBFF80&gt;,\n                                        'polynomial_features__include_bias': [True,\n                                                                              False]},\n                   scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=ShuffleSplit(n_splits=20, random_state=None, test_size=0.2, train_size=None),\n                   estimator=Pipeline(steps=[('polynomial_features',\n                                              PolynomialFeatures(degree=15,\n                                                                 include_bias=False)),\n                                             ('linear_regression',\n                                              Ridge(alpha=np.float64(10000.0)))]),\n                   n_iter=300,\n                   param_distributions={'linear_regression__alpha': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000023215096120&gt;,\n                                        'polynomial_features__degree': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000002320FEBFF80&gt;,\n                                        'polynomial_features__include_bias': [True,\n                                                                              False]},\n                   scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('polynomial_features', PolynomialFeatures(degree=4)),\n                ('linear_regression',\n                 Ridge(alpha=np.float64(3.3921813538938043e-06)))]) PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=4) Ridge?Documentation for RidgeRidge(alpha=np.float64(3.3921813538938043e-06)) \n\n\n\n\nCode\nrandom_search.best_params_ # Once finished, you can see what the best parameters are\n\n\n{'linear_regression__alpha': np.float64(3.3921813538938043e-06),\n 'polynomial_features__degree': 4,\n 'polynomial_features__include_bias': True}\n\n\n\n\nCode\nprint(\"Best MSE for Random Search: {:.2e}\".format(-random_search.best_score_))\n\n\nBest MSE for Random Search: 7.48e-03\n\n\n\n\nCode\nbest_degree = random_search.best_params_['polynomial_features__degree']\nbest_alpha = random_search.best_params_['linear_regression__alpha']\n\nplt.figure(figsize=(7, 7))\nplt.plot(X_plot, random_search.predict(X_plot),'-',label=\"Model\",alpha=0.5)\nplt.plot(X_plot, true_fun(X_plot), ':',label=\"True function\",alpha=1)\nplt.scatter(X,y, c='Blue', s=20, edgecolors='none')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xlim((0, 1))\nplt.ylim((-2, 2))\nsns.despine()\nplt.title(\"Degree {}, Alpha {:.1e}\".format(best_degree,best_alpha))\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#global-bayesian-optimization",
    "href": "notebooks/cross_validation_linear_regression.html#global-bayesian-optimization",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.6 Global Bayesian Optimization",
    "text": "2.6 Global Bayesian Optimization\nSurely, since we are essentially doing optimization, we could approach hyper-parameter selection as an optimization problem as well, right?\nEnter techniques like Global Bayesian Optimization below:\n\n\nCode\ndef f(x):\n    \"\"\"The function to predict.\"\"\"\n    return x * np.sin(x)\n    # Try others!\n    #return 5 * np.sinc(x)\n    #return x\n    \nX = np.atleast_2d(np.linspace(0, 10, 200)).T\n\n# Observations\ny = f(X).ravel()\n\nplt.figure()\nplt.plot(X,y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n########################################################\n# This is just a helper function, no need to worry about\n# The internals.\n# We will return to this example in Week 14\n########################################################\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nnp.random.seed(1)\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n\n# Create a Gaussian Process model\n#kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\nkernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n#gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\nkernel = C(3.0)*RBF(1.5)\ngp = GaussianProcessRegressor(kernel=kernel,alpha=1e-6,optimizer=None)\n#gp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1e-1,random_start=100)\n\n# Now, ready to begin learning:\ntrain_ind ={\n    'Upper CB':   np.zeros(len(X),dtype=bool),\n    'Random':np.zeros(len(X),dtype=bool)\n}\noptions = train_ind.keys()\n\npossible_points = np.array(list(range(len(X))))\n# Possible Initialization options\n# 1. Select different points randomly\n#for i in range(2):\n#    for o in options:\n#        ind = np.random.choice(possible_points[~train_ind[o]],1)\n#        train_ind[o][ind] = True\n\n# 2. Start with end-points\n#for o in options:\n#    train_ind[o][0] = True\n#    train_ind[o][-1] = True\n\n# 3. Start with same random points\nfor ind in np.random.choice(possible_points,2):\n    for o in options:\n        train_ind[o][ind] = True\n\nplot_list = np.array([5,10,20,30,40,50,len(X)])\nfor i in range(10):\n    # As i increases, we increase the number of points\n    plt.figure(figsize=(16,6))\n    for j,o in enumerate(options):\n        plt.subplot(1,2,j+1)\n        gp.fit(X[train_ind[o],:],y[train_ind[o]])\n        yp,sigma = gp.predict(X[~train_ind[o],:], return_std=True)\n        ucb = yp + 1.96*sigma\n        if o == 'Upper CB':\n            #candidates = np.extract(MSE == np.amax(MSE),X[~train_ind[o],:])\n            candidates = np.extract(ucb == np.amax(ucb),X[~train_ind[o],:])\n            next_point = np.random.choice(candidates.flatten())\n            next_ind = np.argwhere(X.flatten() == next_point)\n        elif o == 'Random':\n            next_ind = np.random.choice(possible_points[~train_ind[o]],1)\n        train_ind[o][next_ind] = True\n        \n        # Plot intermediate results\n        yp,sigma = gp.predict(x, return_std=True)\n        plt.fill(np.concatenate([x, x[::-1]]),\n                np.concatenate([yp - 1.9600 * sigma,\n                               (yp + 1.9600 * sigma)[::-1]]),'b',\n                alpha=0.05,  ec='g', label='95% confidence interval')\n    \n        n_train = np.count_nonzero(train_ind[o])\n\n        gp.fit(X[train_ind[o],:],y[train_ind[o]])\n        # Show progress\n        yp,sigma = gp.predict(x, return_std=True)\n        yt = f(x)\n        error = np.linalg.norm(yp-yt.flatten())\n\n        plt.fill(np.concatenate([x, x[::-1]]),\n                np.concatenate([yp - 1.9600 * sigma,\n                               (yp + 1.9600 * sigma)[::-1]]),'b',\n                alpha=0.3,  ec='None', label='95% confidence interval')\n        \n        plt.plot(x,yt,'k--',alpha=1)\n        plt.plot(x,yp,'r-',alpha=1)\n        plt.scatter(X[train_ind[o],:],y[train_ind[o]],color='g',s=100)\n        plt.scatter(X[next_ind,:].flatten(),y[next_ind].flatten(),color='r',s=150)\n        plt.ylim([-10,15])\n        plt.xlim([0,10])\n        plt.title(\"%s\\n%d training points\\n%.2f error\"%(o,n_train,error))\n    plt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/cross_validation_linear_regression.html#the-curse-of-dimensionality",
    "href": "notebooks/cross_validation_linear_regression.html#the-curse-of-dimensionality",
    "title": "2  Evaluating Machine Learning Models",
    "section": "2.7 The Curse of Dimensionality",
    "text": "2.7 The Curse of Dimensionality\nDiscuss on board examples of the Curse of Dimensionality and how it affects algorithms dependent on calculating distances.\n\nSpace-filling properties of inscribed hyper-cube\nDistance ratio between min and max distances\nEffects on nearest neighbor graphs\nEffects on Gaussian Density\n\n\n\nCode\nfrom math import gamma\nV_sphere = lambda d: np.pi**(d/2.0)\nV_cube = lambda d: d*2**(d-1)*gamma(d/2.0)\nvolume_ratio = lambda d: V_sphere(d)/V_cube(d)\n\nd = range(2,50)\nratio = [volume_ratio(i) for i in d]\nplt.figure(figsize=(10,10))\nplt.plot(d,ratio)\nplt.semilogy(d,ratio)\nplt.ylabel(\"Ratio of Hyper-Sphere Vol. to Hyper-Cube Vol.\")\nplt.xlabel(\"Number of Dimensions\")\nplt.show()\n\n# TODO: Add distance min/max example",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html",
    "href": "notebooks/supervised_linear_models.html",
    "title": "3  Introduction to Gradient Descent",
    "section": "",
    "text": "3.1 Experiment:\nWe will now review Linear Regression from the standpoint of Gradient Descent (instead of the normal equations), so as to build our intuition about how Gradient Descent works, and also introduce the concept of Stochastic Gradient Descent (SGD).\nLet’s first set up our notation for the problem: \\[\ny = w\\cdot x + b + \\epsilon\n\\] Or, if we consider \\(x = [1, x]\\) then: \\[\ny = \\mathbf{w^T\\cdot x} + \\epsilon\n\\]\nFrom your earlier statistics classes, you likely learned how to solve for the linear regression weights using the Normal Equations: \\[\n\\hat{w} = (X^T X)^{-1}X^T y\n\\]\nThere are ways of solving the normal equations directly without needing to take the inverse (such as using the Cholesky decomposition), however today we are going to focus on a different kind of solver that has more broader applications: Gradient Descent and it’s cousin Stochastic Gradient Descent (SGD).\nWe first need to start with some sort of Cost function that we wish to minimize. In general, we will consider costs of the form: \\[\nLoss = Error + \\alpha\\cdot Penalty\n\\]\nSpecifically for Linear Models, we will talk about costs (which I’ll call \\(J\\)) of the form:\n\\[\nJ(w,X) = \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - f(\\mathbf{w},\\mathbf{x}_i)\\right)^2 + \\alpha\\cdot\\Omega(\\mathbf{w})\n\\]\nwhere for Linear Models \\(f(w,X) = \\mathbf{w\\cdot X}\\), so that our overall cost becomes:\n\\[\nJ(w,X) = \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - \\mathbf{w\\cdot \\mathbf{x}_i}\\right)^2 + \\alpha\\cdot\\Omega(\\mathbf{w})\n\\]\nWe’ll consider the no-penalty case (\\(\\alpha=0\\)), so that our loss is just:\n\\[\nJ(w,X) = \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - \\mathbf{w\\cdot \\mathbf{x}_i}\\right)^2\n\\]\nLet’s plot this cost as a function of the line slope, just to get an idea of what it looks like:\nWhile it might seem clear to us, visually, where the lowest cost is, actually finding this point automatically via a computer with minimal effort is another story. This is essentially what the field of Optimization tries to do. One simple (but powerful) method of optimization is Gradient Descent. It works by taking a (possibly random) starting point (e.g., w=60), and then computing the gradient of the function at that point. Since gradients will point upwards, and we want to minimize the cost, we will instead walk in the negative gradient direction, which should move us closer to the bottom. Let’s see this on an example, by computing the gradient of our cost function above with respect to the slope (w):\n\\[\n\\begin{aligned}\n\\frac{\\partial J}{\\partial w} &=& \\frac{\\partial}{\\partial w} \\left( \\frac{1}{N}\\Sigma_{i=1}^N \\left(\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i\\right)^2 \\right) \\\\\n&=&\\frac{1}{N}\\Sigma_{i=1}^N  \\frac{\\partial}{\\partial w} \\left(\\left(\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i\\right)^2 \\right) \\\\\n&=&\\frac{2}{N}\\Sigma_{i=1}^N (\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i) \\frac{\\partial}{\\partial w} \\left(\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i \\right) \\\\\n&=&-\\frac{2}{N}\\Sigma_{i=1}^N (\\mathbf{y}_i - \\mathbf{w}\\cdot \\mathbf{x}_i) \\cdot \\mathbf{x}_i\n\\end{aligned}\n\\]\nLet’s plot this:\nOnce we have 1) a starting point, and 2) the gradient at a point, the idea with gradient descent is to take a small step (\\(\\alpha\\)) in the direction of the negative gradient:\n\\[\nw_{t+1} = w_t - \\alpha \\frac{\\partial J}{\\partial w}\n\\]\nNote: here we are just considering a single parameter (the slope, w), but this method extends to multiple parameters (\\(\\mathbf{\\theta}\\)), via the gradient operator:\n\\[\n\\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_t - \\alpha \\nabla_\\theta J\n\\]\nTry modifying the initial guess wg and the step size alpha and re-running the below cells. What do you observe?\n######################\n# Try changing the below\nwg = 80 # Initial guess at slope; Try changing this\nalpha = 0.1  # Try changing alpha (both big and small)\n# What do you notice?\n###########################\nnum_steps = 20 # Take 20 steps\nweights = np.zeros(num_steps)\nweights[0] = wg  # Set the initial weight\nfor i in range(1,num_steps): \n    weights[i] = grad_step(weights[i-1], X, alpha)\nprint(\"Final weight from Gradient Descent is {:.2f}\".format(weights[i]))\nprint(\"Compared to {:.2f} from the Normal Equations\".format(wn))\n\nFinal weight from Gradient Descent is 43.06\nCompared to 42.57 from the Normal Equations\nweight_cost = [loss(w) for w in weights]\nplt.figure(figsize=(10,10))\nplt.plot(wp,cost)\nplt.scatter(weights,weight_cost,facecolors='none', edgecolors='r',linewidth=1)\nax = plt.gca()\nfor i,w in enumerate(weights):\n    ax.annotate('{}'.format(i), xy=(w, weight_cost[i]-10), \n                xytext=(w+1e-8, 10+50*np.random.rand()),\n                ha='center',fontsize=8,\n                arrowprops=dict(facecolor='white', edgecolor='grey',\n                                shrink=0.05,\n                            width=1, headwidth=1)\n               )\nplt.ylabel('Cost')\nplt.xlabel('slope (w)')\nplt.show()\n# Plot how the weights progress\nplt.figure(figsize=(10,5))\nplt.plot(range(len(weights)),weights, label='GD')\nplt.hlines(wn, 0, len(weights), \n           label = \"Optimal Weight\", \n           color='k', linestyle=\"--\")\nplt.xlabel('Gradient Descent Iteration')\nplt.ylabel('Weight')\nplt.legend()\nplt.show()\n# Copying for comparison later\ngd_weights = weights",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html#experiment-1",
    "href": "notebooks/supervised_linear_models.html#experiment-1",
    "title": "3  Introduction to Gradient Descent",
    "section": "4.1 Experiment:",
    "text": "4.1 Experiment:\nWhat do you notice about the behavior of SGD? What happens when alpha is small vs large? What happens when you take multiple passes through the data? If I keep doing more passes, do I eventually converge?\nWhat you are seeing is a result of Stochastic Approximation (trying to approximate a gradient of a function using noisy estimates of that gradient (where the noise here comes from evaluating the gradient using only one data point).\nThis behavior was studied by multiple people in the 1950s and 60s, with one key result coming Herbert Robbins and Sutton Monro, in what is now called the Robbins-Monro Algorithm. The central idea is rather than defining a single step size, we should let the step size decrease over time. Initially, we need to move the weights a lot, but as we get closer to the goal, they exert less influence so that we settle at some point. What they showed was that SGD would converge to the right estimator so long as the sequence satisfies the following properties:\n\\[\n\\begin{aligned}\n\\sum_{n=1}^{\\infty} a_n &= \\infty \\\\\n\\sum_{n=1}^{\\infty} a_n^2 &&lt; \\infty\n\\end{aligned}\n\\]\nFor sequences where \\(a_n&gt;0~\\forall~n&gt;0\\), Robbins and Monro recommended the \\(a_n = a/n\\), however this rate is based on some assumptions about smoothness and convexity which sometimes don’t work well in practice. People generally use decay rates on the order of \\(O(1/\\sqrt(n))\\), however there are entire fields of researchers working on this “optimal step size” problem for SGD and there are many great alternative procedures out there if you know certain things about the function (e.g., can compute things like hessians, etc.)\n\n#################################\n# Try Changing the below\nwg = 80 # Initial guess at slope\nalpha_i = 0.5  # Initial Step Size\n#alpha = lambda n: alpha_i\n#alpha = lambda n: alpha_i/n\nalpha = lambda n: alpha_i/np.sqrt(n)\nnum_passes = 5  # Number of times we pass through the data\nshuffle_after_pass = True  # Whether to shuffle the data\n##########################\n# What do you find?\n\nN = len(y)\nweights = np.zeros(N*num_passes+1)\nk=0\nweights[k] = wg  # Set the initial weight\nprint('Initial weight: ',weights[0])\n\nindex = list(range(N))\nfor n in range(num_passes):\n    if shuffle_after_pass:\n        np.random.shuffle(index)\n    for i in index:\n        k+=1\n        xi = X[i,0]\n        yi = y[i]\n        weights[k] = weights[k-1] - alpha(k)*sgd_dloss(weights[k-1],yi,xi)\nprint('Final Weight from SGD: {:.2f}'.format(weights[-1]))\nprint(\"Compared to {:.2f} (Normal Equations)\".format(wn))\n\nplt.figure(figsize=(10,5))\n# Plot SGD Weights\nplt.plot(range(len(weights)), weights,\n         marker=None,\n         label = 'SGD')\n# Plot Grad. Descent Weights\nplt.plot(np.array(range(len(gd_weights)))*len(index),\n         gd_weights,\n         marker='o',\n         label = 'GD')\n# Plot True Answer\nplt.hlines(wn, 0, len(weights), \n           label = \"Optimal Weight\", \n           color='k', linestyle=\"--\")\nplt.xlabel('# Samples Used')\nplt.ylabel('Weight')\n#plt.ylim([30,55])\nplt.xlim([0,len(weights)])\nplt.legend()\nplt.show()\n\nInitial weight:  80.0\nFinal Weight from SGD: 41.30\nCompared to 42.57 (Normal Equations)\n\n\nC:\\Users\\mafuge\\AppData\\Local\\Temp\\ipykernel_5008\\3561310644.py:2: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  return float(-2*(y-w*x)*x)\n\n\n\n\n\n\n\n\n\nWhile Gradient Descent looks much better than SGD here, let’s now scale the axis by the number of model evaluations needed:\n\nplt.figure(figsize=(10,5))\nplt.plot(range(len(weights)), weights,\n         label = 'SGD')\nplt.plot(np.array(range(len(gd_weights)))*len(index),\n         gd_weights,\n         marker='o',\n         label = 'GD')\nplt.hlines(wn, 0, len(weights), \n           label = \"Optimal Weight\", \n           color='k', linestyle=\"--\")\n#plt.ylim([35,50])\nplt.xlim([0,len(weights)])\nplt.xlabel('# Samples Used')\nplt.ylabel('Weight')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nnp.array(range(len(gd_weights)))*len(index)\n\narray([   0,  100,  200,  300,  400,  500,  600,  700,  800,  900, 1000,\n       1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900])",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html#existing-implementations",
    "href": "notebooks/supervised_linear_models.html#existing-implementations",
    "title": "3  Introduction to Gradient Descent",
    "section": "4.2 Existing Implementations",
    "text": "4.2 Existing Implementations\nSGD is a fairly simple and popular technique for solving many problems where you can easily express the derivatives of those functions. For certain types of loss function (like the square error/L2 norm we discussed above), many folks have written optimized libraries for just that purpose, such as Scikit-Learn’s SGD functions including SGDRegressor.\nFor reference, the SGD Regressor in ScikitLearn uses an update rule similar to: \\[\n\\eta^{(t)} = \\frac{eta_0}{t^{power_t}}\n\\]\n\nfrom sklearn.linear_model import SGDRegressor\nsgd = SGDRegressor(loss = 'squared_error',\n                   eta0 = 0.01,  # Initial Learning rate/step size\n                   power_t = 0.25, # how quickly sould eta decay?\n                   max_iter = 100,  # Max # of passes to do over the data?\n                   tol = 1e-3,     # Tolerance for change in loss\n                   fit_intercept=False # Not worrying about b term in w*x+b\n                  )\n# Here eta = eta0/(t^power_t) where t is the iteration\nX = np.asarray(X)\ny = np.array(y).reshape(len(y),) # Reshape y so that scikit doesn't complain\nsgd.fit(X,y)\nprint('Final Weight from SKLearn SGD: {:.2f}'.format(sgd.coef_[0]))\nprint(\"Compared to {:.2f} (Normal Equations)\".format(wn))\n\nFinal Weight from SKLearn SGD: 42.55\nCompared to 42.57 (Normal Equations)\n\n\n\nplt.figure(figsize=(10,5))\nplt.scatter(np.asarray(X).ravel(),\n            np.asarray(y).ravel(),\n            color='k'\n           )\n#plt.scatter(X,y)\nXp = [[-3],[3]]\nplt.plot(Xp,sgd.predict(Xp),label='SGD')\nplt.plot([-3,3],[-3*wn, 3*wn],\n         label='Normal Eqns',\n         linestyle='--' )\nplt.legend()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "notebooks/supervised_linear_models.html#advanced-techniques",
    "href": "notebooks/supervised_linear_models.html#advanced-techniques",
    "title": "3  Introduction to Gradient Descent",
    "section": "4.3 Advanced Techniques",
    "text": "4.3 Advanced Techniques\nThere are a variety of more advanced SGD techniques, most of which involve one or more of the following tricks: 1. Using “acceleration” procedures that leverage “momentum” of some type. You can read more about this phenomenon at: “Why Momentum Really Works” 2. “Batching” the SGD updates: that is, taking steps that are considering \\(N&gt;n&gt;1\\) in size (e.g., averaging the gradients of, say, 5 data points before taking a step). This can help stablize gradients and improve convergence. 3. “Normalizing” the gradient updates: that is, re-scaling the gradient updates at each step to achieve better convergence. This is commonly used in Neural Networks for things like Batch Normalization (Wikipedia) (or, for a more advanced introduction, you can read the NeurIPS paper Understanding Batch Normalization).",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Gradient Descent</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html",
    "href": "part1/linear_decompositions.html",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "",
    "text": "4.1 Principal Component Analysis\nIn this notebook, we will briefly review some of the key concepts of linear unsupervised learning, including Principal Component Analysis (PCA) and the effects of regularizations on linear decompositions, such as SparsePCA and Non-negative Matrix Factorization (NMF). We will use a set of airfoil geometries to demonstrate these effects visually. For those who need to refresh their memory on basic concepts around Matrices and how they affect data, you can see visual examples in Appendix C\nFirst, let’s load some airfoil geometry coordinates, take a look at the shape of the data matrix, and pick a random one to visualize:\nGreat, as we can see there are 1528 airfoils, expressed as 192 surface coordinates each with an x and y value. We can turn this into a matrix compatible with a linear decomposition by flattening the last two dimensions, so that each airfoil is a row vector of length 384 (2x192).\nNow let’s demonstrate how to use various dimension reduction algorithms on this example.\nMathematically, given centered data \\(X \\in R^{n×d}\\), PCA finds k orthonormal components that best reconstruct the data in least-squares sense. One convenient formulation is\n\\[\n\\min_{W,Z}\\;\\|X - Z W\\|_{F}^{2}\\quad\\text{s.t. }Z=XW^{T},\\;W W^{T}=I.\n\\]\nEquivalently PCA maximizes the projected variance:\n\\[\n\\max_{W:\\;W W^{T}=I} \\;\\mathrm{tr}(W \\Sigma W^{T}),\\quad \\Sigma=\\frac{1}{n}X^{T}X.\n\\]\nNotes:\nfrom sklearn import decomposition\n\n# We can set the maximum number of components that we want to truncate to\n# Or can just leave it as None to get all components\nn_components = 20\nestimator = decomposition.PCA(n_components=n_components)\nZ_pca = estimator.fit_transform(data)\ncomponents_ = estimator.components_\nWe see that we now possess a matrix (i.e., linear operator) that goes from the target 20 components/dimensions back to the original 382 dimensions.\nprint(f\"The shape of the components_ matrix (W) is {components_.shape}\")\n\nThe shape of the components_ matrix (W) is (20, 384)\nIf we wanted to visualize how each of these components looks like in terms of the original airfoil coordinates, we can reshape each row of the components matrix back to the original airfoil shape:\ncomponents_.reshape((n_components, -1, 2)).shape\n\n(20, 192, 2)\nLet’s go ahead and visualize the first all of the learned components:\nCode\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(components_, aspect='auto', cmap='RdBu_r', interpolation='nearest', \n           norm=TwoSlopeNorm(vmin=components_.min(), vcenter=0, vmax=components_.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Feature index ($d \\\\in {Z_pca.shape[0]}$)\")\nplt.ylabel(f\"Latent index ($k \\\\in {Z_pca.shape[1]}$)\")\nplt.title('PCA Component Matrix (latent codes x original features)')\nplt.show()\nWe notice a kind of alternating aliasing pattern in the components, but recall, this is because of how we reshaped the original data, which had rows of x and y coordinates interleaved. To make this clearer, we can visually re-order the indices of the components so that all the x-coordinates come first (first 192 features), followed by all the y-coordinates (second 192 features):\nCode\n# make an array with all of the odd indices of components_\ndef reorder_indices(components):\n    return np.hstack([components[:,0:-1:2],components[:,1:-1:2]])\n\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(components_), aspect='auto', cmap='RdBu_r', interpolation='nearest', \n           norm=TwoSlopeNorm(vmin=components_.min(), vcenter=0, vmax=components_.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Feature index ($d \\\\in {Z_pca.shape[0]}$)\")\nplt.ylabel(f\"Latent index ($k \\\\in {Z_pca.shape[1]}$)\")\nplt.title('PCA Component Matrix (latent codes x original features) - Reordered')\nplt.show()\nOK, this now looks a little clearer. What do you notice?\nWe can next visualize how each data point is mapped to all of the latent coordinates:\nCode\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_pca, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=Z_pca.min(), vcenter=0, vmax=Z_pca.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {Z_pca.shape[1]}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_pca.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\nWhy do you think the first few components are more important than the later ones? We can gain some intuition here by looking at the amount of variance explained by each component:\nCode\nplt.figure()\nplt.plot(estimator.explained_variance_)\nplt.ylabel(\"Explained Variance\")\nplt.xlabel(\"Latent Dimension\")\nplt.title(\"PCA Explained Variance\")\nplt.xticks(np.arange(n_components))\nplt.show()\nWe can also visualize the cumulative explained variance to see how many components are needed to explain a certain amount of variance in the data. For example, we can plot a line at the number of dimensions we need to keep to explain 99% of the variance:\nCode\ncumulative_explained_var_ratio = np.cumsum(estimator.explained_variance_ratio_)\nnumber_of_vars_to_99 = np.argmax(cumulative_explained_var_ratio&gt;.99)\nplt.figure()\nplt.plot(cumulative_explained_var_ratio,label=\"Cumulative Explained Variance\")\nplt.ylabel(\"Explained Variance\")\nplt.xlabel(\"Latent Dimension\")\nplt.xticks(np.arange(n_components))\nplt.vlines(number_of_vars_to_99,\n           np.min(cumulative_explained_var_ratio),1.0,\n           colors=\"k\",linestyles='dashed',\n          label = \"99% Explained Variance\")\nplt.legend()\nplt.title(\"PCA Cumulative Explained Variance\")\nplt.show()\nOr plot the explained variance ratio as a function of the number of components, which tells us how much each additional component contributes to the total explained variance:\nCode\nplt.figure()\nplt.plot(estimator.explained_variance_ratio_)\nplt.ylabel(\"Explained Variance Ratio\")\nplt.xlabel(\"Latent Dimension\")\nplt.xticks(np.arange(n_components))\nplt.title(\"PCA Explained Variance Ratio\")\nplt.show()\nWe can see that this approximately follows the singular values of the data matrix:\nCode\nprint(f\"Singular Values: {estimator.singular_values_}\")\nplt.figure()\nplt.plot(estimator.singular_values_)\nplt.ylabel(\"Singular Values\")\nplt.xlabel(\"Latent Dimension\")\nplt.xticks(np.arange(n_components))\nplt.title(\"PCA Singular Values\")\nplt.show()\n\n\nSingular Values: [7.96347431 7.30793278 2.11271967 1.880563   1.43978062 1.06857152\n 0.86184436 0.79754077 0.46006406 0.40013427 0.3503857  0.33218949\n 0.28107742 0.20708862 0.19063632 0.17582911 0.14392864 0.1288838\n 0.12505431 0.10367465]\nMoving beyond just the singular values, we can also now look at the projection of each airfoil into any of the latent dimensions. It is natural to explore the first two, since those explain the largest variance:\nCode\nz = estimator.transform(data)\nplt.figure()\nplt.scatter(z[:,0],z[:,1],alpha=0.3)\nplt.xlabel(\"1st Principal Component\")\nplt.ylabel(\"2nd Principal Component\")   \nplt.title(\"PCA Projection onto First Two Principal Components\")\nplt.show()\nWe can also pull out the “latent code/coordinates/vector” for an individual airfoil:\nz[airfoil_id,:]\n\narray([ 0.26565954, -0.30692116,  0.18797174, -0.0349173 , -0.0307379 ,\n        0.02908489, -0.02403219,  0.02460046, -0.01230913, -0.0146706 ,\n        0.0124125 ,  0.00509173, -0.01270168,  0.00192036, -0.00573131,\n        0.00744313,  0.00506638,  0.00798497,  0.01136562,  0.00033123])",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#principal-component-analysis",
    "href": "part1/linear_decompositions.html#principal-component-analysis",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "",
    "text": "There is no explicit L1/L2 penalty in vanilla PCA; the constraint \\(W W^{T}=I\\) enforces orthonormality of components.\nThe analytical solution is given by the top-k eigenvectors of the covariance (or the top-k left/right singular vectors from SVD).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.1 How does the # Components affect the Airfoil Reconstruction?\nWe can see visually the effect of including an increasing number of components by looking at how this affects the reconstruction of a single airfoil:\n\n\nCode\nfor n_components in range(2,10):\n    estimator = decomposition.PCA(n_components=n_components, whiten=False)\n    \n    # Train the model\n    estimator.fit(data)\n    # Project down to the low dimensional space\n    z     = np.dot(data - estimator.mean_, estimator.components_.T)\n    # Re-Project back to the high dimensional space\n    x_hat = np.dot(z[0], estimator.components_) + estimator.mean_\n\n    # Now plot them\n    airfoil_original = make_airfoil(data[0])\n    airfoil_reconstructed = make_airfoil(x_hat)\n    airfoil_mean = make_airfoil(estimator.mean_)\n    plt.figure()\n    plt.plot(airfoil_original[:,0],\n             airfoil_original[:,1],\n             alpha=0.75,\n             label = 'Original')\n    plt.plot(airfoil_reconstructed[:,0],\n                airfoil_reconstructed[:,1],\n                alpha=0.5,\n                label='Reconstructed')\n\n    plt.title(f\"Using {n_components} Number of Components\")\n    plt.legend()\n    plt.axis('equal')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=1, min=1, max=9, step=1, description='top_k')\n    def show_topk(k=4):\n        # Find the first k principal components and multiply them with the corresponding latent variables\n        contrib = np.dot(z[airfoil_id,:k], components_[:k,:])+ estimator.mean_\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        plt.plot(make_airfoil(contrib)[:,0], make_airfoil(contrib)[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '-', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n\n\n\n\n\n\nCode\n# Interactive visualization of each component\nif widgets is not None:\n    component_selector = widgets.IntSlider(value=1, min=1, max=n_components, step=1, description='component')\n    def show_component(k=4):\n        # Find the first k principal components and multiply them with the corresponding latent variables\n        #contrib = np.dot(z[airfoil_id,:k], components_[:k,:])+ estimator.mean_\n        plt.figure(figsize=(6,5))\n        selected_component = components_[k-1,:]+ estimator.mean_\n        plt.plot(make_airfoil(selected_component)[:,0], make_airfoil(selected_component)[:,1], label='Component', alpha=1.0)\n        plt.axis('equal')\n        plt.title(f'Component {k}')\n        plt.xlim(-0.6,0.6)\n        plt.show()\n    display(widgets.VBox([component_selector, widgets.interactive_output(show_component, {'k': component_selector})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#sparse-pca",
    "href": "part1/linear_decompositions.html#sparse-pca",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.2 Sparse PCA",
    "text": "4.2 Sparse PCA\nSparse PCA encourages components that have many zeros, which can make the learned components easier to interpret as localized shape features. Below we fit a SparsePCA model and visualize a small set of sparse components and their effect on reconstructing an example airfoil.\nMathematically, Sparse PCA augments a PCA-style reconstruction loss with an L1 penalty on the (unconstrained) components to encourage sparsity. A common optimization is:\n\\[\\min_{Z,W}\\;\\|X - ZW\\|_{F}^{2} + \\alpha\\|W\\|_{1}\\quad\\text{s.t. }\\|Z_{i}\\|_{2}^{2}\\le 1\\;\\forall i,\\,\\]\nwhere \\(W \\in R^{k×d}\\) holds the component vectors (rows), \\(Z \\in R^{n×k}\\) are the codes, and \\(\\alpha&gt;0\\) controls sparsity. Different implementations (e.g., the sklearn SparsePCA) solve related objectives (sometimes using a LASSO subproblem or elastic-net style updates).\nNotes:\n\nThe L1 term on W encourages many component weights to be exactly zero, producing localized/part-like components.\nIn practice one often also adds a small L2 (ridge) term to stabilize optimization (an elastic-net variant).\n\n\nfrom sklearn.decomposition import SparsePCA\n\n#### Try Changing both n_components and alpha  ########\nn_components = 12\n# Warning: setting alpha too low will take the algorithm a long time to converge\nalpha = 0.01\n##########################\nspca = SparsePCA(n_components=n_components, alpha=alpha, random_state=42)\n# fit on the flattened data matrix 'data' used above\nspca.fit(data)\nW_sp = spca.components_\nZ_sp = spca.transform(data)\n\nNow let’s take a look at what the Sparse PCA model has learned and how it reconstructs the example airfoil differently than vanilla PCA:\n\n\nCode\n# Component heatmap (components x features)\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(W_sp), aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=W_sp.min(), vcenter=0, vmax=W_sp.max()))\nplt.colorbar(label='weight')\nplt.ylabel(f\"Latent index ($k \\\\in {W_sp.shape[0]}$)\")\nplt.xlabel(f\"Feature index ($d \\\\in {W_sp.shape[1]}$)\")\nplt.title('SparsePCA component weights (components x features)')\nplt.show()\n\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_sp, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=Z_sp.min(), vcenter=0, vmax=Z_sp.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {Z_sp.shape[1]}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_sp.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\n\n# Now let's try reconstructing an airfoil from the trained model\n# Pull out the latent representation of this airfoil ID\nz = Z_sp[airfoil_id]\nx_hat = spca.inverse_transform(z.reshape(-1, 1).T)\n# Alternatively, we could do the following manual reconstruction:\n#x_hat = np.dot(z, W_sp) + data.mean(axis=0)\n\n# Interactive visualization of each component\nif widgets is not None:\n    component_selector = widgets.IntSlider(value=1, min=1, max=n_components, step=1, description='component')\n    def show_component(k=4):\n        # Find the first k principal components and multiply them with the corresponding latent variables\n        #contrib = np.dot(z[airfoil_id,:k], components_[:k,:])+ estimator.mean_\n        plt.figure(figsize=(6,5))\n        selected_component = W_sp[k-1,:] + data.mean(axis=0)\n        plt.plot(make_airfoil(selected_component)[:,0], make_airfoil(selected_component)[:,1], label='Component', alpha=1.0)\n        plt.axis('equal')\n        plt.title(f'Component {k}')\n        #plt.legend()\n        plt.xlim(-0.6,0.6)\n        #plt.ylim(-0.4,0.4)\n        plt.show()\n    display(widgets.VBox([component_selector, widgets.interactive_output(show_component, {'k': component_selector})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=4, min=1, max=n_components, step=1, description='top_k')\n    def show_topk(k=4):\n        top_idx = np.argsort(np.abs(z))[::-1][:k]\n        contrib = np.sum(z[top_idx][:,None] * W_sp[top_idx], axis=0)\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        plt.plot(make_airfoil(contrib + data.mean(axis=0))[:,0], make_airfoil(contrib + data.mean(axis=0))[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Comparison between PCA and SparsePCA\n\n\n\nCompare both the component weight matrix and the reconstruction of a single airfoil when we used PCA and SparsePCA.\nConsider the following questions:\n\nWhat are the main differences in the component weights between PCA and SparsePCA? How does the addition of the L1 penalty within SparsePCA manifest itself in the weight matrix? How does this manifest itself in the learned components, either individually or as we add them up during reconstruction?\nAlso compare the latent codes \\(Z\\) between PCA and SparsePCA. How do they differ? What does this tell you about how the data is represented in the latent space? How about which components are most important?\nFor SparsePCA, how does changing the value of the sparsity parameter \\(\\alpha\\) affect the learned components and reconstruction? What happens when we increase or decrease \\(\\alpha\\)? Look at both the weight matrix and the components.\nIf you set \\(\\alpha\\) very low (e.g., 0.001) and increase the number of components, how does the result compare to vanilla PCA with the same number of components? What would you expect to happen, comparing the loss functions? Why are they different?\nUnder what practical circumstances or for what types of problems might SparsePCA be preferred over PCA and vice versa?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#dictionary-learning",
    "href": "part1/linear_decompositions.html#dictionary-learning",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.3 Dictionary Learning",
    "text": "4.3 Dictionary Learning\nDictionary learning finds a set of atoms (basis elements) and sparse codes that reconstruct the data. This is useful when localized, part-based representations are desirable. Essentially, unlike Sparse PCA (which adds an L1 penalty to the components), dictionary learning adds an L1 penalty to the codes. Below we fit a Dictionary Learning model and visualize a small set of learned atoms and their effect on reconstructing an example airfoil.\nMathematically, Dictionary learning (sparse coding) models X as the product of a dictionary \\(W \\in R^{k×d}\\) (atoms) and sparse codes \\(Z \\in R^{n×k}\\):\n\\[\n\\min_{Z,W}\\;\\|X - Z W\\|_{F}^{2} + \\alpha\\|Z\\|_{1} \\,\\quad\\text{s.t. }\\|W_{j}\\|_{2}\\le 1\\; \\forall j.\n\\]\nHere Z contains the sparse coefficients for each example and \\(\\alpha&gt;0\\) controls the sparsity of the codes. The constraint on \\(W\\) prevents trivial scaling to reduce the penalty term (i.e., just pulling weight into W and shrinking Z). Intuitively, each data point is reconstructed as a sparse linear combination of dictionary atoms, essentially selecting a few “parts” (where the parts are elements of \\(W\\)) to compose the whole as a weighted sum.\nNotes:\n\nThe L1 penalty acts on the coefficients (Z) rather than \\(W\\) to encourage sparse usage of dictionary elements.\nMany algorithms alternate between solving for Z (a LASSO-type problem) and updating W (a constrained least-squares step).\n\n\n# Warning, running this particular cell takes a long time as the sklearn implementation \n# is not particularly fast. (~4-8 mins)\nfrom sklearn.decomposition import DictionaryLearning\n# Alternatively, you can use the MiniBatchDictionaryLearning which is faster, but less accurate or stable:\n#from sklearn.decomposition import MiniBatchDictionaryLearning as DictionaryLearning\n\n\n### Try changing n_components and alpha ########\nn_components = 10\nalpha = 0.1\n##########################\n\nDL = DictionaryLearning(n_components=n_components, alpha=alpha, random_state=42)\nZ_dl = DL.fit_transform(data)\nW_dl = DL.components_\n\n\n\nCode\n# Visualize first few atoms\nplt.figure(figsize=(12,6))\nfor i in range(n_components):\n    plt.subplot(4,6,i+1)\n    xy = make_airfoil(W_dl[i])\n    plt.plot(xy[:,0], xy[:,1], lw=1)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title(f'Atom {i}')\n    plt.axis('equal')\n    plt.xticks([])\n    plt.yticks([])\nplt.suptitle(f'Dictionary atoms (n={n_components}, alpha={alpha})')\nplt.show()\n\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(W_dl), aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=W_dl.min(), vcenter=0, vmax=W_dl.max()))\nplt.colorbar(label='weight')\nplt.ylabel(f\"Latent index ($k \\\\in {n_components}$)\")\nplt.xlabel(f\"Feature index ($d \\\\in {W_dl.shape[1]}$)\")\nplt.title('Dictionary of Features (aka Component weights)')\nplt.show()\n\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_dl, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n           norm=TwoSlopeNorm(vmin=Z_dl.min(), vcenter=0, vmax=Z_dl.max()))\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {n_components}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_dl.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\n\n# Reconstruction an airfoil using the learned codes\nx_hat = np.dot(Z_dl[airfoil_id], W_dl)\nplt.figure(figsize=(6,3))\nplt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\nplt.plot(make_airfoil(x_hat)[:,0], make_airfoil(x_hat)[:,1], label='Reconstruction', color='C3')\nplt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\nplt.axis('equal')\nplt.title('Comparison of Original and Reconstructed Airfoil')\nplt.legend()\nplt.show()\n\n# Interactive top-k atoms display\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=6, min=1, max=n_components, step=1, description='top_k')\n    def show_top_atoms(k=6):\n        top_idx = np.argsort(np.abs(Z_dl[airfoil_id]))[::-1][:k]\n        plt.figure(figsize=(8,4))\n        for i, idx in enumerate(top_idx):\n            plt.subplot(1,k,i+1)\n            plt.plot(make_airfoil(W_dl[idx])[:,0], make_airfoil(W_dl[idx])[:,1])\n            plt.axis('equal')\n            plt.title(f'atom {idx}')\n            plt.xticks([])\n            plt.yticks([])\n        plt.suptitle('Top atoms used for this example')\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_top_atoms, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=4, min=1, max=8, step=1, description='top_k')\n    def show_topk(k=4):\n        top_idx = np.argsort(np.abs(Z_dl[airfoil_id]))[::-1][:k]\n        #contrib = np.sum(z[top_idx][:,None] * W_sp[top_idx], axis=0)\n        contrib = np.sum(Z_dl[airfoil_id,top_idx][:,None] * W_dl[top_idx], axis=0)\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        #plt.plot(make_airfoil(contrib + data.mean(axis=0))[:,0], make_airfoil(contrib + data.mean(axis=0))[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(contrib)[:,0], make_airfoil(contrib)[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\n# Show sparsity level of codes\nsparsity = np.mean(np.abs(Z_dl) &lt; 1e-6)\nprint(f'Average fraction of near-zero coefficients: {sparsity:.3f}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage fraction of near-zero coefficients: 0.540\n\n\n\n\n\n\n\n\nTipExperiment: Comparison between Dictionary Learning (Sparse Coding) and SparsePCA\n\n\n\nCompare both the component weight matrix and the reconstruction of a single airfoil when we used Sparse PCA and Dictionary Learning (Sparse Coding).\nConsider the following questions:\n\nBoth algorithms use the same sparsity penalty (L1), but on different matrices. How does this difference manifest itself in the learned components and reconstructions? What happens to the component weights versus the latent codes?\nFor Dictionary Learning, how does changing the value of the sparsity parameter \\(\\alpha\\) affect the learned components and reconstruction? What happens when we increase or decrease \\(\\alpha\\)? Look at both the weight matrix and the components.\nUnder what practical circumstances or for what types of problems might Dictionary Learning (Sparse Coding) be preferred over PCA and Sparse PCA, and vice versa?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#non-negative-matrix-factorization-nmf",
    "href": "part1/linear_decompositions.html#non-negative-matrix-factorization-nmf",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.4 Non-Negative Matrix Factorization (NMF)",
    "text": "4.4 Non-Negative Matrix Factorization (NMF)\nNMF constrains both the basis elements and coefficients to be non-negative. This often yields parts-based, additive representations which can be intuitive for shape components under certain circumstances.\nMathematically, Non-negative matrix factorization approximates \\(X\\) (with \\(X\\gt 0\\) after shift) as the product of non-negative factors \\(W\\in R^{n×k}_{+}\\) and \\(H \\in R^{k×d}_{+}\\) by minimizing reconstruction error under non-negativity constraints:\n\\[\\min_{W\\ge 0,H\\ge 0}\\;\\|X - Z W\\|_{F}^{2} + \\beta_Z\\|Z\\|_{1} + \\beta_W\\|W\\|_{1} + \\alpha_Z\\|Z\\|^{2}_{F} + \\alpha_W\\|W\\|^{2}_{F},\\]\nwhere optional L1 penalties (\\(\\beta_Z\\), \\(\\beta_W \\ge 0\\)) encourage sparse parts or sparse activations, and L2 penalties (\\(\\alpha_Z\\), \\(\\alpha_W \\ge 0\\)) encourage stability or shrinkage. The essential constraint is \\(W,Z \\ge 0\\) which induces additive, parts-based representations.\nNotes:\n\nIn this notebook we shift data by the minimum to ensure \\(X \\ge 0\\) before fitting and then undo the shift when plotting reconstructions.\nTypical solvers use multiplicative updates or alternating non-negative least squares; regularization (L1 or L2) can be added to encourage sparsity or stability.\n\n\nfrom sklearn.decomposition import NMF\n\n#### Try changing n_components ########\nn_components = 14\n# This is the L2 penalty on the components\nalpha_W  = 0.001\n# This is the L1 ratio on the components (between 0 and 1)\n# Set to 0 for pure L2, 1 for pure L1, and in between for a mix\nl1_ratio = 0.001\n##########################\n\n# NMF needs non-negative data. We'll shift by the min and remember the offset.\nshift = data.min()\nXpos = data - shift + 1e-6\n\nnmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=5000,\n          alpha_W = alpha_W, l1_ratio=l1_ratio)\nZ_nmf = nmf.fit_transform(Xpos)\nW_nmf = nmf.components_\n\n\n\nCode\n# Visualize NMF components (H) as parts\nplt.figure(figsize=(12,4))\nfor i in range(n_components):\n    plt.subplot(2, n_components//2, i+1)\n    xy = make_airfoil(W_nmf[i])\n    plt.plot(xy[:,0], xy[:,1], lw=1)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.axis('equal')\n    plt.title(f'Part {i}')\n    plt.xticks([])\n    plt.yticks([])\nplt.suptitle('NMF learned parts (components)')\nplt.show()\n\n# Component heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(reorder_indices(W_nmf), aspect='auto', cmap='Reds', interpolation='nearest')\nplt.colorbar(label='weight')\nplt.xlabel('feature index')\nplt.ylabel('part index')\nplt.title('NMF part weights (parts x features)')\nplt.show()\n\n# Latent Coordinate heatmap\nplt.figure(figsize=(10,4))\nplt.imshow(Z_nmf, aspect='auto', cmap='Reds', interpolation='nearest')\nplt.colorbar(label='weight')\nplt.xlabel(f\"Latent index ($k \\\\in {Z_nmf.shape[1]}$)\")\nplt.ylabel(f\"Data Point index ($n \\\\in {Z_nmf.shape[0]}$)\")\nplt.title('Latent codes for each Data Point')\nplt.show()\n\n# Reconstruction the airfoil using W @ H\nx_hat_pos = np.dot(Z_nmf[airfoil_id], W_nmf)\n# undo the shift to bring back to original centered data\nx_hat = x_hat_pos + shift - 1e-6\nmean_airfoil = make_airfoil(data.mean(axis=0))\n\n# Use plot_reconstruction helper for consistent display; but NMF uses shifted data so show overlay manually\nplt.figure(figsize=(6,3))\n#plt.subplot(1,2,1)\nplt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original')\nplt.plot(mean_airfoil[:,0], mean_airfoil[:,1], color='g', label='Mean')\nplt.plot(make_airfoil(x_hat)[:,0], make_airfoil(x_hat)[:,1], label='Reconstructed')\nplt.title('Original')\nplt.axis('equal')\nplt.legend()\nplt.show()\n\n# Interactive top-k parts\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=6, min=1, max=n_components, step=1, description='top_k')\n    def show_top_parts(k=6):\n        top_idx = np.argsort(Z_nmf[airfoil_id])[::-1][:k]\n        plt.figure(figsize=(8,3))\n        for i, idx in enumerate(top_idx):\n            plt.subplot(1,k,i+1)\n            plt.plot(make_airfoil(W_nmf[idx])[:,0], make_airfoil(W_nmf[idx])[:,1])\n            plt.axis('equal')\n            plt.title(f'part {idx}')\n            plt.xlim(-0.1,0.6)\n            plt.xticks([])\n            plt.yticks([])\n        plt.suptitle('Top parts used for this example')\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_top_parts, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually')\n\n# Interactive top-k overlay for contribution\nif widgets is not None:\n    top_k_slider = widgets.IntSlider(value=4, min=1, max=14, step=1, description='top_k')\n    def show_topk(k=4):\n        top_idx = np.argsort(np.abs(Z_nmf[airfoil_id]))[::-1][:k]\n        contrib = np.sum(Z_nmf[airfoil_id,top_idx][:,None] * W_nmf[top_idx], axis=0)\n        plt.figure(figsize=(6,3))\n        plt.plot(make_airfoil(data[airfoil_id])[:,0], make_airfoil(data[airfoil_id])[:,1], label='Original', alpha=0.6)\n        #plt.plot(make_airfoil(contrib + data.mean(axis=0))[:,0], make_airfoil(contrib + data.mean(axis=0))[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(contrib+shift)[:,0], make_airfoil(contrib+shift)[:,1], label='Top-k contribution', color='C3')\n        plt.plot(make_airfoil(data.mean(axis=0))[:,0], make_airfoil(data.mean(axis=0))[:,1], '--', color='g', label='Mean')\n        plt.axis('equal')\n        plt.title(f'Top-{k} component contributions (overlay)')\n        plt.legend()\n        plt.show()\n    display(widgets.VBox([top_k_slider, widgets.interactive_output(show_topk, {'k': top_k_slider})]))\nelse:\n    print('ipywidgets not available; call the top-k overlay code manually.')\n\nprint(f'Explained variance (approx): {1 - np.linalg.norm(Xpos - Z_nmf.dot(W_nmf)) / np.linalg.norm(Xpos):.3f}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplained variance (approx): 0.994",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/linear_decompositions.html#model-comparison",
    "href": "part1/linear_decompositions.html#model-comparison",
    "title": "4  Review of Linear Unsupervised Learning",
    "section": "4.5 Model Comparison",
    "text": "4.5 Model Comparison\nFinally, let’s compare the different models side-by-side. We will visualize the two two learned projections of the data into the latent space, as well as the component reconstruction matrix that translates those components back into the original feature space, and lastly the mapping of all data points to all latent coordinates. This is similar to plots you have seen before, but it is nice to see them all together for comparison.\n\n\nCode\n# Comparison: first-two latent projections and component heatmaps for each method\nfrom sklearn.decomposition import PCA\n\nmodels_info = {}\n# PCA (reuse estimator from above if present, otherwise fit)\npca = PCA(n_components=8, random_state=42)\nZ_pca = pca.fit_transform(data)\nmodels_info['PCA'] = {'Z': Z_pca, 'components': pca.components_}\n\n# SparsePCA (use Z_sp, W_sp computed earlier)\nmodels_info['SparsePCA'] = {'Z': Z_sp, 'components': W_sp}\n\n# DictionaryLearning: use W as Z and H as components\nmodels_info['DictionaryLearning'] = {'Z': Z_dl, 'components': W_dl}\n\n# NMF: use W as Z (already non-negative)\nmodels_info['NMF'] = {'Z': Z_nmf, 'components': W_nmf}\n\n# Plot the first-two latent projections\nplt.figure(figsize=(10,8))\nfor i, (name, info) in enumerate(models_info.items()):\n    plt.subplot(2,2,i+1)\n    Z = info['Z']\n    plt.scatter(Z[:,0], Z[:,1], alpha=0.2, s=8)\n    plt.xlabel('Latent dim 1')\n    plt.ylabel('Latent dim 2')\n    plt.title(f'{name} projection (first two dims)')\nplt.tight_layout()\nplt.show()\n\n# Plot heatmaps of component/weight matrices (showing first 120 features for clarity)\nplt.figure(figsize=(12,8))\nfor i, (name, info) in enumerate(models_info.items()):\n    comps = info['components']\n    # ensure comps is (n_components, n_features)\n    if comps.ndim == 1:\n        comps = comps[None, :]\n    plt.subplot(4,1,i+1)\n    if np.any(comps &lt; 0):\n        plt.imshow(reorder_indices(comps), aspect='auto', cmap='RdBu_r', interpolation='nearest',\n                   norm=TwoSlopeNorm(vmin=comps.min(), vcenter=0, vmax=comps.max()))\n    else:\n        plt.imshow(reorder_indices(comps), aspect='auto', cmap='Reds', interpolation='nearest')\n    plt.colorbar()\n    plt.title(f'{name} components (components x features)')\n    plt.ylabel('component')\nplt.tight_layout()\nplt.show()\n\n# Plot heatmaps of component/weight matrices (showing first 120 features for clarity)\nplt.figure(figsize=(12,8))\nfor i, (name, info) in enumerate(models_info.items()):\n    Z_map = info['Z']\n    # ensure comps is (n_components, n_features)\n    if Z_map.ndim == 1:\n        Z_map = Z_map[None, :]\n    plt.subplot(4,1,i+1)\n    if np.any(Z_map &lt; 0):\n        plt.imshow(Z_map, aspect='auto', cmap='RdBu_r', interpolation='nearest',\n                   norm=TwoSlopeNorm(vmin=Z_map.min(), vcenter=0, vmax=Z_map.max()))\n    else:\n        plt.imshow(Z_map, aspect='auto', cmap='Reds', interpolation='nearest')\n    plt.colorbar()\n    plt.title(f'{name} components (components x features)')\n    plt.ylabel('component')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Linear Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html",
    "href": "part1/taking_derivatives.html",
    "title": "5  Taking Derivatives with Automatic Differentiation",
    "section": "",
    "text": "5.1 Automatic Differentiation\nTaking derivatives with respect to functions or parameters is one of the most common and fundamental operations that we will need for Machine Learning (and in fact for Scientific Computing, in general). Throughout your life so far, you have probably learned about three main ways to compute derivatives:\nThis chapter won’t cover those approaches in detail, as its main goal is to introduce you to a fourth way to compute derivatives called Automatic Differentiation (AD). This approach inherits some of the benefits of analytical and symbolic differentiation, in that it computes exact derivatives (unlike numerical differentiation). It’s main drawback is that the effort used to compute the derivatives will be only useful for a single evaluation point (similar to numerical differentiation). Unlike symbolic differentiation, AD is very efficient and does not suffer from expression swell, although it does require extra bookkeeping to keep track of intermediate values and derivatives (as we will see), which can add some memory and computational overhead.\nWith this overview in mind, we can now introduce Automatic Differentiation using a simple example, and then later demonstrate how to use it via some practical examples.\nTo demonstrate how Automatic Differentiation works, let’s take a simple example function, for which we can easily compute the derivatives manually/analytically, so that we can check out results. Let’s consider the function:\n\\[\nf(x_1, x_2) = \\ln(x_1) + x_1 \\cdot x_2 - \\sin(x_2)\n\\]\nevaluated at \\(x_1 = 2, x_2 = 5\\).\nAnalytically, this function is simple enough that we could actually compute the partial derivatives manually: \\[\\frac{\\partial y}{\\partial x_0} = \\frac{1}{x_0} + x_1 = 0.5 + 5 = 5.5\\]\n\\[\\frac{\\partial y}{\\partial x_1} = x_0 - cos(x_1) = 2 - 0.284 = 1.716\\]\nHowever, for the sake of this example, we will use Automatic Differentiation to compute these derivatives instead. The first step of Automatic Differentiation is to build up a Computational Graph of the function using a library of easy to compute derivatives (e.g., \\(\\frac{\\partial}{\\partial x} x^n \\rightarrow n\\cdot x^{n-1}\\))",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives with Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#automatic-differentiation",
    "href": "part1/taking_derivatives.html#automatic-differentiation",
    "title": "5  Taking Derivatives with Automatic Differentiation",
    "section": "",
    "text": "5.1.1 Build the Computational Graph\nTo build the computational graph, we break the function down into its elementary operations step by step, starting from the beginning (i.e., the inputs to the function). We will introduce intermediate variables (\\(V_\\#\\)) to represent the outputs of these intermediate operations. Let’s define:\n\n\n\n\n\ngraph LR\n    %% Input nodes\n    V0((V0 = x1))\n    V1((V1 = x2))\n    %% Intermediate nodes\n    V2((V2 = ln V0))\n    V3((V3 = V0 x V1))\n    V4((V4 = -sin V1))\n    V5((V5 = V2 + V3))\n    V6((V6 = V5 + V4 = f))\n\n    %% Operations\n    V0 --&gt; V2\n    V0 --&gt; V3\n    V1 --&gt; V3\n    V1 --&gt; V4\n\n    %% Sum nodes\n    V2 --&gt; V5\n    V3 --&gt; V5\n    V5 --&gt; V6\n    V4 --&gt; V6\n\n\n\n\n\n\nNow with the graph in place, we can compute the first stage of Automatic Differentiation, which is the forward pass through the function.\n\n\n5.1.2 Compute the Forward Pass\nThe Forward Pass is simply evaluating the function at the given input values, but we will do this step by step, following the computational graph we just built. It will be useful to keep track of the intermediate values in a table for reference later. (In reality, the computer will do this for us, but we are doing it by hand here to illustrate the process.)\n\n\n\n\n\n\n\n\nNode\nDefinition\nValue\n\n\n\n\n\\(V0\\)\n\\(x_1\\)\n2\n\n\n\\(V1\\)\n\\(x_2\\)\n5\n\n\n\\(V2\\)\n\\(\\ln(V0)\\)\n\\(\\ln(2) \\approx 0.6931\\)\n\n\n\\(V3\\)\n\\(V0 \\cdot V1\\)\n\\(2 \\cdot 5 = 10\\)\n\n\n\\(V4\\)\n\\(-\\sin(V1)\\)\n\\(-\\sin(5) \\approx 0.9589\\)\n\n\n\\(V5\\)\n\\(V2 + V3\\)\n\\(0.6931 + 10 = 10.6931\\)\n\n\n\\(V6\\)\n\\(V5 + V4\\)\n\\(10.6931 + 0.9589 \\approx 11.6520\\)\n\n\n\nIf we pass in our initial points (x1, x2) through this forward pass, we can look at the final node (V6) to get the answer: \\[\nf(2, 5) \\approx 11.6520\n\\]\nGreat, this matches what we would expect. At this point, we have just evaluated the function forward, and we don’t yet have any derivatives. From here, things get interesting and bifurcate into two main types of Automatic Differentiation: Forward Mode AD and Backward Mode AD. Each of these has important but different uses for reasons that will become clear as we work through the example. Let’s start with Forward Mode AD. In both cases, we will start with the initial work we already did with the forward pass above.\n\n\n5.1.3 Computing Forward Mode AD (tangent propagation)\nForward Mode AD allows us to compute directional derivatives of the function, as well as that same directional derivative at any intermediate node in the computational graph. This is useful for computing derivatives of functions that have a small number of inputs (e.g., 1-10), but potentially a large number of outputs that we might be interested in. For example, if we were computing a trajectory of a dynamical system, we might want to know how the final state of the system changes with respect to some initial condition. In this case, the initial condition is the input, and the final state is the output. There might be many intermediate states along the way that we also want to know how they change with respect to the initial condition, such as the state or total energy at each time step. Forward Mode AD allows us to compute all of these derivatives in only a single pass through the computational graph.\nTo see how this works, we will introduce a new variable \\(\\dot{V}\\) to represent the derivative of each node with respect to some input direction. We will use the notation \\(\\dot{V} = \\frac{dV}{dx}\\), where \\(x\\) is some input variable.\nCase A: derivative wrt \\(x_1\\) (\\(\\dot{x}_1 = 1, \\dot{x}_2 = 0\\))\n\n\n\n\n\n\n\n\nNode\nDefinition\nValue\n\n\n\n\n\\(\\dot{V_0}\\)\n\\(\\dot{x_1}\\)\n1\n\n\n\\(\\dot{V_1}\\)\n\\(\\dot{x_2}\\)\n0\n\n\n\\(\\dot{V_2}\\)\n\\(d(\\ln V_0)/dV_0 = 1/V_0 \\dot{V_0}\\)\n\\((1/2)\\cdot 1 = 0.5\\)\n\n\n\\(\\dot{V_3}\\)\n\\(d(V_0 \\cdot V_1)/dV_0 = \\dot{V_0} V_1 + V_0 \\dot{V_1}\\)\n\\(5\\cdot1+2\\cdot0=5\\)\n\n\n\\(\\dot{V_4}\\)\n\\(-\\cos(V_1)\\dot{V_1}\\)\n\\(-\\cos(5)\\cdot 0 = 0\\)\n\n\n\\(\\dot{V_5}\\)\n\\(\\dot{V_3} + \\dot{V_2}\\)\n\\(0.5 + 5 = 5.5\\)\n\n\n\\(\\dot{V_6}\\)\n\\(\\dot{V_5} + \\dot{V_4}\\)\n\\(5.5 + 0 = 5.5\\)\n\n\n\n\\[\n\\frac{\\partial f}{\\partial x_1} = 5.5\n\\]\nCase B: derivative wrt \\(x_2\\) (\\(\\dot{x}_1 = 0, \\dot{x}_2 = 1\\))\nExercise: Fill in a similar table as above to compute the derivative of the function with respect to \\(x_2\\) using Forward Mode AD.\nYou can check your answer using the known analytical derivative of the function that you can compute by hand: \\[\n\\frac{\\partial f}{\\partial x_2} \\approx 1.7163\n\\]\nOK, great, we see that with some bookkeeping, we have correctly computed the derivatives of the function with respect to each input variable. Note that we had to do two passes through the computational graph to get both derivatives, since they were different directional derivatives. If we had a third input variable, we would need a third pass, and so on. This is why Forward Mode AD is best suited for functions with a small number of inputs.\nOn the flip side, if we were interested in the derivative of some intermediate node in the computational graph with respect to an input variable, we would have that information available as well. For example, if we wanted to know how \\(V3\\) changes with respect to \\(x_1\\), we can see from the table above that \\(\\frac{\\partial V3}{\\partial x_1} = 5\\). We got this in the process of computing the final function, so this derivative comes “along for the ride” without additional cost on our part. This is a powerful feature of Forward Mode AD that we will see is not available in Backward Mode AD: we can get a specific directional derivatives of any intermediate node from the computational graph via the same Forward Mode pass, but the cost of this scales with the number of input variables or number of directional derivatives we want to compute.\n\n\n5.1.4 Computing Reverse Mode AD (backpropagation)\nWhile Forward Mode AD efficiently found directional derivatives, if we wanted to compute the full gradient of the output with respect to all of the input variables, we would need to do a separate pass for each input variable. This means that the cost of computing the full gradient scales with the number of input variables. If we have a function with a large number of input variables (e.g., 1000s or more), this can be very expensive. To address this, we can use Reverse Mode AD, which will allow us to compute the full gradient of every input variable to a function using a single “backward pass” through the computational graph. This is particularly useful for functions that have a small number of outputs (e.g., 1-10), but a very large number of inputs, such as a Neural Network in Machine Learning or mesh coordinates in a Finite Element or Computational Fluid Dynamics simulation.\nTo see how this works, we will introduce a new variable \\(\\bar{V}\\) to represent the adjoint of each node with respect to the output. We will use the notation \\(\\bar{V} = \\frac{\\partial f}{\\partial V}\\), where \\(f\\) is the final output of the function. The adjoint represents how much a small change in that intermediate node would affect the final output of the function.\nSimilarly to Forward Mode AD, we will have to pick a specific output that we wish to compute the gradient with respect to. In this case, since we are computing the gradient of the output with respect to all input variables, we can set the final output node with a value of 1, i.e., \\(\\bar{V6} = 1\\). From there, we will propagate each adjoint backward through the computational graph using the chain rule.\n\n\n\n\n\n\n\n\nNode\nEquation for the adjoint\nValue\n\n\n\n\n\\(\\bar{V_6}\\)\n\\(\\frac{\\partial f}{\\partial V_6}\\)\n1\n\n\n\\(\\bar{V_5}\\)\n\\(\\frac{\\partial f}{\\partial V_6} \\frac{\\partial V_6}{\\partial V_5} = \\bar{V_6}1\\)\n1\n\n\n\\(\\bar{V_4}\\)\n\\(\\frac{\\partial f}{\\partial V_6} \\frac{\\partial V_6}{\\partial V_4} = \\bar{V_6}1\\)\n1\n\n\n\\(\\bar{V_3}\\)\n\\(\\frac{\\partial f}{\\partial V_5} \\frac{\\partial V_5}{\\partial V_3} = \\bar{V_5}1\\)\n1\n\n\n\\(\\bar{V_2}\\)\n\\(\\frac{\\partial f}{\\partial V_5} \\frac{\\partial V_5}{\\partial V_2} = \\bar{V_5}1\\)\n1\n\n\n\\(\\bar{V_1}\\)\nfrom V3: \\(\\bar{V_3}\\cdot V_1=2\\)  + from V4: \\(-\\cos(5)\\cdot \\bar{V_4} \\approx -0.2837\\)\n1.7163\n\n\n\\(\\bar{V_0}\\)\nfrom V2: \\(\\bar{V_2}(1/V_0)\\cdot1=0.5\\)  + from V3: \\(\\bar{V_3}\\cdot V_1=5\\)\n5.5\n\n\n\nWe can now verify the final gradients, which match our analytical solution:\n\\[\n\\nabla f(2,5) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2} \\right) = (5.5, \\, 1.7163)\n\\]\nHowever, unlike Forward Mode AD, we see that we now have access not only to the gradients of the input variables, but also to the gradients of all intermediate nodes in the computational graph, and we received all of them via the same amount of work/computation!",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives with Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#pytorch-autograd-example-with-simple-function",
    "href": "part1/taking_derivatives.html#pytorch-autograd-example-with-simple-function",
    "title": "5  Taking Derivatives with Automatic Differentiation",
    "section": "5.2 PyTorch Autograd Example with Simple Function",
    "text": "5.2 PyTorch Autograd Example with Simple Function\nOk let’s use automatic differentiation to compute a simple derivative of our earlier analytical function:\n\\[y = f(x_0,x_1) = \\ln(x_0) + x_0 \\cdot x_1 - \\sin(x_1)\\]\nAnd as with before, we’ll evaluate the derivative of this function at \\[x_0 = 2, x_1=5\\]\nWe can analytically compute the derivative and code it up so that we can verify accuracy later:\n\nimport numpy as np\ndef true_grad(x0,x1):\n    return np.array([\n        1/x0 + x1,\n        x0 - np.cos(x1)\n    ])\ntrue_grad(2,5)\n\narray([5.5       , 1.71633781])\n\n\nBut now let’s see how to use PyTorch to get this using Automatic Differentiation:\n\n5.2.1 Automatic Differentiation using PyTorch\n\nimport torch\nx = torch.tensor([2.0, 5.0], requires_grad=True)\nprint(x)\n\ntensor([2., 5.], requires_grad=True)\n\n\n\ndef f(x):\n    return torch.log(x[0]) + x[0]*x[1] - torch.sin(x[1])\ny = f(x)\nprint(y)\n\ntensor(11.6521, grad_fn=&lt;SubBackward0&gt;)\n\n\n\nx.grad\n\n\n# Now call the backward AD pass so that we can compute gradients\ny.backward()\n# Now we can ask for the gradient:\nx.grad\n\ntensor([5.5000, 1.7163])\n\n\nLet’s see how well it approximated the true gradient:\n\ntrue_grad(2,5) - x.grad.numpy()\n\narray([0.00000000e+00, 1.45108339e-08])\n\n\n\n\n5.2.2 Finite Differences using SciPy\nNow let’s compare this to computing the same gradient, but using Numerical Differentiation (specifically, Central Finite Differences):\n\nfrom scipy import optimize\nx_np = np.array([2.0, 5.0])\ndef f_np(x):\n    return np.log(x[0]) + x[0]*x[1] - np.sin(x[1])\n#y_np = f_np(x_np)\n# This computes finite differences opf f_np at x_np:\noptimize.approx_fprime(x_np, f_np, epsilon=1e-4)\n\narray([5.4999875 , 1.71628987])\n\n\nLet’s see how well it approximated the true gradient:\n\ntrue_grad(2,5) - optimize.approx_fprime(x_np, f_np, epsilon=1e-4)\n\narray([1.24995903e-05, 4.79457300e-05])\n\n\n\ndef numerical_error(e):\n    return true_grad(2,5) - optimize.approx_fprime(x_np, f_np, epsilon=e)\n#error = lambda e: true_grad(2,5) - optimize.approx_fprime(x_np, y_np, epsilon=e)\nepsilons = np.logspace(-13,1,num=21)\nerrors = [np.linalg.norm(numerical_error(e)) for e in epsilons]\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_context('poster')\nnp.random.seed(1)\n\nplt.figure()\nplt.loglog(epsilons,errors,marker='o')\nplt.xlabel('$\\epsilon$')\nplt.ylabel('Error')\nplt.title('Finite Difference Approximation')\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives with Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#pytorch-autograd-example-with-optimization",
    "href": "part1/taking_derivatives.html#pytorch-autograd-example-with-optimization",
    "title": "5  Taking Derivatives with Automatic Differentiation",
    "section": "5.3 PyTorch Autograd Example with Optimization",
    "text": "5.3 PyTorch Autograd Example with Optimization\nThis example shows how to use AD and PyTorch to perform gradient based optimization on a simple test function\n\n# Example of the McCormick Function\ndef mccormick(x):\n    return torch.sin(x[0]+x[1]) + (x[0]-x[1])**2 - 1.5*x[0]+2.5*x[1]+1\nf = mccormick\n\nx = torch.tensor([-4.0, 4.0], requires_grad=True)\ny = f(x)\n\nWhat does this function look like?\n\nX_plot = torch.meshgrid(torch.linspace(-5,4,100),torch.linspace(-5,4,100))\nx_plot,y_plot = X_plot\nplt.figure()\nplt.contour(x_plot,y_plot,f(X_plot))\nplt.show()\n\nc:\\Users\\fuge\\AppData\\Local\\miniforge3\\envs\\ml4me-student\\Lib\\site-packages\\torch\\functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4316.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n\n\n\n\n\n\n\n\nfig = plt.figure()\nax = plt.axes(projection='3d')\nax.contour3D(x_plot, y_plot, f(X_plot), 50, cmap='binary_r')\nax.view_init(40, 90)\nplt.show()\n\n\n\n\n\n\n\n\nNow let’s say I want to optimize this. I could compute the analytical derivative. Or, I could compute the backward-mode AD on the inputs:\n\n# Pick a starting point:\nx = torch.tensor([-4.0, 4.0], requires_grad=True)\n# Evaluate y\ny = f(x)\n# Now call the backward AD pass so that we can compute gradients\ny.backward()\n# Now we can get the gradient\nx.grad\n\ntensor([-16.5000,  19.5000])\n\n\nNow we just stick it in a loop and run SGD on it:\n\n# Take an initial guess at the optimum:\nx = torch.tensor([-4.0, 4.0], requires_grad=True)\n# Note that the true answer should be x_opt = [5, 5]\n# Initialize the optimizer\noptimizer = torch.optim.AdamW([x], lr=1)\nnum_steps = 50\nsteps = [np.array(x.detach().numpy())]\n# Take 10 steps\nfor i in range(num_steps):\n    optimizer.zero_grad()\n    y = f(x)\n    y.backward()\n    optimizer.step()\n    with torch.no_grad():\n        steps.append(np.array(x.detach().numpy()))\n        print(x)\nsteps = np.array(steps)\n\ntensor([-2.9600,  2.9600], requires_grad=True)\ntensor([-1.9481,  1.9436], requires_grad=True)\ntensor([-0.9857,  0.9653], requires_grad=True)\ntensor([-0.1051,  0.0453], requires_grad=True)\ntensor([ 0.6508, -0.7906], requires_grad=True)\ntensor([ 1.2368, -1.5138], requires_grad=True)\ntensor([ 1.6219, -2.0992], requires_grad=True)\ntensor([ 1.8020, -2.5325], requires_grad=True)\ntensor([ 1.7979, -2.8129], requires_grad=True)\ntensor([ 1.6423, -2.9514], requires_grad=True)\ntensor([ 1.3704, -2.9664], requires_grad=True)\ntensor([ 1.0147, -2.8804], requires_grad=True)\ntensor([ 0.6043, -2.7165], requires_grad=True)\ntensor([ 0.1659, -2.4983], requires_grad=True)\ntensor([-0.2759, -2.2482], requires_grad=True)\ntensor([-0.6981, -1.9879], requires_grad=True)\ntensor([-1.0796, -1.7375], requires_grad=True)\ntensor([-1.4026, -1.5143], requires_grad=True)\ntensor([-1.6534, -1.3326], requires_grad=True)\ntensor([-1.8238, -1.2020], requires_grad=True)\ntensor([-1.9113, -1.1271], requires_grad=True)\ntensor([-1.9188, -1.1078], requires_grad=True)\ntensor([-1.8539, -1.1396], requires_grad=True)\ntensor([-1.7274, -1.2146], requires_grad=True)\ntensor([-1.5524, -1.3224], requires_grad=True)\ntensor([-1.3433, -1.4512], requires_grad=True)\ntensor([-1.1152, -1.5883], requires_grad=True)\ntensor([-0.8830, -1.7215], requires_grad=True)\ntensor([-0.6606, -1.8395], requires_grad=True)\ntensor([-0.4605, -1.9328], requires_grad=True)\ntensor([-0.2926, -1.9947], requires_grad=True)\ntensor([-0.1641, -2.0210], requires_grad=True)\ntensor([-0.0788, -2.0109], requires_grad=True)\ntensor([-0.0372, -1.9664], requires_grad=True)\ntensor([-0.0366, -1.8921], requires_grad=True)\ntensor([-0.0720, -1.7947], requires_grad=True)\ntensor([-0.1359, -1.6823], requires_grad=True)\ntensor([-0.2198, -1.5637], requires_grad=True)\ntensor([-0.3146, -1.4478], requires_grad=True)\ntensor([-0.4113, -1.3430], requires_grad=True)\ntensor([-0.5017, -1.2561], requires_grad=True)\ntensor([-0.5791, -1.1926], requires_grad=True)\ntensor([-0.6384, -1.1558], requires_grad=True)\ntensor([-0.6769, -1.1466], requires_grad=True)\ntensor([-0.6937, -1.1639], requires_grad=True)\ntensor([-0.6901, -1.2045], requires_grad=True)\ntensor([-0.6693, -1.2638], requires_grad=True)\ntensor([-0.6353, -1.3361], requires_grad=True)\ntensor([-0.5934, -1.4148], requires_grad=True)\ntensor([-0.5490, -1.4935], requires_grad=True)\n\n\n\nplt.figure()\nplt.contour(x_plot,y_plot,f(X_plot))\nplt.plot(steps[:,0],steps[:,1],marker='+',label = \"Adam\")\nplt.legend()\nplt.title(\"Adam optimizer\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Take an initial guess at the optimum:\nx = torch.tensor([-4.0, 4.0], requires_grad=True)\n# Note that the true answer should be x_opt = [5, 5]\n# Initialize the optimizer\n# Here using LBFGS, which is much faster convergence on small problems\noptimizer = torch.optim.LBFGS([x],lr=0.05)\nnum_steps = 5\nsteps = [np.array(x.detach().numpy())]\n# Take 10 steps\nfor i in range(num_steps):\n    def closure():\n        optimizer.zero_grad()\n        y = f(x)\n        y.backward()\n        return y\n    optimizer.step(closure)\n    with torch.no_grad():\n            steps.append(np.array(x.detach().numpy()))\n            print(x)\nsteps = np.array(steps)\n\ntensor([-1.9382,  0.4356], requires_grad=True)\ntensor([-1.0402, -0.8307], requires_grad=True)\ntensor([-0.7223, -1.2888], requires_grad=True)\ntensor([-0.6097, -1.4543], requires_grad=True)\ntensor([-0.5696, -1.5139], requires_grad=True)\n\n\n\nplt.figure()\nplt.contour(x_plot,y_plot,f(X_plot))\nplt.scatter([-0.54719],[-1.54719],marker='*',label=\"Optima\")\nplt.plot(steps[:,0],steps[:,1],marker='+',label = \"LFBGS\")\nplt.legend()\nplt.title(\"LFBGS optimizer\")\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives with Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "part1/taking_derivatives.html#demonstration-of-ad-on-verlet-integration",
    "href": "part1/taking_derivatives.html#demonstration-of-ad-on-verlet-integration",
    "title": "5  Taking Derivatives with Automatic Differentiation",
    "section": "5.4 Demonstration of AD on Verlet Integration",
    "text": "5.4 Demonstration of AD on Verlet Integration\nThis notebook demonstrates how to use Automatic Differentiation to determine the gradients of the initial conditions of a dynamical system (in this case a damped oscillator). To do this, we will define a numerical routine (Verlet Integration) and then use Automatic Differentiation to back propagate the gradient information from the output (Total system energy) to the initial conditions.\nBelow is a typical simulation script that is not current set up for automatic differentiation:\n\n\n###############################################################################\nN = 1000\nt = np.linspace(0,10,N)\ndt = t[1] - t[0]\n\n###############################################################################\n# functions\ndef integrate_original(F,x0,v0,gamma):\n    ###########################################################################\n    # arrays are allocated and filled with zeros\n    Ef = 0\n    x = np.zeros(N)\n    v = np.zeros(N)\n    E = np.zeros(N)\n    \n    ###########################################################################    \n    # initial conditions\n    x[0] = x0\n    v[0] = v0\n    \n    ###########################################################################\n    # Do the Verlet Integration\n    fac1 = 1.0 - 0.5*gamma*dt\n    fac2 = 1.0/(1.0 + 0.5*gamma*dt)\n    \n    for i in range(N-1):\n        vn = fac1*fac2*v0 - fac2*dt*x0 + fac2*dt*F[i]\n        xn = x0 + dt*vn\n        Ef = 0.5*(x0**2 + ((v0 + vn)/2.0)**2)\n        v0 = vn\n        x0 = xn\n        # For Plotting/Debug\n\n        v[i + 1] = vn\n        x[i + 1] = xn\n        E[i] = Ef\n\n    Ef = 0.5*(x0**2 + v0**2)\n    \n    E[-1] = Ef\n    \n    ###########################################################################\n    # return solution\n    return ( (x0,v0,Ef) , (x,v,E) )\n\nNow note how we modify the code to include AD via PyTorch (see comments and references to torch):\n\n\n###############################################################################\nN = 1000\nt = np.linspace(0,10,N)\ndt = t[1] - t[0]\n\n###############################################################################\n# functions\ndef integrate(F,x0,v0,gamma):\n    ###########################################################################\n    # arrays are allocated and filled with zeros\n    #x = torch.tensor([0.0],requires_grad=True)\n    #v = torch.zeros(N)\n    Ef = torch.tensor([0.0],requires_grad=True)\n    x = np.zeros(N)\n    v = np.zeros(N)\n    E = np.zeros(N)\n    \n    ###########################################################################    \n    # initial conditions\n    with torch.no_grad():\n        x[0] = x0\n        v[0] = v0\n    \n    ###########################################################################\n    # Do the Verlet Integration\n    fac1 = 1.0 - 0.5*gamma*dt\n    fac2 = 1.0/(1.0 + 0.5*gamma*dt)\n    \n    for i in range(N-1):\n        vn = fac1*fac2*v0 - fac2*dt*x0 + fac2*dt*F[i]\n        xn = x0 + dt*vn\n        Ef = 0.5*(x0**2 + ((v0 + vn)/2.0)**2)\n        v0 = vn\n        x0 = xn\n        # For Plotting/Debug\n        with torch.no_grad():\n            v[i + 1] = vn\n            x[i + 1] = xn\n            E[i] = Ef\n    \n    Ef = 0.5*(x0**2 + v0**2)\n    with torch.no_grad():\n        E[-1] = Ef\n    \n    ###########################################################################\n    # return solution\n    return ( (x0,v0,Ef) , (x,v,E) )\n\n\n###############################################################################\n# Do the actual numerical integration\nF = np.zeros(N)\n\nx_initial = torch.tensor([1.0], requires_grad = True)\nv_initial = torch.tensor([1.0], requires_grad = True)\n#gamma = torch.tensor([0.05], requires_grad = True)\ngamma = torch.tensor([.05], requires_grad = True)\n((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma) # x0 = 0.0, v0 = 1.0, gamma = 0.0\n\n\n#((),(x2,v2,E2)) = integrate(F,0.0,1.0,0.05) # x0 = 0.0, v0 = 1.0, gamma = 0.01\n#((),(x2,v2,E2)) = integrate(F,0.0,1.0,0.05) # x0 = 0.0, v0 = 1.0, gamma = 0.01\n\n#((),(x3,v3,E3)) = integrate(F,0.0,1.0,0.4) # x0 = 0.0, v0 = 1.0, gamma = 0.5\n\n###############################################################################\ndef plot_solution(x1,E1,gamma):\n    plt.rcParams[\"axes.grid\"] = True\n    plt.rcParams['font.size'] = 14\n    plt.rcParams['axes.labelsize'] = 18\n    plt.figure()\n    plt.subplot(211)\n    plt.plot(t,x1)\n    #plt.plot(t,x2)\n    #plt.plot(t,x3)\n    plt.ylabel(\"x(t)\")\n\n    plt.subplot(212)\n    plt.plot(t,E1,label=fr\"$\\gamma = {float(gamma):.2f}$\")\n    #plt.plot(t,E2,label=r\"$\\gamma = 0.01$\")\n    #plt.plot(t,E3,label=r\"$\\gamma = 0.5$\")\n    plt.ylim(0,1.0)\n    plt.ylabel(\"E(t)\")\n\n    plt.xlabel(\"Time\")\n    plt.legend(loc=\"center right\")\n\n    plt.tight_layout()\n\nplot_solution(x1,E1,gamma)\n\n\n\n\n\n\n\n\n\nprint(Ef)\nEf.backward(retain_graph=True)\n\ntensor([0.6137], grad_fn=&lt;MulBackward0&gt;)\n\n\nNow let’s print the gradient of the system Energy with respect to some of the initial conditions:\n\nprint(gamma.grad)\nprint(v_initial.grad)\nprint(x_initial.grad)\n\ntensor([-5.9562])\ntensor([0.6026])\ntensor([0.6248])\n\n\n\nprint(vf)\nvf.backward()\n\ntensor([-0.2245], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nprint(gamma.grad)\nprint(v_initial.grad)\nprint(x_initial.grad)\n\ntensor([-4.7535])\ntensor([-0.0437])\ntensor([1.0466])\n\n\n\n5.4.1 Optimizing the Damping Coefficient via SGD and AD\nFirst let’s just get a visual intuition for how \\(\\gamma\\) affects the final energy:\n\nnum_gammas = 30\ngamma_plot = np.logspace(-0.5,1.0,num_gammas)\nEfs = np.zeros(num_gammas)\nfor i,g in enumerate(gamma_plot):\n    ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,g)\n    Efs[i] = Ef\n\n\nplt.figure()\nplt.semilogx(gamma_plot,Efs)\nplt.xlabel(r'$\\gamma$')\nplt.ylabel('E Final')\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that there is a pretty flat plateau from around \\(\\gamma=1\\) until around \\(\\gamma=3\\).\nNow let’s use our backward mode AD to actually optimize \\(\\gamma\\) directly by calling backward on the output of the final energy of the Verlet integration of the ODE:\n\n# This part is just a helper library for plotting\ndef plot_optimization(initial_gamma, num_steps, optimizer, opt_kwargs={}):\n    # Take an initial guess at the optimum:\n    gamma = torch.tensor([initial_gamma], requires_grad=True)\n\n    # Initialize the optimizer\n    optimizer = optimizer([gamma], **opt_kwargs)\n\n    steps = [ ] # Here is where we'll keep track of the steps\n    # Take num_steps of the optimizer\n    for i in range(num_steps):\n        # This function runs an actual optimization step. We wrap it in closure so that optimizers\n        # that take multiple function calls per step can do so -- e.g., LBFGS.\n        def closure():\n            # Get rid of the existing gradients on the tape\n            optimizer.zero_grad()\n            # Run the numerical integration -- this is the forward pass through the solver\n            ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma) # x0 = 0.0, v0 = 1.0, gamma = 0.0\n            # Compute the backward mode AD pass\n            Ef.backward()\n            return Ef\n        # Now ask the optimizer to take a step\n        optimizer.step(closure)\n        \n        # The below part is just for printing/plotting. We call torch.no_grad() here to signify that\n        # we do not need to track this as part of the gradient operations. That is, these parts will not\n        # be added to the computational graph or used for backward mode AD.\n        with torch.no_grad():\n            #print(gamma)\n            # Run again just to plot the solution for this gamma\n            ((xf,vf,Ef),(x1,v1,E1)) = integrate(F,x_initial,v_initial,gamma)\n            #print(Ef)\n            if num_steps&gt;10 and i%3==0:\n                plot_solution(x1,E1,gamma)\n            # Add it to steps so that we can see/plot it later.\n            steps.append(np.array(gamma.detach().numpy()))\n            \n    steps = np.array(steps)\n    return steps\n\n\n5.4.1.1 ADAM Example\n\nsteps_Adam = plot_optimization(initial_gamma=0.05, \n                               num_steps=20,\n                               optimizer=torch.optim.AdamW,\n                               opt_kwargs={'lr':0.5})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.2 SGD Example\n\nsteps_SGD = plot_optimization(initial_gamma=0.05, \n                               num_steps=20,\n                               optimizer=torch.optim.SGD,\n                               opt_kwargs={'lr':0.05,'momentum':0.9})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.3 LBFGS Example\n(Warning: Per-run solves of LBFGS take a while, so don’t set num_steps too high here)\n\nsteps_LBFGS = plot_optimization(initial_gamma=0.05,\n                                num_steps=5,\n                                optimizer=torch.optim.LBFGS,\n                                opt_kwargs={'lr':0.3})\n\n\n\n\n5.4.2 Compare the steps taken\n\nplt.figure()\nplt.semilogx(gamma_plot,Efs)\nsteps_Adam = steps_Adam.flatten()\nplt.plot(steps_Adam,[0.0]*len(steps_Adam),'|', color = 'r', label = 'Adam Steps')\nplt.plot(steps_SGD,[0.005]*len(steps_SGD),'|', color = 'g', label = 'SGD Steps')\nplt.plot(steps_LBFGS,[0.01]*len(steps_LBFGS),'|', color = 'k', label = 'LBFGS Steps')\nplt.xlabel(r'$\\gamma$')\nplt.ylabel('E Final')\nplt.title(\"Comparison of Optimizers\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Taking Derivatives with Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html",
    "href": "part1/distribution_distance.html",
    "title": "6  Measuring Distribution Distances",
    "section": "",
    "text": "6.1 A Toy Example: Comparing Two Gaussians\nThis notebook explores several ways of quantifying distances between one-dimensional probability distributions. We will contrast their mathematical definitions, visualize how they react to parameter changes, and compare the gradients that each metric produces. The goal is to build intuition for why the choice of divergence matters in optimization problems such as generative modeling or system identification. We will explore the following:\nWe will compare a target distribution \\(p(x)\\) against a model distribution \\(q_\\mu(x)\\) whose mean \\(\\mu\\) we can adjust. Unless stated otherwise, the target will be the standard normal distribution. You can change that later to a uniform or a bimodal mixture to reveal different behaviours.\nOur baseline parameterization keeps the model standard deviation fixed at \\(\\sigma_q = 1\\). Adjusting the mean already highlights how asymmetric divergences produce different gradient signals.\nShow Code\n# Baseline visualization: standard normal target vs. shifted model Gaussian\nTARGET = TARGETS[\"gaussian\"]\nMODEL_STD = torch.tensor(1.0, dtype=DEFAULT_DTYPE, device=device)\nMU_BASE = torch.tensor(1.5, dtype=DEFAULT_DTYPE, device=device)\n\nwith torch.no_grad():\n    target_pdf = TARGET.pdf(GRID_X)\n    model_pdf = gaussian_pdf_torch(GRID_X, MU_BASE, MODEL_STD)\n\nplot_distribution_pair(GRID_X, target_pdf, model_pdf, title=\"Baseline Comparison\", target_label=TARGET.name, model_label=f\"Gaussian(μ={MU_BASE.item():.1f}, σ=1.0)\")",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#kullbackleibler-divergence",
    "href": "part1/distribution_distance.html#kullbackleibler-divergence",
    "title": "6  Measuring Distribution Distances",
    "section": "6.2 Kullback–Leibler Divergence",
    "text": "6.2 Kullback–Leibler Divergence\nForward KL divergence compares how much mass the target distribution assigns relative to the model: \\[\n\\mathrm{KL}(p\\,\\|\\,q) = \\int p(x) \\log \\frac{p(x)}{q(x)}\\,dx.\n\\] Because it integrates expectations under \\(p\\), it heavily penalizes situations where \\(q(x)\\) is small while \\(p(x)\\) is large, even if the model allocates extra mass elsewhere.\n\n6.2.1 Analytical Gradient for Gaussian vs. Gaussian\nFor \\(p = \\mathcal{N}(\\mu_p, \\sigma_p^2)\\) and \\(q = \\mathcal{N}(\\mu, \\sigma_q^2)\\), forward KL admits a closed form: \\[\n\\mathrm{KL}(p\\,\\|\\,q) = \\log \\frac{\\sigma_q}{\\sigma_p} + \\frac{\\sigma_p^2 + (\\mu_p - \\mu)^2}{2\\sigma_q^2} - \\frac{1}{2}.\n\\] Differentiating w.r.t. the model mean gives \\[\n\\frac{\\partial}{\\partial \\mu} \\mathrm{KL}(p\\,\\|\\,q) = \\frac{\\mu - \\mu_p}{\\sigma_q^2},\n\\] which pushes \\(\\mu\\) directly toward \\(\\mu_p\\). We will evaluate the integral numerically to keep the pipeline consistent when we later switch to mixtures.\n\n\nShow Code\n# Forward and reverse KL implementations using numerical integration\n_target_pdf_cache: Dict[str, torch.Tensor] = {}\n\n\ndef get_target_pdf(spec: DistributionSpec) -&gt; torch.Tensor:\n    if spec.name not in _target_pdf_cache:\n        with torch.no_grad():\n            _target_pdf_cache[spec.name] = spec.pdf(GRID_X).detach()\n    return _target_pdf_cache[spec.name]\n\n\ndef forward_kl(mu: torch.Tensor, target: DistributionSpec, model_std: float = 1.0) -&gt; torch.Tensor:\n    p_pdf = get_target_pdf(target)\n    q_pdf = gaussian_pdf_torch(GRID_X, mu, model_std)\n    integrand = p_pdf * torch.log((p_pdf + EPS) / (q_pdf + EPS))\n    return torch.trapz(integrand, GRID_X)\n\n\ndef reverse_kl(mu: torch.Tensor, target: DistributionSpec, model_std: float = 1.0) -&gt; torch.Tensor:\n    p_pdf = get_target_pdf(target)\n    q_pdf = gaussian_pdf_torch(GRID_X, mu, model_std)\n    integrand = q_pdf * torch.log((q_pdf + EPS) / (p_pdf + EPS))\n    return torch.trapz(integrand, GRID_X)\n\n\nNow let’s look at how the forward KL behaves as the model mean \\(\\mu\\) shifts away from the target mean \\(\\mu_p = 0\\). We will plot the KL divergence, and its gradient, as we shift the model mean from -15 to 15. What do you notice about the behavior of the KL Divergence and it’s gradient?\n\n# Forward KL distance curve for standard normal target\n##### Change the following to see what happens #######\nmu_values = np.linspace(-15.0, 15.0, 121)\nmodel_std = 1.0\n####################################\n\nforward_metrics, forward_grads = evaluate_metric_curve(\n    mu_values,\n    lambda mu: forward_kl(mu, TARGETS[\"gaussian\"], model_std=model_std),\n)\nplot_metric_and_gradient(mu_values, forward_metrics, forward_grads, metric_name=\"Forward KL\", target_desc=TARGETS[\"gaussian\"].name)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Forward KL on Gaussian Targets\n\n\n\n\nWhat happens to both the KL Divergence and its gradient when the model mean is close to the target mean? What about when it is far away? Why is this?\nHow would the curve change if we widened the model variance? Try modifying model_std in the code cell above and re-running it.\n\n\n\n\n\nShow Code\n# Interactive widget for forward KL\nif widgets is None:\n    maybe_display(None)\nelse:\n    target_dropdown = widgets.Dropdown(\n        options=[(spec.name, key) for key, spec in TARGETS.items()],\n        value=\"gaussian\",\n        description=\"Target\",\n        style={\"description_width\": \"initial\"},\n    )\n    mu_slider = widgets.FloatSlider(value=0.0, min=-15.0, max=15.0, step=0.5, description=\"μ\")\n    std_slider = widgets.FloatSlider(value=1.0, min=0.4, max=2.5, step=0.1, description=\"σ_q\")\n    output = widgets.Output()\n\n    def _update_forward_kl(*_):\n        with output:\n            output.clear_output(wait=True)\n            target = TARGETS[target_dropdown.value]\n            mu_val = torch.tensor(mu_slider.value, dtype=DEFAULT_DTYPE, device=device, requires_grad=True)\n            std_val = float(std_slider.value)\n            kl_value = forward_kl(mu_val, target, model_std=std_val)\n            grad = torch.autograd.grad(kl_value, mu_val)[0].detach().cpu().item()\n            target_pdf = get_target_pdf(target)\n            model_pdf = gaussian_pdf_torch(GRID_X, mu_slider.value, std_val)\n\n            mu_values = np.linspace(mu_slider.min, mu_slider.max, 181)\n            metrics_curve, grads_curve = evaluate_metric_curve(\n                mu_values,\n                lambda mu: forward_kl(mu, target, model_std=std_val),\n            )\n\n            grid_np = GRID_X.cpu().numpy()\n            target_np = target_pdf.cpu().numpy()\n            model_np = model_pdf.cpu().numpy()\n\n            fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\n            axes[0].plot(grid_np, target_np, label=target.name, linewidth=2)\n            axes[0].plot(grid_np, model_np, label=f\"Model μ={mu_slider.value:.1f}, σ={std_val:.1f}\", linestyle=\"--\", linewidth=2)\n            axes[0].set_title(\"Density comparison\")\n            axes[0].set_xlabel(\"x\")\n            axes[0].set_ylabel(\"Density\")\n            axes[0].legend()\n\n            axes[1].plot(mu_values, metrics_curve, color=\"tab:blue\", linewidth=2)\n            axes[1].axvline(mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[1].scatter([mu_slider.value], [kl_value.detach().cpu().item()], color=\"tab:blue\")\n            axes[1].set_title(\"Forward KL vs. μ\")\n            axes[1].set_xlabel(\"Model mean μ\")\n            axes[1].set_ylabel(\"Forward KL\")\n\n            axes[2].plot(mu_values, grads_curve, color=\"tab:red\", linewidth=2)\n            axes[2].axhline(0.0, color=\"black\", linestyle=\":\")\n            axes[2].axvline(mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[2].scatter([mu_slider.value], [grad], color=\"tab:red\")\n            axes[2].set_title(\"Gradient d(KL)/dμ\")\n            axes[2].set_xlabel(\"Model mean μ\")\n            axes[2].set_ylabel(\"Gradient\")\n\n            fig.suptitle(\n                f\"Forward KL = {kl_value.detach().cpu().item():.3f} | Gradient = {grad:.3f}\",\n                fontsize=14,\n            )\n            fig.tight_layout()\n            plt.show()\n\n    for control in (target_dropdown, mu_slider, std_slider):\n        control.observe(_update_forward_kl, names=\"value\")\n\n    _update_forward_kl()\n    maybe_display(widgets.VBox([widgets.HTML(\"&lt;h4&gt;Interactive Forward KL Explorer&lt;/h4&gt;\"), target_dropdown, mu_slider, std_slider, output]))\n\n\n\n\n\n\n\n6.2.2 Forward vs. Reverse KL on a Bimodal Target\nReverse KL, \\(\\mathrm{KL}(q\\,\\|\\,p)\\), prefers to avoid regions where the model would place mass but the target does not. On multimodal targets this often leads to mode seeking: the optimizer chooses one mode to cover while ignoring others.\n\n6.2.2.1 Mathematical Overview\nThe reverse KL is defined as \\[\n\\mathrm{KL}(q\\,\\|\\,p) = \\int q(x) \\log \\frac{q(x)}{p(x)}\\,dx,\n\\] so the expectation is taken under the model distribution \\(q\\). If \\(q(x)\\) places mass where \\(p(x)\\) is negligible, the logarithm term explodes and strongly penalizes those regions. Conversely, areas where \\(p\\) has mass but \\(q\\) does not contribute nothing, which is why the optimizer can collapse onto a single mode when matching multimodal targets.\nLet us compare the two on a mixture of Gaussians.\n\n\nShow Code\n# Forward vs. reverse KL on a bimodal target\nmixture_target = TARGETS[\"gaussian_mixture\"]\ncomponent_means = mixture_target.metadata.get(\"means\", (-2.5, 2.5))\ncomponent_stds = mixture_target.metadata.get(\"stds\", (0.6, 0.6))\ncomponent_weights = mixture_target.metadata.get(\"weights\", (0.5, 0.5))\n\nwith torch.no_grad():\n    mixture_pdf = mixture_target.pdf(GRID_X).cpu().numpy()\n    component_curves = []\n    for mean, std, weight in zip(component_means, component_stds, component_weights):\n        component_pdf = weight * gaussian_pdf_torch(GRID_X, mean, std)\n        component_curves.append((mean, weight, component_pdf.cpu().numpy()))\n\ngrid_np = GRID_X.cpu().numpy()\nfig_density, ax_density = plt.subplots(figsize=(10, 4))\nax_density.plot(grid_np, mixture_pdf, label=\"Target mixture\", linewidth=3, color=\"tab:blue\")\nfor mean, weight, comp_pdf in component_curves:\n    ax_density.plot(\n        grid_np,\n        comp_pdf,\n        linestyle=\"--\",\n        linewidth=1.5,\n        label=f\"Component μ={mean:.1f}, w={weight:.2f}\",\n    )\nax_density.set_title(\"Gaussian Mixture Target and Component Contributions\")\nax_density.set_xlabel(\"x\")\nax_density.set_ylabel(\"Density\")\nax_density.legend()\nfig_density.tight_layout()\nplt.show()\n\nmu_values = np.linspace(-4.0, 4.0, 161)\nforward_metrics_mix, forward_grads_mix = evaluate_metric_curve(\n    mu_values,\n    lambda mu: forward_kl(mu, mixture_target, model_std=1.0),\n)\nreverse_metrics_mix, reverse_grads_mix = evaluate_metric_curve(\n    mu_values,\n    lambda mu: reverse_kl(mu, mixture_target, model_std=1.0),\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True)\naxes[0].plot(mu_values, forward_metrics_mix, label=\"Forward KL\", linewidth=2)\naxes[0].plot(mu_values, reverse_metrics_mix, label=\"Reverse KL\", linewidth=2)\naxes[0].set_title(\"KL Divergences vs. μ (Mixture Target)\")\naxes[0].set_xlabel(\"Model mean μ\")\naxes[0].set_ylabel(\"Divergence\")\naxes[0].legend()\n\naxes[1].plot(mu_values, forward_grads_mix, label=\"Forward KL gradient\", linewidth=2)\naxes[1].plot(mu_values, reverse_grads_mix, label=\"Reverse KL gradient\", linewidth=2)\naxes[1].axhline(0.0, color=\"black\", linestyle=\":\")\naxes[1].set_title(\"Gradient signals\")\naxes[1].set_xlabel(\"Model mean μ\")\naxes[1].set_ylabel(\"Gradient\")\naxes[1].legend()\n\nfig.suptitle(\"Mode-Covering vs. Mode-Seeking Behaviour\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipReflection: Mode Covering vs. Mode Seeking\n\n\n\n\nWhy does the forward KL gradient stay non-zero even between the two modes?\n\nIdentify the regions where the reverse KL gradient vanishes. How does that explain a model that only tracks one mode?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#jensenshannon-divergence",
    "href": "part1/distribution_distance.html#jensenshannon-divergence",
    "title": "6  Measuring Distribution Distances",
    "section": "6.3 Jensen–Shannon Divergence",
    "text": "6.3 Jensen–Shannon Divergence\nThe Jensen–Shannon (JS) divergence symmetrizes KL by averaging the two distributions: \\(m(x) = \\tfrac{1}{2}(p(x)+q(x))\\). It stays finite even when supports do not match and is bounded between 0 and \\(\\log 2\\).\n\n6.3.1 Mathematical Formulation\n\\[\n\\mathrm{JS}(p, q) = \\tfrac{1}{2} \\mathrm{KL}(p\\,\\|\\,m) + \\tfrac{1}{2} \\mathrm{KL}(q\\,\\|\\,m), \\quad m = \\tfrac{1}{2}(p + q).\n\\] We will compute JS numerically on the same grid. Autograd gives us gradients through the logarithms as long as we keep everything in PyTorch tensors.\n\ndef jensen_shannon(mu: torch.Tensor, target: DistributionSpec, model_std: float = 1.0) -&gt; torch.Tensor:\n    p_pdf = get_target_pdf(target)\n    q_pdf = gaussian_pdf_torch(GRID_X, mu, model_std)\n    m_pdf = 0.5 * (p_pdf + q_pdf)\n    term_p = p_pdf * torch.log((p_pdf + EPS) / (m_pdf + EPS))\n    term_q = q_pdf * torch.log((q_pdf + EPS) / (m_pdf + EPS))\n    return 0.5 * torch.trapz(term_p, GRID_X) + 0.5 * torch.trapz(term_q, GRID_X)\n\n\n\nShow Code\n# JS divergence vs. μ for Gaussian target compared to forward KL\nmu_values = np.linspace(-15.0, 15.0, 121)\njs_metrics, js_grads = evaluate_metric_curve(\n    mu_values,\n    lambda mu: jensen_shannon(mu, TARGETS[\"gaussian\"], model_std=1.0),\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True)\naxes[0].plot(mu_values, forward_metrics, label=\"Forward KL\", linewidth=2)\naxes[0].plot(mu_values, js_metrics, label=\"JS\", linewidth=2)\naxes[0].set_title(\"Forward KL vs. JS\")\naxes[0].set_xlabel(\"Model mean μ\")\naxes[0].set_ylabel(\"Divergence\")\naxes[0].legend()\n\naxes[1].plot(mu_values, forward_grads, label=\"Forward KL gradient\", linewidth=2)\naxes[1].plot(mu_values, js_grads, label=\"JS gradient\", linewidth=2)\naxes[1].axhline(0.0, color=\"black\", linestyle=\":\")\naxes[1].set_title(\"Gradient Comparison\")\naxes[1].set_xlabel(\"Model mean μ\")\naxes[1].set_ylabel(\"Gradient\")\naxes[1].legend()\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipReflection: Bounded Divergence, Softer Gradients\n\n\n\n\nJS approaches \\(\\log 2\\) when the two distributions barely overlap. Where in the plot does that happen?\n\nCompare the slope near \\(\\mu=0\\) for JS and forward KL. What do you notice?\nWhat happens when the distributions move far away from each other?\n\n\n\n\n\nShow Code\n# Interactive JS explorer\nif widgets is None:\n    maybe_display(None)\nelse:\n    js_target_dropdown = widgets.Dropdown(\n        options=[(spec.name, key) for key, spec in TARGETS.items()],\n        value=\"gaussian\",\n        description=\"Target\",\n        style={\"description_width\": \"initial\"},\n    )\n    js_mu_slider = widgets.FloatSlider(value=0.0, min=-4.0, max=4.0, step=0.1, description=\"μ\")\n    js_std_slider = widgets.FloatSlider(value=1.0, min=0.5, max=2.5, step=0.1, description=\"σ_q\")\n    js_output = widgets.Output()\n\n    def _update_js(*_):\n        with js_output:\n            js_output.clear_output(wait=True)\n            target = TARGETS[js_target_dropdown.value]\n            mu_val = torch.tensor(js_mu_slider.value, dtype=DEFAULT_DTYPE, device=device, requires_grad=True)\n            std_val = float(js_std_slider.value)\n            js_val = jensen_shannon(mu_val, target, model_std=std_val)\n            grad = torch.autograd.grad(js_val, mu_val)[0].detach().cpu().item()\n            target_pdf = get_target_pdf(target)\n            model_pdf = gaussian_pdf_torch(GRID_X, js_mu_slider.value, std_val)\n\n            mu_values = np.linspace(js_mu_slider.min, js_mu_slider.max, 181)\n            metrics_curve, grads_curve = evaluate_metric_curve(\n                mu_values,\n                lambda mu: jensen_shannon(mu, target, model_std=std_val),\n            )\n\n            grid_np = GRID_X.cpu().numpy()\n            target_np = target_pdf.cpu().numpy()\n            model_np = model_pdf.cpu().numpy()\n\n            fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\n            axes[0].plot(grid_np, target_np, label=target.name, linewidth=2)\n            axes[0].plot(grid_np, model_np, label=f\"Model μ={js_mu_slider.value:.1f}, σ={std_val:.1f}\", linestyle=\"--\", linewidth=2)\n            axes[0].set_title(\"Density comparison\")\n            axes[0].set_xlabel(\"x\")\n            axes[0].set_ylabel(\"Density\")\n            axes[0].legend()\n\n            axes[1].plot(mu_values, metrics_curve, color=\"tab:blue\", linewidth=2)\n            axes[1].axvline(js_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[1].scatter([js_mu_slider.value], [js_val.detach().cpu().item()], color=\"tab:blue\")\n            axes[1].set_title(\"JS divergence vs. μ\")\n            axes[1].set_xlabel(\"Model mean μ\")\n            axes[1].set_ylabel(\"JS divergence\")\n\n            axes[2].plot(mu_values, grads_curve, color=\"tab:red\", linewidth=2)\n            axes[2].axhline(0.0, color=\"black\", linestyle=\":\")\n            axes[2].axvline(js_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[2].scatter([js_mu_slider.value], [grad], color=\"tab:red\")\n            axes[2].set_title(\"Gradient d(JS)/dμ\")\n            axes[2].set_xlabel(\"Model mean μ\")\n            axes[2].set_ylabel(\"Gradient\")\n\n            fig.suptitle(\n                f\"JS = {js_val.detach().cpu().item():.3f} | Gradient = {grad:.3f}\",\n                fontsize=14,\n            )\n            fig.tight_layout()\n            plt.show()\n\n    for control in (js_target_dropdown, js_mu_slider, js_std_slider):\n        control.observe(_update_js, names=\"value\")\n\n    _update_js()\n    maybe_display(widgets.VBox([widgets.HTML(\"&lt;h4&gt;Interactive JS Explorer&lt;/h4&gt;\"), js_target_dropdown, js_mu_slider, js_std_slider, js_output]))",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#maximum-mean-discrepancy-mmd",
    "href": "part1/distribution_distance.html#maximum-mean-discrepancy-mmd",
    "title": "6  Measuring Distribution Distances",
    "section": "6.4 Maximum Mean Discrepancy (MMD)",
    "text": "6.4 Maximum Mean Discrepancy (MMD)\nMMD measures the distance between mean embeddings of distributions in a reproducing kernel Hilbert space (RKHS). With a kernel \\(k\\), \\[\n\\mathrm{MMD}^2(p, q) = \\mathbb{E}_{x, x' \\sim p}[k(x, x')] - 2\\, \\mathbb{E}_{x \\sim p, y \\sim q}[k(x, y)] + \\mathbb{E}_{y, y' \\sim q}[k(y, y')].\n\\] We will use the radial basis function (RBF) kernel \\[\nk_\\sigma(x, y) = \\exp\\!\\left(-\\tfrac{(x - y)^2}{2\\sigma_k^2}\\right),\n\\] which assigns high similarity when two samples are closer than the bandwidth \\(\\sigma_k\\) and decays smoothly otherwise. A small \\(\\sigma_k\\) emphasises very local discrepancies—only nearby points contribute—so MMD becomes sensitive to fine-grained differences but can miss global shifts. A large \\(\\sigma_k\\) blurs the notion of neighbourhood, making the kernel respond to broader trends while down-weighting local wiggles. Balancing \\(\\sigma_k\\) therefore tunes whether we care more about microstructure or coarse alignment between \\(p\\) and \\(q\\).\n\n# Shared noise tensors for deterministic MMD estimates\nBASE_NOISE_NORMAL = torch.randn(4096, dtype=DEFAULT_DTYPE, device=device)\nBASE_NOISE_UNIFORM = torch.linspace(0.0, 1.0, 4096, dtype=DEFAULT_DTYPE, device=device)\n\n\ndef sample_with_matching_noise(spec: DistributionSpec, sample_count: int) -&gt; torch.Tensor:\n    if \"Uniform\" in spec.name:\n        base = BASE_NOISE_UNIFORM[:sample_count]\n    else:\n        base = BASE_NOISE_NORMAL[:sample_count]\n    return spec.sampler(sample_count, base)\n\n\ndef rbf_kernel(x: torch.Tensor, y: torch.Tensor, bandwidth: float) -&gt; torch.Tensor:\n    bw = torch.as_tensor(bandwidth, dtype=DEFAULT_DTYPE, device=device)\n    diff = x[:, None] - y[None, :]\n    return torch.exp(-0.5 * diff.pow(2) / (bw ** 2 + EPS))\n\n\ndef maximum_mean_discrepancy(\n    mu: torch.Tensor,\n    target: DistributionSpec,\n    model_std: float = 1.0,\n    bandwidth: float = 1.0,\n    sample_count: int = 512,\n) -&gt; torch.Tensor:\n    samples_p = sample_with_matching_noise(target, sample_count)\n    base_gauss = BASE_NOISE_NORMAL[:sample_count]\n    samples_q = mu + model_std * base_gauss\n    xx = rbf_kernel(samples_p, samples_p, bandwidth)\n    yy = rbf_kernel(samples_q, samples_q, bandwidth)\n    xy = rbf_kernel(samples_p, samples_q, bandwidth)\n    xx = xx - torch.diag(torch.diag(xx))\n    yy = yy - torch.diag(torch.diag(yy))\n    n = sample_count\n    mmd2 = xx.sum() / (n * (n - 1) + EPS) - 2.0 * xy.mean() + yy.sum() / (n * (n - 1) + EPS)\n    return torch.sqrt(torch.clamp(mmd2, min=0.0))\n\n\n\nShow Code\n# MMD vs. μ for different kernel bandwidths\nmu_values = np.linspace(-15.0, 15.0, 121)\nbandwidths = [0.3, 1.0, 2.5]\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True)\nfor bw in bandwidths:\n    metrics, grads = evaluate_metric_curve(\n        mu_values,\n        lambda mu, bw=bw: maximum_mean_discrepancy(mu, TARGETS[\"gaussian\"], model_std=1.0, bandwidth=bw),\n    )\n    axes[0].plot(mu_values, metrics, label=f\"σ_k={bw}\")\n    axes[1].plot(mu_values, grads, label=f\"σ_k={bw}\")\n\naxes[0].set_title(\"MMD vs. μ (Gaussian target)\")\naxes[0].set_xlabel(\"Model mean μ\")\naxes[0].set_ylabel(\"MMD\")\naxes[0].legend()\n\naxes[1].set_title(\"MMD gradient vs. μ\")\naxes[1].set_xlabel(\"Model mean μ\")\naxes[1].set_ylabel(\"Gradient\")\naxes[1].legend()\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect of MMD Kernel Bandwidth\n\n\n\n\nWhich bandwidth reacts most strongly to small shifts in \\(\\mu\\)?\n\nObserve how the gradient flattens for large \\(\\sigma_k\\). When might that be desirable?\n\nHow would you select \\(\\sigma_k\\) automatically in a learning system?\n\n\n\n\n\nShow Code\n# Interactive MMD explorer\nif widgets is None:\n    maybe_display(None)\nelse:\n    mmd_target_dropdown = widgets.Dropdown(\n        options=[(spec.name, key) for key, spec in TARGETS.items()],\n        value=\"gaussian\",\n        description=\"Target\",\n        style={\"description_width\": \"initial\"},\n    )\n    mmd_mu_slider = widgets.FloatSlider(value=0.0, min=-15.0, max=15.0, step=0.5, description=\"μ\")\n    mmd_std_slider = widgets.FloatSlider(value=1.0, min=0.5, max=2.5, step=0.1, description=\"σ_q\")\n    mmd_bw_slider = widgets.FloatLogSlider(value=1.0, base=10, min=-1, max=1, step=0.05, description=\"σ_k\")\n    mmd_output = widgets.Output()\n\n    def _update_mmd(*_):\n        with mmd_output:\n            mmd_output.clear_output(wait=True)\n            target = TARGETS[mmd_target_dropdown.value]\n            mu_val = torch.tensor(mmd_mu_slider.value, dtype=DEFAULT_DTYPE, device=device, requires_grad=True)\n            std_val = float(mmd_std_slider.value)\n            bw_val = float(mmd_bw_slider.value)\n            mmd_val = maximum_mean_discrepancy(mu_val, target, model_std=std_val, bandwidth=bw_val)\n            grad = torch.autograd.grad(mmd_val, mu_val)[0].detach().cpu().item()\n\n            mu_values = np.linspace(mmd_mu_slider.min, mmd_mu_slider.max, 181)\n            metrics_curve, grads_curve = evaluate_metric_curve(\n                mu_values,\n                lambda mu: maximum_mean_discrepancy(mu, target, model_std=std_val, bandwidth=bw_val),\n            )\n\n            target_pdf = get_target_pdf(target)\n            model_pdf = gaussian_pdf_torch(GRID_X, mmd_mu_slider.value, std_val)\n\n            grid_np = GRID_X.cpu().numpy()\n            target_np = target_pdf.cpu().numpy()\n            model_np = model_pdf.cpu().numpy()\n\n            fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\n            axes[0].plot(grid_np, target_np, label=target.name, linewidth=2)\n            axes[0].plot(grid_np, model_np, label=f\"Model μ={mmd_mu_slider.value:.1f}, σ={std_val:.1f}\", linestyle=\"--\", linewidth=2)\n            axes[0].set_title(\"Density comparison\")\n            axes[0].set_xlabel(\"x\")\n            axes[0].set_ylabel(\"Density\")\n            axes[0].legend()\n\n            axes[1].plot(mu_values, metrics_curve, color=\"tab:blue\", linewidth=2)\n            axes[1].axvline(mmd_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[1].scatter([mmd_mu_slider.value], [mmd_val.detach().cpu().item()], color=\"tab:blue\")\n            axes[1].set_title(\"MMD vs. μ\")\n            axes[1].set_xlabel(\"Model mean μ\")\n            axes[1].set_ylabel(\"MMD\")\n\n            axes[2].plot(mu_values, grads_curve, color=\"tab:red\", linewidth=2)\n            axes[2].axhline(0.0, color=\"black\", linestyle=\":\")\n            axes[2].axvline(mmd_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[2].scatter([mmd_mu_slider.value], [grad], color=\"tab:red\")\n            axes[2].set_title(\"Gradient d(MMD)/dμ\")\n            axes[2].set_xlabel(\"Model mean μ\")\n            axes[2].set_ylabel(\"Gradient\")\n\n            fig.suptitle(\n                f\"MMD = {mmd_val.detach().cpu().item():.3f} | Gradient = {grad:.3f}\",\n                fontsize=14,\n            )\n            fig.tight_layout()\n            plt.show()\n\n    for control in (mmd_target_dropdown, mmd_mu_slider, mmd_std_slider, mmd_bw_slider):\n        control.observe(_update_mmd, names=\"value\")\n\n    _update_mmd()\n    maybe_display(\n        widgets.VBox(\n            [\n                widgets.HTML(\"&lt;h4&gt;Interactive MMD Explorer&lt;/h4&gt;\"),\n                mmd_target_dropdown,\n                mmd_mu_slider,\n                mmd_std_slider,\n                mmd_bw_slider,\n                mmd_output,\n            ]\n        )\n    )",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#optimal-transport-wasserstein-2-distance",
    "href": "part1/distribution_distance.html#optimal-transport-wasserstein-2-distance",
    "title": "6  Measuring Distribution Distances",
    "section": "6.5 Optimal Transport (Wasserstein-2 Distance)",
    "text": "6.5 Optimal Transport (Wasserstein-2 Distance)\nOptimal Transport (OT) frames distribution comparison as the problem of moving probability mass from a source distribution \\(q\\) to a target distribution \\(p\\) at minimal cost. Given a ground cost \\(c(x, y)\\) measuring how expensive it is to transport mass from \\(x\\) to \\(y\\), the quadratic OT problem seeks a coupling \\(\\pi(x, y)\\) with marginals \\(q\\) and \\(p\\) that minimises the total transport cost: \\[\nW_2^2(p, q) = \\inf_{\\pi \\in \\Pi(p, q)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} \\|x - y\\|^2 \\, d\\pi(x, y),\n\\] where \\(\\Pi(p, q)\\) denotes the set of joint distributions whose marginals are \\(p\\) and \\(q\\). This formulation differs from divergence-based distances above (KL, JS, MMD) because it explicitly models where probability mass must move, rather than comparing densities pointwise or via kernel similarities. As a result, OT remains informative even when supports do not overlap: the metric still reflects how far mass must travel to align the distributions.\nIn one dimension, the optimal coupling sorts both distributions and pairs quantiles. The squared Wasserstein-2 distance simplifies to \\[\nW_2^2(p, q) = \\int_0^1 \\left|F_p^{-1}(u) - F_q^{-1}(u)\\right|^2 \\, du,\n\\] with \\(F^{-1}\\) denoting the quantile function. For Gaussians, this has a closed form \\(W_2^2 = (\\mu_p - \\mu)^2 + (\\sigma_p - \\sigma_q)^2\\). We will compute it via quantile functions to keep the numerical pipeline consistent for non-Gaussian targets.\n\n\nShow Code\ndef gaussian_quantile(u: torch.Tensor, mean: float, std: float) -&gt; torch.Tensor:\n    mean_t = torch.as_tensor(mean, dtype=DEFAULT_DTYPE, device=device)\n    std_t = torch.as_tensor(std, dtype=DEFAULT_DTYPE, device=device)\n    return mean_t + std_t * math.sqrt(2.0) * torch.erfinv(2.0 * u - 1.0)\n\n\ndef wasserstein_2(\n    mu: torch.Tensor,\n    target: DistributionSpec,\n    model_std: float = 1.0,\n    sample_count: int = 1024,\n) -&gt; torch.Tensor:\n    if target.name.startswith(\"Gaussian\"):  # use closed form\n        mu_p = target.metadata.get(\"mean\", 0.0)\n        sigma_p = target.metadata.get(\"std\", 1.0)\n        sigma_q = model_std\n        diff_sq = (mu - mu_p) ** 2 + (sigma_q - sigma_p) ** 2\n        return torch.sqrt(torch.clamp(diff_sq, min=0.0))\n\n    # Empirical quantile-based estimate for non-Gaussian targets\n    u = torch.linspace(0.0 + 0.5 / sample_count, 1.0 - 0.5 / sample_count, sample_count, dtype=DEFAULT_DTYPE, device=device)\n    samples_p = torch.sort(sample_with_matching_noise(target, sample_count))[0]\n    samples_q = torch.sort(mu + model_std * BASE_NOISE_NORMAL[:sample_count])[0]\n    return torch.sqrt(torch.mean((samples_p - samples_q) ** 2))\n\n\n\n\nShow Code\n# Wasserstein-2 distance vs. μ for Gaussian target\nmu_values = np.linspace(-15.0, 15.0, 121)\nw2_metrics, w2_grads = evaluate_metric_curve(\n    mu_values,\n    lambda mu: wasserstein_2(mu, TARGETS[\"gaussian\"], model_std=1.0),\n)\nplot_metric_and_gradient(mu_values, w2_metrics, w2_grads, metric_name=\"W2\", target_desc=TARGETS[\"gaussian\"].name)\n\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Visualizing the optimal transport map in 1D\nmu_demo = 2.5\nsample_count = 64\nu = torch.linspace(0.0 + 0.5 / sample_count, 1.0 - 0.5 / sample_count, sample_count, dtype=DEFAULT_DTYPE, device=device)\nsource_samples = torch.sort(mu_demo + BASE_NOISE_NORMAL[:sample_count])[0].cpu().numpy()\ntarget_samples = torch.sort(sample_with_matching_noise(TARGETS[\"gaussian\"], sample_count))[0].cpu().numpy()\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.scatter(source_samples, np.zeros_like(source_samples), label=\"Model\", color=\"tab:orange\", s=40)\nax.scatter(target_samples, np.ones_like(target_samples), label=\"Target\", color=\"tab:blue\", s=40)\nfor xs, xt in zip(source_samples, target_samples):\n    ax.plot([xs, xt], [0.0, 1.0], color=\"gray\", alpha=0.4)\nax.set_title(\"Optimal Transport Matching for μ = 2.5\")\nax.set_yticks([0, 1])\nax.set_yticklabels([\"Model\", \"Target\"])\nax.set_xlabel(\"x\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipReflection: Transport as Matching\n\n\n\n\nHow does the transport map change if you halve the model variance?\n\nCompare the length of the transport lines with the Wasserstein distance you computed earlier.\n\nWhat would happen if the target distribution had gaps (e.g., a mixture)? Sketch the expected matching pattern.\n\n\n\n\n\nShow Code\n# Interactive Wasserstein explorer\nif widgets is None:\n    maybe_display(None)\nelse:\n    w2_target_dropdown = widgets.Dropdown(\n        options=[(spec.name, key) for key, spec in TARGETS.items()],\n        value=\"gaussian\",\n        description=\"Target\",\n        style={\"description_width\": \"initial\"},\n    )\n    w2_mu_slider = widgets.FloatSlider(value=0.0, min=-15.0, max=15.0, step=0.1, description=\"μ\")\n    w2_std_slider = widgets.FloatSlider(value=1.0, min=0.5, max=2.5, step=0.1, description=\"σ_q\")\n    w2_output = widgets.Output()\n\n    def _update_w2(*_):\n        with w2_output:\n            w2_output.clear_output(wait=True)\n            target = TARGETS[w2_target_dropdown.value]\n            mu_val = torch.tensor(w2_mu_slider.value, dtype=DEFAULT_DTYPE, device=device, requires_grad=True)\n            std_val = float(w2_std_slider.value)\n            w2_val = wasserstein_2(mu_val, target, model_std=std_val)\n            grad = torch.autograd.grad(w2_val, mu_val)[0].detach().cpu().item()\n\n            mu_values = np.linspace(w2_mu_slider.min, w2_mu_slider.max, 181)\n            metrics_curve, grads_curve = evaluate_metric_curve(\n                mu_values,\n                lambda mu: wasserstein_2(mu, target, model_std=std_val),\n            )\n\n            target_pdf = get_target_pdf(target)\n            model_pdf = gaussian_pdf_torch(GRID_X, w2_mu_slider.value, std_val)\n\n            grid_np = GRID_X.cpu().numpy()\n            target_np = target_pdf.cpu().numpy()\n            model_np = model_pdf.cpu().numpy()\n\n            fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\n            axes[0].plot(grid_np, target_np, label=target.name, linewidth=2)\n            axes[0].plot(grid_np, model_np, label=f\"Model μ={w2_mu_slider.value:.1f}, σ={std_val:.1f}\", linestyle=\"--\", linewidth=2)\n            axes[0].set_title(\"Density comparison\")\n            axes[0].set_xlabel(\"x\")\n            axes[0].set_ylabel(\"Density\")\n            axes[0].legend()\n\n            axes[1].plot(mu_values, metrics_curve, color=\"tab:blue\", linewidth=2)\n            axes[1].axvline(w2_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[1].scatter([w2_mu_slider.value], [w2_val.detach().cpu().item()], color=\"tab:blue\")\n            axes[1].set_title(\"Wasserstein-2 vs. μ\")\n            axes[1].set_xlabel(\"Model mean μ\")\n            axes[1].set_ylabel(\"W2 distance\")\n\n            axes[2].plot(mu_values, grads_curve, color=\"tab:red\", linewidth=2)\n            axes[2].axhline(0.0, color=\"black\", linestyle=\":\")\n            axes[2].axvline(w2_mu_slider.value, color=\"black\", linestyle=\":\")\n            axes[2].scatter([w2_mu_slider.value], [grad], color=\"tab:red\")\n            axes[2].set_title(\"Gradient d(W2)/dμ\")\n            axes[2].set_xlabel(\"Model mean μ\")\n            axes[2].set_ylabel(\"Gradient\")\n\n            fig.suptitle(\n                f\"W2 = {w2_val.detach().cpu().item():.3f} | Gradient = {grad:.3f}\",\n                fontsize=14,\n            )\n            fig.tight_layout()\n            plt.show()\n\n            if not target.name.startswith(\"Gaussian\"):\n                transport_samples = 80\n                src = torch.sort(mu_val.detach() + std_val * BASE_NOISE_NORMAL[:transport_samples])[0].cpu().numpy()\n                tgt = torch.sort(sample_with_matching_noise(target, transport_samples))[0].cpu().numpy()\n                fig_map, ax_map = plt.subplots(figsize=(8, 4))\n                ax_map.scatter(src, np.zeros_like(src), label=\"Model\", color=\"tab:orange\", s=25)\n                ax_map.scatter(tgt, np.ones_like(tgt), label=\"Target\", color=\"tab:blue\", s=25)\n                for xs, xt in zip(src, tgt):\n                    ax_map.plot([xs, xt], [0.0, 1.0], color=\"gray\", alpha=0.3)\n                ax_map.set_yticks([0, 1])\n                ax_map.set_yticklabels([\"Model\", \"Target\"])\n                ax_map.set_title(\"Transport plan snapshot\")\n                ax_map.legend(loc=\"upper left\")\n                fig_map.tight_layout()\n                plt.show()\n\n    for control in (w2_target_dropdown, w2_mu_slider, w2_std_slider):\n        control.observe(_update_w2, names=\"value\")\n\n    _update_w2()\n    maybe_display(widgets.VBox([widgets.HTML(\"&lt;h4&gt;Interactive Wasserstein Explorer&lt;/h4&gt;\"), w2_target_dropdown, w2_mu_slider, w2_std_slider, w2_output]))",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#information-geometry-vs.-optimal-transport",
    "href": "part1/distribution_distance.html#information-geometry-vs.-optimal-transport",
    "title": "6  Measuring Distribution Distances",
    "section": "6.6 Information Geometry vs. Optimal Transport",
    "text": "6.6 Information Geometry vs. Optimal Transport\nInformation Geometry treats families of probability distributions as curved manifolds equipped with the Fisher information metric. Intuitively, it asks: how sensitive are likelihoods to infinitesimal parameter changes? The geometry that emerges is tailored to statistical inference—geodesics correspond to paths that keep models maximally informative, and inner products reflect the Cramér–Rao notion of efficiency. This is why information-geometric tools appear in natural gradient descent, variational inference, and sensor placement problems where we care about how much information parameters carry about data.\nBecause the Fisher–Rao metric lives on the parameter manifold, it is invariant to reparameterisations and emphasises directions where the distribution changes statistically rather than spatially. In contrast, Optimal Transport metrics such as \\(W_2\\) operate directly on the sample space and quantify how far probability mass must move. OT is therefore ideal when spatial structure matters—think of matching shapes, aligning time-series histograms, or enforcing smooth transport maps in PDE-constrained problems.\nWhen would you pick one over the other? - Use Information Geometry when optimizing probabilistic models with latent variables or exponential-family structure, where natural gradients provide preconditioning aligned with likelihood curvature. - Use OT when discrepancies in physical space are key, such as calibrating simulators to sensor data, training generative models that must respect spatial coherence, or comparing distributions with disjoint supports. It is also useful when the target distributions do not have an analytical form but samples are available.\nLet us compare the two on Gaussian families and visualize the geodesic paths they induce.\n\n\nShow Code\ntry:\n    from geomstats.geometry.normal_distribution import NormalDistributions\n    from geomstats.learning.frechet_mean import FrechetMean\n    GEOMSTATS_AVAILABLE = True\nexcept Exception:\n    GEOMSTATS_AVAILABLE = False\n\n\ndef fisher_rao_distance(mu1: float, sigma1: float, mu2: float, sigma2: float) -&gt; float:\n    term = (sigma1 ** 2 + sigma2 ** 2 + (mu1 - mu2) ** 2) / (2.0 * sigma1 * sigma2)\n    term = max(term, 1.0)\n    return math.sqrt(2.0) * math.acosh(term)\n\n\ndef ot_geodesic(mu1: float, sigma1: float, mu2: float, sigma2: float, num_points: int = 50) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    ts = np.linspace(0.0, 1.0, num_points)\n    mus = (1 - ts) * mu1 + ts * mu2\n    sigmas = (1 - ts) * sigma1 + ts * sigma2\n    return ts, mus, sigmas\n\n\ndef fisher_rao_geodesic(mu1: float, sigma1: float, mu2: float, sigma2: float, num_points: int = 50) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    ts = np.linspace(0.0, 1.0, num_points)\n    if GEOMSTATS_AVAILABLE:\n        manifold = NormalDistributions()\n        path = manifold.geodesic(initial_point=np.array([mu1, sigma1 ** 2]), end_point=np.array([mu2, sigma2 ** 2]))\n        geodesic_points = np.array([path(t) for t in ts])\n        mus = geodesic_points[:, 0]\n        sigmas = np.sqrt(np.abs(geodesic_points[:, 1]))\n        return ts, mus, sigmas\n    # Fallback: interpolate in natural parameters (μ/σ, log σ)\n    theta1 = np.array([mu1 / sigma1, math.log(sigma1)])\n    theta2 = np.array([mu2 / sigma2, math.log(sigma2)])\n    thetas = np.outer(1 - ts, theta1) + np.outer(ts, theta2)\n    sigmas = np.exp(thetas[:, 1])\n    mus = thetas[:, 0] * sigmas\n    return ts, mus, sigmas\n\n\n\n\nShow Code\n# Compare Wasserstein-2 and Fisher-Rao distances/geodesics\nif widgets is None:\n    mu1, sigma1 = 0.0, 1.0\n    mu2, sigma2 = 2.5, 0.5\n\n    w2_val = wasserstein_2(\n        torch.tensor(mu2, dtype=DEFAULT_DTYPE, device=device),\n        make_gaussian(mean=mu1, std=sigma1),\n        model_std=sigma2,\n    ).item()\n    fr_val = fisher_rao_distance(mu1, sigma1, mu2, sigma2)\n    print(f\"W2(p, q) = {w2_val:.3f}\\nFisher–Rao(p, q) = {fr_val:.3f}\")\n\n    ts_ot, mus_ot, sigmas_ot = ot_geodesic(mu1, sigma1, mu2, sigma2, num_points=200)\n    ts_fr, mus_fr, sigmas_fr = fisher_rao_geodesic(mu1, sigma1, mu2, sigma2, num_points=200)\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    axes[0].plot(ts_ot, mus_ot, label=\"OT geodesic\", linewidth=2)\n    axes[0].plot(ts_fr, mus_fr, label=\"Fisher–Rao geodesic\", linewidth=2)\n    axes[0].set_xlabel(\"t\")\n    axes[0].set_ylabel(\"Mean μ(t)\")\n    axes[0].set_title(\"Mean evolution\")\n    axes[0].legend()\n\n    axes[1].plot(ts_ot, sigmas_ot, label=\"OT geodesic\", linewidth=2)\n    axes[1].plot(ts_fr, sigmas_fr, label=\"Fisher–Rao geodesic\", linewidth=2)\n    axes[1].set_xlabel(\"t\")\n    axes[1].set_ylabel(\"Std σ(t)\")\n    axes[1].set_title(\"Scale evolution\")\n    axes[1].legend()\n\n    fig.tight_layout()\n    plt.show()\n\n    sample_ts = np.linspace(0.0, 1.0, 6)\n    colors = plt.cm.viridis(sample_ts)\n    grid_np = GRID_X.cpu().numpy()\n\n    fig_interp, interp_axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n    for t_val, color in zip(sample_ts, colors):\n        mu_t_ot = np.interp(t_val, ts_ot, mus_ot)\n        sigma_t_ot = np.interp(t_val, ts_ot, sigmas_ot)\n        pdf_ot = gaussian_pdf_torch(GRID_X, mu_t_ot, sigma_t_ot).cpu().numpy()\n        interp_axes[0].plot(grid_np, pdf_ot, color=color, linewidth=2, label=f\"t={t_val:.2f}\")\n\n    interp_axes[0].set_title(\"OT interpolation (W2 geodesic)\")\n    interp_axes[0].set_xlabel(\"x\")\n    interp_axes[0].set_ylabel(\"Density\")\n    interp_axes[0].legend(loc=\"upper right\", ncol=2)\n\n    for t_val, color in zip(sample_ts, colors):\n        mu_t_fr = np.interp(t_val, ts_fr, mus_fr)\n        sigma_t_fr = np.interp(t_val, ts_fr, sigmas_fr)\n        pdf_fr = gaussian_pdf_torch(GRID_X, mu_t_fr, sigma_t_fr).cpu().numpy()\n        interp_axes[1].plot(grid_np, pdf_fr, color=color, linewidth=2, label=f\"t={t_val:.2f}\")\n\n    interp_axes[1].set_title(\"Information-geometry interpolation (Fisher–Rao)\")\n    interp_axes[1].set_xlabel(\"x\")\n    interp_axes[1].legend(loc=\"upper right\", ncol=2)\n\n    fig_interp.tight_layout()\n    plt.show()\nelse:\n    ig_mu1_slider = widgets.FloatSlider(value=0.0, min=-4.0, max=4.0, step=0.1, description=\"μ₁\")\n    ig_sigma1_slider = widgets.FloatSlider(value=1.0, min=0.2, max=3.0, step=0.05, description=\"σ₁\")\n    ig_mu2_slider = widgets.FloatSlider(value=2.5, min=-4.0, max=4.0, step=0.1, description=\"μ₂\")\n    ig_sigma2_slider = widgets.FloatSlider(value=0.5, min=0.2, max=3.0, step=0.05, description=\"σ₂\")\n    ig_output = widgets.Output()\n\n    def _update_geodesic(*_):\n        with ig_output:\n            ig_output.clear_output(wait=True)\n            mu1 = float(ig_mu1_slider.value)\n            sigma1 = float(ig_sigma1_slider.value)\n            mu2 = float(ig_mu2_slider.value)\n            sigma2 = float(ig_sigma2_slider.value)\n\n            w2_val = wasserstein_2(\n                torch.tensor(mu2, dtype=DEFAULT_DTYPE, device=device),\n                make_gaussian(mean=mu1, std=sigma1),\n                model_std=sigma2,\n            ).item()\n            fr_val = fisher_rao_distance(mu1, sigma1, mu2, sigma2)\n            print(f\"W2(p, q) = {w2_val:.3f}\\nFisher–Rao(p, q) = {fr_val:.3f}\")\n\n            ts_ot, mus_ot, sigmas_ot = ot_geodesic(mu1, sigma1, mu2, sigma2, num_points=200)\n            ts_fr, mus_fr, sigmas_fr = fisher_rao_geodesic(mu1, sigma1, mu2, sigma2, num_points=200)\n\n            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n            axes[0].plot(ts_ot, mus_ot, label=\"OT geodesic\", linewidth=2)\n            axes[0].plot(ts_fr, mus_fr, label=\"Fisher–Rao geodesic\", linewidth=2)\n            axes[0].set_xlabel(\"t\")\n            axes[0].set_ylabel(\"Mean μ(t)\")\n            axes[0].set_title(\"Mean evolution\")\n            axes[0].legend()\n\n            axes[1].plot(ts_ot, sigmas_ot, label=\"OT geodesic\", linewidth=2)\n            axes[1].plot(ts_fr, sigmas_fr, label=\"Fisher–Rao geodesic\", linewidth=2)\n            axes[1].set_xlabel(\"t\")\n            axes[1].set_ylabel(\"Std σ(t)\")\n            axes[1].set_title(\"Scale evolution\")\n            axes[1].legend()\n\n            fig.tight_layout()\n            plt.show()\n\n            sample_ts = np.linspace(0.0, 1.0, 6)\n            colors = plt.cm.viridis(sample_ts)\n            grid_np = GRID_X.cpu().numpy()\n\n            fig_interp, interp_axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n            for t_val, color in zip(sample_ts, colors):\n                mu_t_ot = np.interp(t_val, ts_ot, mus_ot)\n                sigma_t_ot = np.interp(t_val, ts_ot, sigmas_ot)\n                pdf_ot = gaussian_pdf_torch(GRID_X, mu_t_ot, sigma_t_ot).cpu().numpy()\n                interp_axes[0].plot(grid_np, pdf_ot, color=color, linewidth=2, label=f\"t={t_val:.2f}\")\n\n            interp_axes[0].set_title(\"OT interpolation (W2 geodesic)\")\n            interp_axes[0].set_xlabel(\"x\")\n            interp_axes[0].set_ylabel(\"Density\")\n            interp_axes[0].legend(loc=\"upper right\", ncol=2)\n\n            for t_val, color in zip(sample_ts, colors):\n                mu_t_fr = np.interp(t_val, ts_fr, mus_fr)\n                sigma_t_fr = np.interp(t_val, ts_fr, sigmas_fr)\n                pdf_fr = gaussian_pdf_torch(GRID_X, mu_t_fr, sigma_t_fr).cpu().numpy()\n                interp_axes[1].plot(grid_np, pdf_fr, color=color, linewidth=2, label=f\"t={t_val:.2f}\")\n\n            interp_axes[1].set_title(\"Information-geometry interpolation (Fisher–Rao)\")\n            interp_axes[1].set_xlabel(\"x\")\n            interp_axes[1].legend(loc=\"upper right\", ncol=2)\n\n            fig_interp.tight_layout()\n            plt.show()\n\n    for control in (ig_mu1_slider, ig_sigma1_slider, ig_mu2_slider, ig_sigma2_slider):\n        control.observe(_update_geodesic, names=\"value\")\n\n    _update_geodesic()\n    controls = widgets.HBox([widgets.VBox([ig_mu1_slider, ig_sigma1_slider]), widgets.VBox([ig_mu2_slider, ig_sigma2_slider])])\n    maybe_display(widgets.VBox([widgets.HTML(\"&lt;h4&gt;IG vs. OT Geodesic Explorer&lt;/h4&gt;\"), controls, ig_output]))\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Comparison of OT versus Fisher–Rao\n\n\n\n\nOT interpolates linearly in the mean; Fisher–Rao bends the path. How might that influence optimization trajectories?\n\nWhich metric is more sensitive to changes in variance vs. mean in this example?\n\nIf you needed to regularize a generative model toward a reference Gaussian, which geometry would you prefer and why?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/distribution_distance.html#wrap-up",
    "href": "part1/distribution_distance.html#wrap-up",
    "title": "6  Measuring Distribution Distances",
    "section": "6.7 Wrap-Up",
    "text": "6.7 Wrap-Up\nWe inspected five prominent distribution distances, paying attention to the gradients they induce for a simple Gaussian parameter. Key takeaways: - KL divergences emphasize support mismatch and can be asymmetric in how they penalize missing mass. - JS divergence softens gradients and stays bounded, making it attractive for adversarial training. - MMD exposes the role of kernel bandwidth in shaping sensitivity to local vs. global structure. - Wasserstein distance reasons about mass transport and yields interpretable gradient flows. - Fisher–Rao geometry offers a complementary, information-theoretic notion of proximity with different geodesics.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measuring Distribution Distances</span>"
    ]
  },
  {
    "objectID": "part1/introduction_to_inference.html",
    "href": "part1/introduction_to_inference.html",
    "title": "7  Introduction to Inference",
    "section": "",
    "text": "7.1 Motivation and Toy Problem Definition\nLearning Objectives\nBy the end of this notebook, you will:\nThe Big Picture\nImagine you have sparse sensor measurements from a mechanical system. You want to:\nThis notebook shows you how to do this using Bayesian inference, starting with simple linear models and progressing to flexible neural networks where variational inference becomes essential.\nLet’s start with a fundamental question: when you fit a model to data, how confident should you be in its predictions?\nIn traditional machine learning, we find a single “best” set of parameters. But in engineering, we often need to know: - How uncertain are we about the model? - Where in the input space are predictions reliable? - What happens if we had slightly different data?\nBayesian inference answers these questions by maintaining a distribution over models rather than picking a single one.",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "part1/introduction_to_inference.html#motivation-and-toy-problem-definition",
    "href": "part1/introduction_to_inference.html#motivation-and-toy-problem-definition",
    "title": "7  Introduction to Inference",
    "section": "",
    "text": "7.1.1 The Dataset: Nonlinear Sensor Measurements\nWe’ll use a simple nonlinear function to represent sensor data: \\[y = \\sin(x) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.1^2)\\]\nThis represents, for example: - Cyclic behavior (vibrations, temperatures, etc.) - Noisy measurements - Sparse observations (we won’t measure everywhere)\nWe will be interested in understanding how the model can capture the uncertainty of its prediction.\n\n\nShow Code\n# Setup and imports\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom typing import Tuple\nimport ipywidgets as widgets\n\n# Set style\nsns.set_style('whitegrid')\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams['savefig.dpi'] = 300\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n\n&lt;torch._C.Generator at 0x280584e30f0&gt;\n\n\n\n\nShow Code\n# Generate nonlinear dataset\ndef generate_nonlinear_data(n_samples: int = 20, \n                           noise_std: float = 0.1,\n                           x_range: Tuple[float, float] = (-3, 3)) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate sparse noisy samples from a sinusoidal function.\n    \n    This simulates sensor measurements from a cyclic process.\n    \n    Args:\n        n_samples: Number of observations\n        noise_std: Standard deviation of measurement noise\n        x_range: Input domain (min, max)\n        \n    Returns:\n        x: Input locations (n_samples,)\n        y: Noisy measurements (n_samples,)\n    \"\"\"\n    x_min, x_max = x_range\n    \n    # Sample input locations (sparse, irregular spacing)\n    x = np.random.uniform(x_min, x_max, n_samples)\n    \n    # True underlying function (unknown to the model)\n    y_true = np.sin(x)\n    \n    # Add measurement noise\n    y = y_true + noise_std * np.random.randn(n_samples)\n    \n    # Sort for easier visualization\n    sort_idx = np.argsort(x)\n    x = x[sort_idx]\n    y = y[sort_idx]\n    \n    return x, y\n\n# Generate training data\nn_train = 20\nnoise_std = 0.1\nx_train, y_train = generate_nonlinear_data(n_train, noise_std)\n\n# Create dense grid for visualization (the \"true\" function)\nx_test = np.linspace(-3, 3, 200)\ny_test_true = np.sin(x_test)\n\n# Visualize the dataset\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot true function\nax.plot(x_test, y_test_true, 'k-', linewidth=2, label='True function: $y = \\sin(x)$', alpha=0.7)\n\n# Plot noisy training observations\nax.scatter(x_train, y_train, s=80, c='red', edgecolors='black', \n           linewidth=1.5, zorder=5, label=f'Training data (n={n_train})')\n\n# Shade noise region\nax.fill_between(x_test, y_test_true - 2*noise_std, y_test_true + 2*noise_std,\n                alpha=0.2, color='gray', label=f'±2sigma noise band (sigma={noise_std})')\n\nax.set_xlabel('Input $x$', fontsize=13)\nax.set_ylabel('Output $y$', fontsize=13)\nax.set_title('Nonlinear Sensor Measurements', fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='upper right')\nax.grid(True, alpha=0.3)\nax.set_xlim([-3.2, 3.2])\nax.set_ylim([-1.5, 1.5])\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "part1/introduction_to_inference.html#linear-bayesian-regression",
    "href": "part1/introduction_to_inference.html#linear-bayesian-regression",
    "title": "7  Introduction to Inference",
    "section": "7.2 Linear Bayesian Regression",
    "text": "7.2 Linear Bayesian Regression\nLet’s start with the simplest approach: linear regression with Bayesian inference.\n\n7.2.1 The Model\nWe assume a linear relationship: \\[y = w_0 + w_1 x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\nIn Bayesian inference, we place a prior over the weights: \\[p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{0}, \\alpha^{-1} \\mathbf{I})\\]\nAfter seeing data \\((X, Y)\\), the posterior over weights is: \\[p(\\mathbf{w} | X, Y) = \\mathcal{N}(\\mathbf{m}_N, \\mathbf{S}_N)\\]\nwhere (for linear-Gaussian models, we can derive it exactly): \\[\\mathbf{S}_N = (\\alpha \\mathbf{I} + \\beta \\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1}\\] \\[\\mathbf{m}_N = \\beta \\mathbf{S}_N \\mathbf{\\Phi}^T \\mathbf{y}\\]\nwhere \\(\\beta = 1/\\sigma^2\\) is the noise precision and \\(\\mathbf{\\Phi}\\) is the design matrix.\n\n\n7.2.2 The Posterior Predictive Distribution\nFor a new input \\(x_*\\), the predictive distribution integrates over all possible weights: \\[p(y_* | x_*, X, Y) = \\int p(y_* | x_*, \\mathbf{w}) p(\\mathbf{w} | X, Y) d\\mathbf{w}\\]\nFor linear-Gaussian models, this is also Gaussian: \\[p(y_* | x_*, X, Y) = \\mathcal{N}(\\mathbf{m}_N^T \\phi(x_*), \\sigma_N^2(x_*))\\]\nThe variance \\(\\sigma_N^2(x_*)\\) tells us how uncertain we are about predictions at \\(x_*\\).\nLet’s implement this and see what happens when applied to our non-linear data.\n\nclass BayesianLinearRegression:\n    \"\"\"\n    Bayesian linear regression with Gaussian prior and likelihood.\n    \n    Closed-form posterior over weights for polynomial basis functions.\n    \"\"\"\n    \n    def __init__(self, degree: int = 1, alpha: float = 1.0, beta: float = 100.0):\n        \"\"\"\n        Args:\n            degree: Degree of polynomial basis (1 = linear)\n            alpha: Prior precision (inverse variance)\n            beta: Noise precision (1/sigma^2)\n        \"\"\"\n        self.degree = degree\n        self.alpha = alpha\n        self.beta = beta\n        self.m_N = None  # Posterior mean\n        self.S_N = None  # Posterior covariance\n        \n    def _design_matrix(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute polynomial design matrix.\"\"\"\n        x = x.reshape(-1, 1)\n        Phi = np.concatenate([x**i for i in range(self.degree + 1)], axis=1)\n        return Phi\n    \n    def fit(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"Compute posterior over weights given training data.\"\"\"\n        Phi = self._design_matrix(X)\n        n_features = Phi.shape[1]\n        \n        # Posterior covariance\n        self.S_N = np.linalg.inv(self.alpha * np.eye(n_features) + \n                                  self.beta * Phi.T @ Phi)\n        \n        # Posterior mean\n        self.m_N = self.beta * self.S_N @ Phi.T @ y\n        \n        return self\n    \n    def predict(self, X_test: np.ndarray, return_std: bool = True):\n        \"\"\"\n        Posterior predictive distribution.\n        \n        Returns:\n            mean: Predictive mean\n            std: Predictive standard deviation (if return_std=True)\n        \"\"\"\n        Phi_test = self._design_matrix(X_test)\n        \n        # Predictive mean\n        y_mean = Phi_test @ self.m_N\n        \n        if return_std:\n            # Predictive variance (includes noise and weight uncertainty)\n            y_var = 1.0/self.beta + np.sum(Phi_test @ self.S_N * Phi_test, axis=1)\n            y_std = np.sqrt(y_var)\n            return y_mean, y_std\n        \n        return y_mean\n    \n    def sample_weights(self, n_samples: int = 100) -&gt; np.ndarray:\n        \"\"\"Sample weight vectors from posterior.\"\"\"\n        return np.random.multivariate_normal(self.m_N, self.S_N, size=n_samples)\n\n# Fit linear Bayesian regression\nblr = BayesianLinearRegression(degree=1, alpha=1.0, beta=1.0/noise_std**2)\nblr.fit(x_train, y_train)\n\n# Posterior predictive distribution\ny_pred_mean, y_pred_std = blr.predict(x_test)\n\nprint(\"Linear Bayesian Regression Fitted!\")\nprint(f\"Posterior mean weights: {blr.m_N}\")\nprint(f\"Posterior std of weights: {np.sqrt(np.diag(blr.S_N))}\")\n\nLinear Bayesian Regression Fitted!\nPosterior mean weights: [-0.07553835  0.33272507]\nPosterior std of weights: [0.0225762  0.01242588]\n\n\n\n\nShow Code\n# Visualize linear Bayesian regression predictions\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plot 1: Posterior predictive with uncertainty bands\nax = axes[0]\n\n# True function\nax.plot(x_test, y_test_true, 'k-', linewidth=2, label='True: $y = \\sin(x)$', alpha=0.7)\n\n# Training data\nax.scatter(x_train, y_train, s=80, c='red', edgecolors='black', \n           linewidth=1.5, zorder=5, label='Training data')\n\n# Predictive mean\nax.plot(x_test, y_pred_mean, 'b-', linewidth=2, label='Linear prediction (posterior mean)')\n\n# Uncertainty bands (±1sigma and ±2sigma)\nax.fill_between(x_test, y_pred_mean - y_pred_std, y_pred_mean + y_pred_std,\n                alpha=0.3, color='blue', label='±1sigma predictive')\nax.fill_between(x_test, y_pred_mean - 2*y_pred_std, y_pred_mean + 2*y_pred_std,\n                alpha=0.2, color='blue', label='±2sigma predictive')\n\nax.set_xlabel('Input $x$', fontsize=13)\nax.set_ylabel('Output $y$', fontsize=13)\nax.set_title('Linear Model: Cannot Capture Nonlinearity', fontsize=14, fontweight='bold')\nax.legend(fontsize=10, loc='upper right')\nax.grid(True, alpha=0.3)\nax.set_xlim([-3.2, 3.2])\nax.set_ylim([-2, 2])\n\n# Plot 2: Function samples from posterior\nax = axes[1]\n\n# Sample weight vectors and plot corresponding functions\nw_samples = blr.sample_weights(n_samples=50)\nPhi_test = np.concatenate([x_test.reshape(-1,1)**i for i in range(2)], axis=1)\n\nfor i in range(50):\n    y_sample = Phi_test @ w_samples[i]\n    ax.plot(x_test, y_sample, 'b-', alpha=0.1, linewidth=1)\n\n# True function and data\nax.plot(x_test, y_test_true, 'k-', linewidth=2, label='True function', alpha=0.7)\nax.scatter(x_train, y_train, s=80, c='red', edgecolors='black', \n           linewidth=1.5, zorder=5, label='Training data')\n\nax.set_xlabel('Input $x$', fontsize=13)\nax.set_ylabel('Output $y$', fontsize=13)\nax.set_title('Posterior Function Samples (50 draws)', fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='upper right')\nax.grid(True, alpha=0.3)\nax.set_xlim([-3.2, 3.2])\nax.set_ylim([-2, 2])\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "part1/introduction_to_inference.html#a-small-neural-network-nonlinear-model",
    "href": "part1/introduction_to_inference.html#a-small-neural-network-nonlinear-model",
    "title": "7  Introduction to Inference",
    "section": "7.3 A Small Neural Network (Nonlinear Model)",
    "text": "7.3 A Small Neural Network (Nonlinear Model)\nThe linear model fails because the true function is nonlinear. What if we try to use a simple neural network instead?\n\n7.3.1 Architecture\nWe’ll use a simple 1-hidden-layer network: \\[f(x; \\mathbf{w}) = \\mathbf{w}_2^T \\sigma(\\mathbf{w}_1 x + \\mathbf{b}_1) + b_2\\]\nwhere \\(\\sigma\\) is ReLU activation. With just 5 hidden units, this can approximate smooth nonlinear functions.\n\n\n7.3.2 The New Challenge: Intractable Posterior\nUnlike linear regression, the posterior \\(p(\\mathbf{w} | X, Y)\\) is no longer Gaussian! We cannot compute it analytically.\nHowever, we can still: 1. Define a prior \\(p(\\mathbf{w})\\) (e.g., Gaussian on all weights) 2. Sample functions from the prior to see what kinds of functions are admissible under the model 3. Later, we can use VI or MCMC to approximate the posterior\nLet’s first see what the prior predictive distribution looks like.\n\nclass SmallNN(nn.Module):\n    \"\"\"\n    Simple 1-hidden-layer neural network for regression.\n    \"\"\"\n    \n    def __init__(self, hidden_size: int = 5):\n        super().__init__()\n        self.fc1 = nn.Linear(1, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        x = x.view(-1, 1)\n        h = F.relu(self.fc1(x))\n        y = self.fc2(h)\n        return y.squeeze()\n    \n    def get_weights_flat(self) -&gt; torch.Tensor:\n        \"\"\"Get all weights as a flat vector.\"\"\"\n        return torch.cat([p.flatten() for p in self.parameters()])\n    \n    def set_weights_flat(self, w_flat: torch.Tensor):\n        \"\"\"Set all weights from a flat vector.\"\"\"\n        offset = 0\n        for p in self.parameters():\n            n_params = p.numel()\n            p.data = w_flat[offset:offset+n_params].view(p.shape)\n            offset += n_params\n\n# Create network and count parameters\nhidden_size = 5\nmodel = SmallNN(hidden_size=hidden_size)\nn_params = sum(p.numel() for p in model.parameters())\n\nprint(\"Neural Network Architecture:\")\nprint(f\"  Input → {hidden_size} hidden (ReLU) → 1 output\")\nprint(f\"  Total parameters: {n_params}\")\nprint(\"Parameter breakdown:\")\nfor name, param in model.named_parameters():\n    print(f\"  {name}: shape {param.shape}, count {param.numel()}\")\n\nNeural Network Architecture:\n  Input → 5 hidden (ReLU) → 1 output\n  Total parameters: 16\nParameter breakdown:\n  fc1.weight: shape torch.Size([5, 1]), count 5\n  fc1.bias: shape torch.Size([5]), count 5\n  fc2.weight: shape torch.Size([1, 5]), count 5\n  fc2.bias: shape torch.Size([1]), count 1\n\n\n\n\nShow Code\n# Sample functions from the PRIOR (before seeing any data)\ndef sample_prior_functions(model, x_test, n_samples=20, prior_std=1.0):\n    \"\"\"\n    Sample function predictions from prior p(w) = N(0, prior_std^2 * I).\n    \"\"\"\n    x_test_tensor = torch.FloatTensor(x_test)\n    predictions = []\n    \n    with torch.no_grad():\n        for _ in range(n_samples):\n            # Sample weights from prior\n            for param in model.parameters():\n                param.data.normal_(0, prior_std)\n            \n            # Forward pass\n            y_pred = model(x_test_tensor).numpy()\n            predictions.append(y_pred)\n    \n    return np.array(predictions)\n\n# Generate prior predictive samples\nprior_std = 1.0\nn_prior_samples = 50\nprior_predictions = sample_prior_functions(model, x_test, n_prior_samples, prior_std)\n\n# Visualize prior predictive distribution\nfig, ax = plt.subplots(figsize=(12, 7))\n\n# Plot prior function samples\nfor i in range(n_prior_samples):\n    ax.plot(x_test, prior_predictions[i], 'purple', alpha=0.3, linewidth=1)\n\n# Plot true function and data\nax.plot(x_test, y_test_true, 'k-', linewidth=3, label='True function: $y = \\sin(x)$', zorder=10)\nax.scatter(x_train, y_train, s=100, c='red', edgecolors='black', \n           linewidth=2, zorder=15, label='Training data (not yet used!)')\n\n# Add dummy line for legend\nax.plot([], [], 'purple', alpha=0.6, linewidth=2, \n        label=f'Prior samples (n={n_prior_samples})')\n\nax.set_xlabel('Input $x$', fontsize=13)\nax.set_ylabel('Output $y$', fontsize=13)\nax.set_title(f'Prior Predictive: Neural Network Before Training\\n(Prior: $w \\sim \\mathcal{{N}}(0, {prior_std}^2 I)$)', \n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='upper right')\nax.grid(True, alpha=0.3)\nax.set_xlim([-3.2, 3.2])\nax.set_ylim([-3, 3])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that just sampling functions from the prior, the possible Neural Network functions are a lot more diverse that those we could sample from the linear model. But how do we find out which of these weights are (probabilistically) likely?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "part1/introduction_to_inference.html#mcmc-approximation",
    "href": "part1/introduction_to_inference.html#mcmc-approximation",
    "title": "7  Introduction to Inference",
    "section": "7.4 MCMC Approximation",
    "text": "7.4 MCMC Approximation\nThe neural network posterior \\(p(\\mathbf{w} | X, Y)\\) is intractable, but we can sample from it using Markov Chain Monte Carlo (MCMC). Before we dive into how to apply this to neural networks, let’s first gain some intuition on how and why MCMC works on a simple 2D example.\n\n7.4.1 Why can’t we just compute the posterior using regular Monte Carlo Integration?\nA classical Monte Carlo approach would involve: 1. Drawing many samples \\(\\mathbf{w}^{(i)}\\) from the prior \\(p(\\mathbf{w})\\), or (as is more common) from a proposal distribution \\(q(\\mathbf{w})\\) that we hope covers the posterior well. 2. Weighting each sample by its likelihood \\(p(Y | X, \\mathbf{w}^{(i)})\\) 3. Normalizing empirically to get an approximate posterior.\nThis is called importance sampling and while it is conceptually simple, it doesn’t work for many complex models or distributions because the proposal distribution needs to be well matched to the true posterior. If the posterior is concentrated in a small region of parameter space, most samples will have negligible weight, and this will make our “Effective Sample Size” (ESS) very small, i.e., we will need to do lots of sampling, but ultimately throw out most of those samples, so this is inefficient. For low-dimensional, fairly simple distributions, importance sampling can work well, but for high-dimensional models like neural networks or very complex posterior distributions, it typically fails (as we will see below).\nTo address this, we can use Markov Chain Monte Carlo (MCMC) methods, which adaptively explore the posterior distribution. The key idea is to construct a Markov chain whose stationary distribution is the target posterior distribution. By running the chain for a long time, we can obtain samples that are approximately distributed according to the posterior. It’s main disadvantage is that because many of the samples are correlated, we need many more samples to get a good approximation of the posterior compared to independent sampling methods, but this downside is offset by the fact that MCMC can explore more complex distributions more effectively even using a simple proposal distribution.\n\n\n7.4.2 What is MCMC?\nMCMC methods (like Metropolis-Hastings, Hamiltonian Monte Carlo, and others) generate samples \\(\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)}, \\ldots\\) from the posterior by: 1. Starting at a random initial weight vector 2. Proposing updated samples that respect the posterior density 3. Eventually converging to samples from \\(p(\\mathbf{w} | X, Y)\\)\nSo, in a nutshell, it allows you to generate plausible samples from \\(p(\\mathbf{w} | X, Y)\\) without ever having to strictly compute the posterior.\nPros: Asymptotically exact (given infinite samples)\nCons: Slow, especially for high-dimensional models\nFor our small network, MCMC is feasible. We will use it as a reference to compare against variational inference at the end of this notebook.\n\n\n7.4.3 Note on Implementation\nFor this notebook, we’ll use a simple Metropolis-Hastings sampler, although many more advanced algorithms exist.1\n1 There are some tricks to reduce the variance by subtracting a moving “baseline” average to the advantage term, which is implemented below, but we will not go into the details of this here.\n7.4.3.1 The Metropolis-Hastings Algorithm\nThe algorithm works as follows:\n\nInitialize: Start with random weights \\(\\mathbf{w}^{(0)}\\)\nPropose: Generate a candidate \\(\\mathbf{w}^* \\sim q(\\mathbf{w}^* | \\mathbf{w}^{(t)})\\) (typically a Gaussian centered at current weights)\nAccept/Reject: Compute the acceptance probability: \\[\\alpha = \\min\\left(1, \\frac{p(\\mathbf{w}^* | X, Y)}{p(\\mathbf{w}^{(t)} | X, Y)}\\right) = \\min\\left(1, \\frac{p(Y | X, \\mathbf{w}^*) p(\\mathbf{w}^*)}{p(Y | X, \\mathbf{w}^{(t)}) p(\\mathbf{w}^{(t)})}\\right)\\] Accept \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^*\\) with probability \\(\\alpha\\), otherwise keep \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)}\\)\nRepeat: Continue for many iterations until the chain converges\n\nThe useful thing about this sampler (and indeed MCMC approaches in general), is that we only ever need to evaluate the unnormalized posterior (likelihood x prior), and thus can avoid computing the intractable normalizing constant.\nKey Caveats (with Metropolis-Hastings or MCMC methods in general): - You cannot just take individual samples and treat them as independent draws from the posterior, since the starting location of one sample was dependent on the location of the last sample. This means that the samples are correlated. In practice, this means you will need to discard the first certain number of samples (this is called “Burn In”) and also consider only keeping every K number of samples (this is called “Thinning”) so that the MCMC samples become less correlated with one another and more like true independent samples from the posterior. - While the proposal distribution can be simple (e.g., Gaussian), its parameters (like standard deviation) need to be carefully tuned since if it is too large we will have low sample acceptance rates, but it is is too small we will explore the space very slowly, requiring many more samples to converge.\nFor the 2D example we will do next, MCMC works very well and will give us a good approximation of the posterior. For our 16-parameter network that we will return to later in the notebook, MCMC sampling is still tractable. For modern deep networks with millions of parameters, however, MCMC becomes prohibitively slow and this is where Variational Inference can come to the rescue, as we will see later in the second part of the notebook.\n\n\n\n7.4.4 Toy Example: 2D Posterior Sampling from the Banana Distribution\nWe will start with a simple 2D example to illustrate how MCMC works. This will come from the famous “Banana Distribution”, which is a common test case for MCMC methods due to its curved, non-Gaussian shape, as we can see below.\n\n\nShow Code\n# Reuse global RNG seed for reproducibility (already set above)\nrng = np.random.default_rng(42)\n\n# Unnormalized log-density of banana target:\n# log p~(x,y) = -0.5 * ( x^2/9 + (y - 0.1 x^2)^2 )\n\ndef log_p_tilde(x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    x = np.asarray(x)\n    y = np.asarray(y)\n    return -0.5 * ( (x**2) / 9.0 + (y - 0.1 * x**2)**2 )\n\n# Simple isotropic Gaussian proposal q(x,y) = N(0, I)\n# log q(x,y) = -0.5 * (x^2 + y^2) + const; constants cancel in ratios, so we can drop them\n\ndef log_q(x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    x = np.asarray(x)\n    y = np.asarray(y)\n    return -0.5 * (x**2 + y**2)\n\nprint(\"Defined banana target log-density and Gaussian proposal.\")\n\n# Now plot the target distribution for visualization\nx_vals = np.linspace(-10, 10, 300)\ny_vals = np.linspace(-5, 15, 300)\nX, Y = np.meshgrid(x_vals, y_vals)\nZ = np.exp(log_p_tilde(X, Y))\nfig, ax = plt.subplots(figsize=(10, 6))\ncontour = ax.contourf(X, Y, Z, levels=50, cmap='viridis')\nplt.colorbar(contour, ax=ax, label='Density')\nax.set_title('Banana-shaped Target Distribution', fontsize=14, fontweight='bold')\nax.set_xlabel('x', fontsize=13)\nax.set_ylabel('y', fontsize=13)\nplt.show()\n\n\nDefined banana target log-density and Gaussian proposal.\n\n\n\n\n\n\n\n\n\nWe can attempt to do Monte Carlo integration on this distribution, but because of its curved shape, simple importance sampling is unlikely to cover this distribution well, as we can see below:\n\n# Importance Sampling on the banana target\nN_is = 10_000\n\n# Draw from q ~ N(0, I)\nx_is = rng.normal(size=N_is)*.3\ny_is = rng.normal(size=N_is)*.3\n\n# Unnormalized log weights: log w_i = log p~(x_i,y_i) - log q(x_i,y_i)\nlog_w = log_p_tilde(x_is, y_is) - log_q(x_is, y_is)\n# Stabilize weights by subtracting max log_w\nlog_w_shift = log_w - np.max(log_w)\nw = np.exp(log_w_shift)\nw_norm = w / np.sum(w)\n\n# Effective sample size\nESS = 1.0 / np.sum(w_norm**2)\n\n# Weighted means\nmean_x_is = np.sum(w_norm * x_is)\nmean_y_is = np.sum(w_norm * y_is)\n\nprint(f\"Importance Sampling: N={N_is}, ESS={ESS:.1f} ({100*ESS/N_is:.1f}% of N)\")\nprint(f\"Weighted means: E[x]={mean_x_is:.3f}, E[y]={mean_y_is:.3f}\")\n\nImportance Sampling: N=10000, ESS=9959.7 (99.6% of N)\nWeighted means: E[x]=-0.003, E[y]=0.007\n\n\n\n\nShow Code\n# Visualize weighted samples\nfig, ax = plt.subplots(figsize=(6, 5))\nsc = ax.scatter(x_is, y_is, s=8, alpha=0.3)\n#cb = plt.colorbar(sc, ax=ax)\n#cb.set_label('Normalized weight')\nax.set_title(f'Importance Sampling\\nESS={ESS:.0f} of N={N_is}')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.grid(True, alpha=0.3)\n# Overlay contour of target distribution\nx_vals = np.linspace(-10, 10, 300)\ny_vals = np.linspace(-4, 5, 300)\nX, Y = np.meshgrid(x_vals, y_vals)\nZ = np.exp(log_p_tilde(X, Y))\ncontour = ax.contour(X, Y, Z, levels=10, colors='black', alpha=0.3)\nax.set_aspect('equal', adjustable='box')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Modifying the Importance Sampling Proposal Distribution\n\n\n\n\nWhat happens if you change the proposal distribution to have a larger or smaller variance? Does this improve the coverage of the posterior?\nCompare the Effective Sample Size (ESS) for different proposal variances – does this align with your intuition when you compare the coverage of the sample points with the posterior?\n\n\n\nNow that we have seen the limitations of importance sampling, let’s see how MCMC can help us better sample from this complex posterior. We will implement Metropolis-Hastings below:\n\n# Metropolis–Hastings MCMC on the banana target\ndef mcmc_banana(N=10_000, sigma=0.5, burn_in=1_000, start=(0.0, 0.0), seed=42):\n    \"\"\"Run a simple MH sampler with isotropic Gaussian proposals.\"\"\"\n    rng_local = np.random.default_rng(seed)\n    x_curr, y_curr = float(start[0]), float(start[1])\n    logp_curr = log_p_tilde(x_curr, y_curr)\n\n    samples = np.zeros((N, 2), dtype=float)\n    accepts = 0\n\n    for i in range(N):\n        # Propose from q(x'|x) = N(x, sigma^2 I)\n        dx, dy = rng_local.normal(scale=sigma, size=2)\n        x_prop, y_prop = x_curr + dx, y_curr + dy\n        logp_prop = log_p_tilde(x_prop, y_prop)\n\n        # Symmetric proposal -&gt; Hastings ratio reduces to p~(x')/p~(x)\n        log_alpha = logp_prop - logp_curr\n        if np.log(rng_local.uniform()) &lt; min(0.0, log_alpha):\n            x_curr, y_curr = x_prop, y_prop\n            logp_curr = logp_prop\n            accepts += 1\n        samples[i] = (x_curr, y_curr)\n\n    acc_rate = accepts / N\n    chain = samples[burn_in:]\n    mean_x_mcmc = float(np.mean(chain[:, 0]))\n    mean_y_mcmc = float(np.mean(chain[:, 1]))\n    return chain, acc_rate, mean_x_mcmc, mean_y_mcmc\n\n\n\nShow Code\n# Run MCMC\n## Try changing these and see what happens to the behavior of the MCMC samples\n# (e.g., no burn-in, different starting point, low vs high sigma)\nN_mcmc = 100_000\nburn_in = 1_000\nsigma = .1\nstart = (0.0, 0.0)\n##\nchain, acc_rate, mean_x_mcmc, mean_y_mcmc = mcmc_banana(N=N_mcmc, sigma=sigma, burn_in=burn_in, start=start, seed=7)\n\nprint(f\"MCMC: N={N_mcmc}, burn-in={burn_in}, sigma={sigma}, acceptance={acc_rate:.2f}\")\nprint(f\"MCMC means: E[x]={mean_x_mcmc:.3f}, E[y]={mean_y_mcmc:.3f}\")\n\n\nMCMC: N=100000, burn-in=1000, sigma=0.1, acceptance=0.96\nMCMC means: E[x]=1.061, E[y]=0.729\n\n\n\n\nShow Code\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\n# Left: MCMC (plot scatter and path)\nax = axes[0]\nax.scatter(chain[:,0], chain[:,1], s=5, c='tab:orange', alpha=0.3, label='MCMC samples')\n# Plot a thin path for the first few thousand steps to show exploration\npath_len = min(2000, len(chain))\nax.plot(chain[:path_len,0], chain[:path_len,1], color='tab:red', linewidth=1, alpha=0.5, label='Trace (first 2k)')\nax.set_title(f'MCMC (MH)\\naccept.={acc_rate:.2f}, N={N_mcmc}, burn-in={burn_in}, sigma={sigma}')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.legend(loc='upper right', fontsize=9)\nax.grid(True, alpha=0.3)\n# overlay contour of target distribution\nx_vals = np.linspace(-10, 10, 300)\ny_vals = np.linspace(-4, 5, 300)\nX, Y = np.meshgrid(x_vals, y_vals)\nZ = np.exp(log_p_tilde(X, Y))\ncontour = ax.contour(X, Y, Z, levels=10, colors='black', alpha=0.3)\nax.set_aspect('equal', adjustable='box')\n\n# Right: Line plot of MCMC trace\nax = axes[1]\nax.plot(chain[:,0], label='x', color='tab:blue', alpha=0.7)\nax.plot(chain[:,1], label='y', color='tab:green', alpha=0.7)\nax.set_title('MCMC Trace Plot')\nax.set_xlabel('Iteration')\nax.set_ylabel('Value')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Summary diagnostics:\")\nprint(f\"  Importance Sampling: ESS={ESS:.1f}/{N_is} ({100*ESS/N_is:.1f}%), E[x]={mean_x_is:.3f}, E[y]={mean_y_is:.3f}\")\nprint(f\"  MCMC: acceptance={acc_rate:.2f}, E[x]={mean_x_mcmc:.3f}, E[y]={mean_y_mcmc:.3f}\")\n\n\n\n\n\n\n\n\n\nSummary diagnostics:\n  Importance Sampling: ESS=9959.7/10000 (99.6%), E[x]=-0.003, E[y]=0.007\n  MCMC: acceptance=0.96, E[x]=1.061, E[y]=0.729\n\n\n\n\n\n\n\n\nTipExperiment: Metropolis-Hastings sampling parameters\n\n\n\nModify the number of samples, proposal variance, burn-in period, and the starting point. - What happens if the proposal variance is very small? Is the chain able to explore the posterior well? How is this behavior reflected in the acceptance rate or the trace plot? - What happens if the starting point is very far from the high-density region of the posterior? How long does it take for the chain to converge to the target distribution? What does this imply for the burn-in period and how is this reflected in the trace plot? - How do the various factors (proposal variance, starting point, burn-in) interact to affect the acceptance rate and the quality of the samples? Is a higher or lower acceptance rate always better? Why do you think this is?\n\n\n\n\n7.4.5 Why IS struggles and MCMC succeeds\n\nImportance Sampling (IS) relies on a global reweighting of proposal samples. When \\(q\\) poorly overlaps with the curved high-density region of the banana target, almost all weights are near zero. This leads to a tiny effective sample size (ESS) and high estimator variance.\nMCMC with local proposals uses acceptance ratios that only depend on pairwise density ratios. Even with an unnormalized target \\(\\tilde p\\), the normalizing constant cancels and the chain can move along the curved ridge, accumulating many effective samples.\nBoth methods can use unnormalized \\(p\\); the practical difference is normalization: IS requires a global normalization of weights, while MCMC only needs local ratios. As dimensionality and curvature increase, IS becomes very brittle unless \\(q\\) is very well matched to \\(p\\), whereas MCMC typically remains viable with reasonable proposals.\n\n\n\n7.4.6 Returning to Neural Networks (now with MCMC sampling)\nNow that we have a good understanding of how MCMC works on a simple 2D example, let’s return to our small neural network and use MCMC to sample from its posterior distribution over weights. This will allow us to approximate the posterior predictive distribution for our nonlinear regression problem. We will add on a few tricks that we didn’t have in our simple 2D example, namely: - We’ll thin out samples in the chain to reduce correlation between samples - We’ll discard a burn-in period to allow the chain to converge before collecting samples (as we did before) - Before starting the MCMC chain, we’ll pretrain the network using MAP estimation to find a good starting point for the chain.\n\n# Simple MCMC sampler for neural network weights\ndef log_likelihood(model, x, y, noise_std=0.1):\n    \"\"\"Compute log p(y|x,w) for current weights.\"\"\"\n    with torch.no_grad():\n        x_tensor = torch.FloatTensor(x)\n        y_pred = model(x_tensor).numpy()\n        log_lik = -0.5 * np.sum(((y - y_pred) / noise_std) ** 2)\n        log_lik -= len(y) * np.log(noise_std * np.sqrt(2 * np.pi))\n    return log_lik\n\ndef log_prior(model, prior_std=1.0):\n    \"\"\"Compute log p(w) for current weights.\"\"\"\n    log_p = 0\n    for param in model.parameters():\n        log_p -= 0.5 * torch.sum((param / prior_std) ** 2).item()\n    return log_p\n\ndef metropolis_hastings_nn(x_train, y_train, n_samples=5000, proposal_std=0.01,\n                           prior_std=1.0, noise_std=0.1, thin=10):\n    \"\"\"\n    Simple Metropolis-Hastings MCMC for neural network weights.\n    \n    Args:\n        thin: Only keep every 'thin'-th sample to reduce autocorrelation\n    \n    Returns:\n        weight_samples: List of weight vectors\n        acceptance_rate: Fraction of proposals accepted\n    \"\"\"\n    model = SmallNN(hidden_size=5)\n    \n    # Initialize at MAP estimate (rough approximation: just train briefly)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    x_tensor = torch.FloatTensor(x_train).view(-1, 1)\n    y_tensor = torch.FloatTensor(y_train)\n    \n    print(\"Finding initial weights (quick MAP estimate)...\")\n    for _ in range(200):\n        optimizer.zero_grad()\n        loss = F.mse_loss(model(x_tensor.squeeze()), y_tensor)\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Starting MCMC sampling ({n_samples} iterations)...\")\n    \n    # Current state\n    current_log_posterior = log_likelihood(model, x_train, y_train, noise_std) + \\\n                            log_prior(model, prior_std)\n    \n    weight_samples = []\n    n_accepted = 0\n    \n    for i in range(n_samples):\n        # Propose new weights\n        proposed_model = SmallNN(hidden_size=5)\n        with torch.no_grad():\n            for p_curr, p_prop in zip(model.parameters(), proposed_model.parameters()):\n                p_prop.data = p_curr.data + proposal_std * torch.randn_like(p_curr)\n        \n        # Compute proposed log posterior\n        proposed_log_posterior = log_likelihood(proposed_model, x_train, y_train, noise_std) + \\\n                                  log_prior(proposed_model, prior_std)\n        \n        # Accept/reject\n        log_alpha = proposed_log_posterior - current_log_posterior\n        if np.log(np.random.rand()) &lt; log_alpha:\n            # Accept\n            model = proposed_model\n            current_log_posterior = proposed_log_posterior\n            n_accepted += 1\n        \n        # Store sample (with thinning)\n        if i % thin == 0:\n            weight_samples.append(model.get_weights_flat().clone().detach().numpy())\n        \n        if (i + 1) % 2000 == 0:\n            acc_rate = n_accepted / (i + 1)\n            print(f\"  Iteration {i+1}/{n_samples}, acceptance rate: {acc_rate:.3f}\")\n    \n    acceptance_rate = n_accepted / n_samples\n    print(f\"MCMC complete! Final acceptance rate: {acceptance_rate:.3f}\")\n    \n    return weight_samples, acceptance_rate\n\n# Run MCMC (this will take a minute or two)\nprint(\"Running MCMC to sample from posterior p(w|X,Y)...\")\nprint(\"This may take 1-2 minutes for accurate results.\")\nmcmc_samples, acc_rate = metropolis_hastings_nn(\n    x_train, y_train, \n    n_samples=10_000, \n    proposal_std=0.01,\n    prior_std=1.0,\n    noise_std=noise_std,\n    thin=10\n)\n\nprint(f\"Collected {len(mcmc_samples)} posterior samples (after thinning)\")\nprint(f\"Acceptance rate: {acc_rate:.1%}\")\n\nRunning MCMC to sample from posterior p(w|X,Y)...\nThis may take 1-2 minutes for accurate results.\nFinding initial weights (quick MAP estimate)...\nStarting MCMC sampling (10000 iterations)...\nFinding initial weights (quick MAP estimate)...\nStarting MCMC sampling (10000 iterations)...\n  Iteration 2000/10000, acceptance rate: 0.460\n  Iteration 2000/10000, acceptance rate: 0.460\n  Iteration 4000/10000, acceptance rate: 0.458\n  Iteration 4000/10000, acceptance rate: 0.458\n  Iteration 6000/10000, acceptance rate: 0.462\n  Iteration 6000/10000, acceptance rate: 0.462\n  Iteration 8000/10000, acceptance rate: 0.466\n  Iteration 8000/10000, acceptance rate: 0.466\n  Iteration 10000/10000, acceptance rate: 0.455\nMCMC complete! Final acceptance rate: 0.455\nCollected 1000 posterior samples (after thinning)\nAcceptance rate: 45.5%\n  Iteration 10000/10000, acceptance rate: 0.455\nMCMC complete! Final acceptance rate: 0.455\nCollected 1000 posterior samples (after thinning)\nAcceptance rate: 45.5%\n\n\n\n\nShow Code\n# Plot the trace plots for all of the weights as a quick diagnostic using figure subplots:\nmcmc_samples_array = np.array(mcmc_samples)  # shape (n_samples, n_params)\nn_params = mcmc_samples_array.shape[1]\nfig, axes = plt.subplots(n_params, 1, figsize=(5, n_params), sharex=True)\nfor i in range(n_params):\n    axes[i].plot(mcmc_samples_array[:, i], color='tab:blue', alpha=0.7)\n    axes[i].set_ylabel(f'W[{i}]', fontsize=10)\n    axes[i].grid(True, alpha=0.3)\naxes[-1].set_xlabel('MCMC Sample Index', fontsize=12)\nfig.suptitle('MCMC Trace Plots for Neural Network Weights', fontsize=14, fontweight='bold')\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Visualize MCMC posterior predictive\ndef predict_with_weight_samples(weight_samples, x_test, n_samples=None):\n    \"\"\"Generate predictions using sampled weights.\"\"\"\n    if n_samples is None:\n        n_samples = len(weight_samples)\n    \n    model_temp = SmallNN(hidden_size=5)\n    x_test_tensor = torch.FloatTensor(x_test)\n    predictions = []\n    \n    # Randomly select samples to plot\n    sample_indices = np.random.choice(len(weight_samples), size=min(n_samples, len(weight_samples)), replace=False)\n    \n    with torch.no_grad():\n        for idx in sample_indices:\n            model_temp.set_weights_flat(torch.FloatTensor(weight_samples[idx]))\n            y_pred = model_temp(x_test_tensor).numpy()\n            predictions.append(y_pred)\n    \n    return np.array(predictions)\n\n# Generate MCMC posterior predictions\nmcmc_predictions = predict_with_weight_samples(mcmc_samples, x_test, n_samples=100)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(12, 7))\n\n# Plot MCMC posterior samples\nfor i in range(len(mcmc_predictions)):\n    ax.plot(x_test, mcmc_predictions[i], 'orange', alpha=0.15, linewidth=1)\n\n# True function and data\nax.plot(x_test, y_test_true, 'k-', linewidth=3, label='True function', zorder=10)\nax.scatter(x_train, y_train, s=100, c='red', edgecolors='black', \n           linewidth=2, zorder=15, label='Training data')\n\n# Posterior mean\nmcmc_mean = mcmc_predictions.mean(axis=0)\nax.plot(x_test, mcmc_mean, 'orange', linewidth=3, label='MCMC posterior mean', linestyle='--', zorder=5)\n\n# Dummy line for legend\nax.plot([], [], 'orange', alpha=0.6, linewidth=2, label='MCMC posterior samples')\n\nax.set_xlabel('Input $x$', fontsize=13)\nax.set_ylabel('Output $y$', fontsize=13)\nax.set_title('MCMC Posterior Predictive Distribution\\n(Reference \"Ground Truth\")', \n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='upper right')\nax.grid(True, alpha=0.3)\nax.set_xlim([-3.2, 3.2])\nax.set_ylim([-1.5, 1.5])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n7.4.7 Advanced Sampling: Hamiltonian Monte Carlo (HMC)\nThere have been many substantial improvements to MCMC sampling methods over the past several decades, and one of the main advances came through the recognition that we need not only sample with knowledge of \\(q\\) and \\(p\\), but that we could leverage the gradients of \\(p\\). This was previously very limited or difficult to do, but with the widespread use of Automatic Differentiation libraries became much more powerful.\nUpon widespread access to gradients of \\(p\\), we could now think about sampling processes that could leverage the slope and curvature of the likelihood to guide us toward better samples. One of the mainstream ways to do this is Hamiltonian Monte Carlo which treats sampling as analogous to estimating a dynamical system. Specifically, we treat the samples/parameters as positions, introduce an auxiliary momentum variable, and simulate Hamiltonian dynamics that preserve the joint density, typically using a symplectic integretor, such as the leapfrog integrator. Because the leapfrog integrator follows smooth trajectories, HMC avoids the random walk behaviour that slowed down our Metropolis–Hastings chains above. HMC is still fundamentally accepting or rejecting samples like MH did, accept that it’s method of producing new samples is significantly more likely to lead to acceptable samples, and thus can be far more sample efficient.\nIt does this through a few basic concepts: - It uses the likelihood as a Potential Energy function \\(U(\\theta) = -\\log p(\\theta \\mid \\text{data})\\) which pulls particle trajectories toward higher posterior density. - Each particle has Kinetic Energy \\(K(\\mathbf{p}) = \\tfrac{1}{2} \\mathbf{p}^\\top M^{-1} \\mathbf{p}\\) which comes from a tunable mass matrix \\(M\\) that can re-scale parameters. - A symplectic integrator (Leapfrog integration) alternates half-steps in momentum with full steps in position and nearly conserves energy.\nThe algorith itself is fairly straightforward in that you:\n\nSample a fresh momentum from \\(\\mathcal{N}(0, M)\\) to randomize the next trajectory.\nIntegrate the Hamiltonian dynamics for \\(L\\) leapfrog steps with step size \\(\\varepsilon\\).\nApply a Metropolis accept/reject step using the Hamiltonian.\n\nSome pecularities with HMC include: - Smaller step size \\(\\varepsilon\\) improves accuracy but increases computation. If step sizes are too large, it can overshoot and causes many sample rejections. - The number of leapfrog steps \\(L\\) controls how far each proposal travels across the posterior ridge. Too small and the samples become auto-correlated since the sample cannot move far enough away from the current sample. - A well-chosen mass matrix and initial adaptation help HMC cope with areas of strong curvature in the posterior. - HMC relies on gradients, so we need target distributions that are differentiable and numerically stable.\nBelow we implement a minimal HMC sampler for the banana distribution so you can compare its behaviour with the random-walk MH sampler above.\n\n# Hamiltonian Monte Carlo on the banana target\ndef grad_log_p_tilde(x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the gradient of the banana log-density with respect to (x, y).\"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n    grad_x = -x / 9.0 + 0.2 * x * (y - 0.1 * x**2)\n    grad_y = -(y - 0.1 * x**2)\n    return np.stack([grad_x, grad_y], axis=-1)\n\ndef hmc_banana(num_samples: int = 8_000, burn_in: int = 1_000, step_size: float = 0.25,\n                num_leapfrog: int = 20, mass: np.ndarray | float = 1.0,\n                start: tuple[float, float] = (0.0, 0.0), seed: int = 2024) -&gt; tuple[np.ndarray, float]:\n    \"\"\"Run a basic HMC sampler with (scalar or diagonal) mass matrix for the banana target.\"\"\"\n    rng_local = np.random.default_rng(seed)\n    q_current = np.array(start, dtype=float)\n    if q_current.shape != (2,):\n        raise ValueError(\"start must be a length-2 tuple or array.\")\n\n    mass_array = np.asarray(mass, dtype=float)\n    if mass_array.ndim == 0:\n        mass_vec = np.full(2, mass_array.item())\n    elif mass_array.shape == (2,):\n        mass_vec = mass_array\n    else:\n        raise ValueError(\"mass must be a scalar or length-2 array for this example.\")\n    if np.any(mass_vec &lt;= 0):\n        raise ValueError(\"mass components must be positive.\")\n    inv_mass = 1.0 / mass_vec\n\n    logp_current = log_p_tilde(q_current[0], q_current[1])\n    samples = np.zeros((num_samples, 2), dtype=float)\n    accepts = 0\n\n    for i in range(num_samples):\n        # Sample fresh momentum from N(0, M) with diagonal mass matrix.\n        p_current = rng_local.normal(scale=np.sqrt(mass_vec), size=2)\n        q = q_current.copy()\n        p = p_current.copy()\n\n        # Leapfrog integration\n        grad = grad_log_p_tilde(q[0], q[1])\n        p += 0.5 * step_size * grad\n        for lf_step in range(num_leapfrog):\n            q += step_size * p * inv_mass\n            grad = grad_log_p_tilde(q[0], q[1])\n            if lf_step != num_leapfrog - 1:\n                p += step_size * grad\n        p += 0.5 * step_size * grad\n        p = -p  # Reverse momentum for detailed balance\n\n        logp_prop = log_p_tilde(q[0], q[1])\n        current_H = -logp_current + 0.5 * np.sum((p_current**2) * inv_mass)\n        proposed_H = -logp_prop + 0.5 * np.sum((p**2) * inv_mass)\n        log_alpha = current_H - proposed_H\n\n        if np.log(rng_local.uniform()) &lt; min(0.0, log_alpha):\n            q_current = q\n            logp_current = logp_prop\n            accepts += 1\n\n        samples[i] = q_current\n\n    if burn_in &gt;= num_samples:\n        raise ValueError(\"burn_in must be smaller than num_samples.\")\n    chain = samples[burn_in:]\n    acc_rate = accepts / num_samples\n    return chain, acc_rate\n\n# Run HMC and summarize diagnostics\nN_hmc = 8_000\nburn_in_hmc = 1_000\nstep_size = 0.25\nnum_leapfrog = 20\nmass_vec = np.array([0.7, 1.3])  # Slight re-scaling along x and y\nhmc_chain, hmc_acc_rate = hmc_banana(num_samples=N_hmc, burn_in=burn_in_hmc,\n                                     step_size=step_size, num_leapfrog=num_leapfrog,\n                                     mass=mass_vec, start=(10.0, 10.0), seed=11)\nmean_x_hmc = float(np.mean(hmc_chain[:, 0]))\nmean_y_hmc = float(np.mean(hmc_chain[:, 1]))\n\nprint(f\"HMC: N={N_hmc}, burn-in={burn_in_hmc}, step size={step_size}, L={num_leapfrog}, acceptance={hmc_acc_rate:.2f}\")\nprint(f\"HMC means: E[x]={mean_x_hmc:.3f}, E[y]={mean_y_hmc:.3f}\")\n\nHMC: N=8000, burn-in=1000, step size=0.25, L=20, acceptance=1.00\nHMC means: E[x]=0.046, E[y]=0.860\n\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Visualize samples and trace using the same style as earlier\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\nax = axes[0]\nax.scatter(hmc_chain[:, 0], hmc_chain[:, 1], s=5, c='tab:purple', alpha=0.3, label='HMC samples')\npath_len = min(2_000, len(hmc_chain))\nax.plot(hmc_chain[:path_len, 0], hmc_chain[:path_len, 1], color='tab:purple', linewidth=1, alpha=0.5, label='Trace (first 2k)')\nax.set_title(f'HMC\\naccept.={hmc_acc_rate:.2f}, N={N_hmc}, burn-in={burn_in_hmc}, step={step_size}, L={num_leapfrog}')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.legend(loc='upper right', fontsize=9)\nax.grid(True, alpha=0.3)\nx_vals = np.linspace(-10, 10, 300)\ny_vals = np.linspace(-4, 5, 300)\nX, Y = np.meshgrid(x_vals, y_vals)\nZ = np.exp(log_p_tilde(X, Y))\nax.contour(X, Y, Z, levels=10, colors='black', alpha=0.3)\nax.set_aspect('equal', adjustable='box')\n\nax = axes[1]\nax.plot(hmc_chain[:, 0], label='x', color='tab:blue', alpha=0.7)\nax.plot(hmc_chain[:, 1], label='y', color='tab:green', alpha=0.7)\nax.set_title('HMC Trace Plot')\nax.set_xlabel('Iteration')\nax.set_ylabel('Value')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTipExperiment: Tuning HMC Hyperparameters\n\n\n\n\nDecrease the step size while increasing the leapfrog count so the trajectory length stays similar. What happens to acceptance and exploration?\nTry a very large step size or very few leapfrog steps. Do you see the chain reverting to random-walk behaviour?\nReplace the scalar mass with a small diagonal matrix (e.g., different masses for \\(x\\) and \\(y\\)). Does this help the sampler hug the banana ridge more quickly?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "part1/introduction_to_inference.html#variational-inference",
    "href": "part1/introduction_to_inference.html#variational-inference",
    "title": "7  Introduction to Inference",
    "section": "7.5 Variational Inference",
    "text": "7.5 Variational Inference\nMCMC works but is slow, since it essentially has to generate thousands of good samples. In constrast, Variational Inference methods turn the integration problem into an optimization problem. Instead of sampling from \\(p(\\mathbf{w} | X, Y)\\), we:\n\nChoose a tractable family \\(q_\\phi(\\mathbf{w})\\) (e.g., Gaussian)\nFind \\(\\phi\\) that makes \\(q_\\phi\\) close to the true posterior\nMeasure “closeness” using the ELBO (Evidence Lower Bound)\n\n\n7.5.1 Chosing a tractable family \\(q_\\phi(\\mathbf{w})\\)\nThere are many options for this, as we will see in later more advanced notebooks, but for now let’s use something simple: a mean-field Gaussian approximation: \\[q_\\phi(\\mathbf{w}) = \\mathcal{N}(\\boldsymbol{\\mu}_\\phi, \\text{diag}(\\boldsymbol{\\sigma}_\\phi^2))\\]\nWe will look at two examples below: the first is just a simple 2D Gaussian with a 2D mean vector and two \\(\\sigma\\) s corresponding to a diagonal covariance. Later, we extend this concept to a Neural Network (like we did with MCMC above) each weight in the network will get its own mean and variance, and for our network with n_params parameters, we would then optimize both for all weights, leading to \\(2 \\times {n_{params}}\\) variational parameters.\nBut, in either case, what should we be optimizing, exactly?\n\n\n7.5.2 1. Marginal Likelihood (Evidence)\nThe goal in Bayesian inference is to compute the marginal likelihood (also called the evidence):\n\\[\np(Y|X) = \\int p(Y|X, \\mathbf{w})\\, p(\\mathbf{w})\\, d\\mathbf{w}\n\\]\nWhich sounds fine in theory, but this integral is usually intractable for neural networks or other complex models we might be interested in. How can we solve this?\n\n\n7.5.3 2. Introducing a Variational Distribution\nWe introduce a tractable distribution \\(q_\\phi(\\mathbf{w})\\) to approximate the true posterior \\(p(\\mathbf{w}|X,Y)\\).\nWe can rewrite the log evidence using \\(q_\\phi\\):\n\\[\n\\log p(Y|X) = \\log \\int p(Y|X, \\mathbf{w})\\, p(\\mathbf{w})\\, d\\mathbf{w}\n\\]\n\\[\n= \\log \\int q_\\phi(\\mathbf{w})\\, \\frac{p(Y|X, \\mathbf{w})\\, p(\\mathbf{w})}{q_\\phi(\\mathbf{w})}\\, d\\mathbf{w}\n\\]\nSo far, this mathematical maneuver hasn’t really bought us much, but we can apply an approximation to help us separate out some terms.\n\n\n7.5.4 3. Applying Jensen’s Inequality\nJensen’s Inequality states that, for a convex function \\(f\\):\n\\[\nf(\\mathbb{E}[x]) \\le \\mathbb{E}[f(x)]\n\\]\nand if we apply Jensen’s inequality to the above assuming \\(f=\\log\\) and \\(\\mathbb{E}=\\int\\) (note we flip the sign of the inequality since \\(\\log\\) is concave, not convex), we get:\n\\[\n\\log \\mathbb{E}_{q_\\phi}\\left[\\frac{p(Y|X, \\mathbf{w})\\, p(\\mathbf{w})}{q_\\phi(\\mathbf{w})}\\right]\n\\geq \\mathbb{E}_{q_\\phi}\\left[\\log \\frac{p(Y|X, \\mathbf{w})\\, p(\\mathbf{w})}{q_\\phi(\\mathbf{w})}\\right]\n\\]\nNow we can bring the log inside to help break out some of the terms and rearrange them.\n\n\n7.5.5 4. The ELBO Expression\nThis gives us the Evidence Lower Bound (ELBO):\n\\[\n\\log p(Y|X) \\geq \\mathbb{E}_{q_\\phi}\\left[\\log p(Y|X, \\mathbf{w})\\right]\n+ \\mathbb{E}_{q_\\phi}\\left[\\log p(\\mathbf{w})\\right]\n- \\mathbb{E}_{q_\\phi}\\left[\\log q_\\phi(\\mathbf{w})\\right]\n\\]\nOr, by grouping \\(\\mathbb{E}_{q_\\phi}\\left[\\log p(\\mathbf{w})\\right]\\) and \\(\\mathbb{E}_{q_\\phi}\\left[\\log q_\\phi(\\mathbf{w})\\right]\\) into the (negative) KL Divergence:\n\\[\n\\mathcal{L}(\\phi) = \\mathbb{E}_{q_\\phi}\\left[\\log p(Y|X, \\mathbf{w})\\right]\n- \\mathrm{KL}\\left(q_\\phi(\\mathbf{w})\\,\\|\\,p(\\mathbf{w})\\right)\n\\]\nWe arrive at the common form of the ELBO, where:\n\nThe first term encourages \\(q_\\phi\\) to place mass on weights that explain the data well. This is the Expected log-likelihood and measures How well \\(q_\\phi\\) explains the data.\nThe second term (the KL divergence) regularizes \\(q_\\phi\\) to stay close to the prior.\n\nThis is the objective we will then optimize when we perform variational inference. We will see this first applied to a simple 2D example so that we can understand what is going on, and then later move to a more complex and higher dimension example of a Neural Network.\n\n\n7.5.6 Example of using VI for the Banana Distribution\nLet’s use our concrete example from earlier, the Banana distribution, to illustrate how Variational Inference works. Here we will use a very simple proposal distribution—a Gaussian with a mean and diagonal covariance—which is often called the mean-fild Gaussian approximation as \\(q_\\phi(\\mathbf{w})\\). We will then optimize the ELBO to approximate the posterior. That is, we will optimize:\n\\[\n\\begin{align}\n\\mathcal{L}(\\phi) &= \\mathbb{E}_{q_\\phi}\\left[\\log p(Y|X, \\mathbf{w})\\right]\n- \\mathrm{KL}\\left(q_\\phi(\\mathbf{w})\\,\\|\\,p(\\mathbf{w})\\right)\\\\\n& = \\mathbb{E}_{q_\\phi}\\left[\\log p(Y|X, \\mathbf{w})\\right]\n+ \\mathbb{E}_{q_\\phi}\\left[\\log p(\\mathbf{w})\\right]\n- \\mathbb{E}_{q_\\phi}\\left[\\log q_\\phi(\\mathbf{w})\\right]\\\\\n& = \\mathbb{E}_{q_\\phi}\\left[\\log p(Y,w|X)\\right]\n- \\mathbb{E}_{q_\\phi}\\left[\\log q_\\phi(\\mathbf{w})\\right]\n\\end{align}\n\\]\nThe reason that we are only computing \\(\\mathbb{E}_{q_\\phi}\\left[\\log p(Y,w|X)\\right]\\) in this case is because we are using a simple toy distribution where we are computing the joint distribution \\(p(Y,w|X)\\) directly, rather than having a likelihood and prior separately as we would in a more complex model like a neural network.\n\n# Variational Inference on the banana target distribution with interactive visualization\nrng_vi = np.random.default_rng(0)\nLOG_2PI = math.log(2.0 * math.pi)\n\ndef mf_elbo(mu_vec: torch.Tensor, raw_diag: torch.Tensor, num_samples: int = None) -&gt; torch.Tensor:\n    \"\"\"Analytic ELBO for a mean-field (diagonal) Gaussian q.\n    \n    Assumes raw_diag stores log(sigma) for each dimension.\n    Computes E_q[log p] analytically for the banana target:\n        log p(x,y) = -0.5 * ( x^2/9 + (y - 0.1 x^2)^2 )\n    and uses the closed-form moments of a Gaussian.\n    \"\"\"\n    # diagonal std and variances\n    sigma = torch.exp(raw_diag)\n    var = sigma**2\n\n    mu_x, mu_y = mu_vec[0], mu_vec[1]\n    var_x, var_y = var[0], var[1]\n\n    # moments for x\n    E_x2 = var_x + mu_x**2\n    # Var(x^2) for Gaussian x ~ N(mu_x, var_x)\n    Var_x2 = 2.0 * var_x**2 + 4.0 * mu_x**2 * var_x\n\n    # First term in the ELBO\n    # E_q[ (y - 0.1 x^2)^2 ] = Var(y) + (E[y] - 0.1 E[x^2])^2 + 0.01 Var(x^2)\n    E_y_term = var_y + (mu_y - 0.1 * E_x2) ** 2 + 0.01 * Var_x2\n\n    # Second term in the ELBO\n    # E_q[log p]\n    E_log_p = -0.5 * (E_x2 / 9.0 + E_y_term)\n\n    # Third term in the ELBO\n    # E_q[log q] for diagonal Gaussian: -0.5*(D + D*log(2π) + 2 * sum(log sigma))\n    D = mu_vec.numel()\n    sum_log_sigma = torch.sum(raw_diag)\n    E_log_q = -0.5 * (D + D * LOG_2PI + 2.0 * sum_log_sigma)\n\n    return E_log_p - E_log_q\n\ndef run_vi_optimization(num_steps: int = 800, lr: float = 0.08, record_points: int = 60):\n    \"\"\"Optimize the ELBO using the mean-field (diagonal) analytic ELBO (mf_elbo).\n    \n    Records intermediate means, covariances (diagonal), and ELBO values for visualization.\n    \"\"\"\n    mu = torch.tensor([0.0, 0.0], requires_grad=True)\n    raw_diag = torch.log(torch.ones(2) * 2.5)\n    raw_diag.requires_grad_(True)\n\n    optimizer = Adam([mu, raw_diag], lr=lr)\n    record_every = 1\n    history = {\"steps\": [], \"mu\": [], \"cov\": [], \"elbo\": []}\n\n    for step in range(num_steps):\n        optimizer.zero_grad()\n        elbo = mf_elbo(mu, raw_diag)\n        loss = -elbo\n        loss.backward()\n        optimizer.step()\n\n        if (step % record_every == 0) or (step == num_steps - 1):\n            # build diagonal cov_diagonal and covariance for recording/plotting\n            cov_diagonal = np.diag(np.exp(raw_diag.detach().cpu().numpy()))\n            cov_np = cov_diagonal @ cov_diagonal.T\n            history[\"steps\"].append(step)\n            history[\"mu\"].append(mu.detach().cpu().numpy())\n            history[\"cov\"].append(cov_np)\n            history[\"elbo\"].append(elbo.detach().cpu().item())\n\n    return history\n\n# Run VI once and cache the trajectory for the interactive plot\nvi_history = run_vi_optimization(num_steps=150, record_points=150)\n\n\n\nShow Code\nhistory_steps = np.array(vi_history[\"steps\"])\nhistory_mu = np.stack(vi_history[\"mu\"])\nhistory_cov = np.stack(vi_history[\"cov\"])\nhistory_elbo = np.array(vi_history[\"elbo\"])\n\nx_grid = np.linspace(-10, 10, 240)\ny_grid = np.linspace(-5, 10, 220)\nX_grid, Y_grid = np.meshgrid(x_grid, y_grid)\ntarget_density_grid = np.exp(log_p_tilde(X_grid, Y_grid))\n\ndef gaussian_contour_density(X: np.ndarray, Y: np.ndarray, mu: np.ndarray, cov: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Evaluate a full-covariance Gaussian density on a meshgrid.\"\"\"\n    diff_x = X - mu[0]\n    diff_y = Y - mu[1]\n    inv_cov = np.linalg.inv(cov)\n    det_cov = float(np.maximum(np.linalg.det(cov), 1e-12))\n    exponent = (inv_cov[0, 0] * diff_x**2 +\n                2.0 * inv_cov[0, 1] * diff_x * diff_y +\n                inv_cov[1, 1] * diff_y**2)\n    norm_const = 1.0 / (2.0 * math.pi * np.sqrt(det_cov))\n    return norm_const * np.exp(-0.5 * exponent)\n\ndef plot_vi_state(frame_idx: int) -&gt; None:\n    \"\"\"Render the variational approximation and ELBO trace for the given optimization step.\"\"\"\n    mu = history_mu[frame_idx]\n    cov = history_cov[frame_idx]\n    elbo_val = history_elbo[frame_idx]\n    vi_density = gaussian_contour_density(X_grid, Y_grid, mu, cov)\n\n    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\n    ax = axes[0]\n    ax.contour(X_grid, Y_grid, target_density_grid, levels=14, colors='#b3b3b3', linewidths=1.0, alpha=0.8)\n    contourf = ax.contourf(X_grid, Y_grid, vi_density, levels=14, cmap='Blues', alpha=0.35)\n    ax.contour(X_grid, Y_grid, vi_density, levels=14, colors='#1f77b4', linewidths=1.1, alpha=0.9)\n    ax.scatter(mu[0], mu[1], color='tab:blue', s=40, label='VI mean')\n    ax.set_title(f\"VI approximation (step {history_steps[frame_idx]})\")\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_aspect('equal', adjustable='box')\n    ax.grid(True, alpha=0.25)\n    ax.legend(loc='upper right', fontsize=9)\n\n    ax_elbo = axes[1]\n    ax_elbo.plot(history_steps, history_elbo, color='tab:blue', linewidth=2)\n    ax_elbo.axvline(history_steps[frame_idx], color='tab:purple', linestyle='--', alpha=0.7)\n    ax_elbo.scatter(history_steps[frame_idx], elbo_val, color='tab:purple', s=40)\n    ax_elbo.set_title('ELBO during VI optimization')\n    ax_elbo.set_xlabel('Optimization step')\n    ax_elbo.set_ylabel('ELBO (MC estimate)')\n    ax_elbo.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\nstep_slider = widgets.IntSlider(value=len(history_steps) - 1, min=0, max=len(history_steps) - 1, step=1, description='Frame')\nplay_ctrl = widgets.Play(interval=120, value=0, min=0, max=len(history_steps) - 1, step=1, description='Press play')\nwidgets.jslink((play_ctrl, 'value'), (step_slider, 'value'))\ncontrols = widgets.HBox([play_ctrl, step_slider])\ninteractive_plot = widgets.interactive_output(plot_vi_state, {'frame_idx': step_slider})\n\ndisplay(controls, interactive_plot)\n\n\n\n\n\n\n\n\nWhat do we notice about the VI approximation compared to MCMC? In what ways is it better or worse? Comparing the ELBO plot versus the MCMC trace plot, what differences do you notice about the convergence behavior of the two methods?\n\n\n7.5.7 Monte Carlo Variational Inference\nBy assuming a certain form of the proposal distribution \\(q_\\phi(\\mathbf{w})\\), we were able to compute the KL divergence term in closed form and perform VI analytically. However, in more complex models (like neural networks), we often cannot compute these expectations exactly. In such cases, we can resort to using Monte Carlo Variational Inference to approximate the ELBO and its gradients.\nInstead of integrating analytically, we draw samples from our Gaussian variational family and use the score-function identity below to update the parameters. This reliance on Monte Carlo introduces sampling noise, so we will see below how the ELBO estimate bounces around and how the mean and covariance of \\(q_\\phi\\) drift across the banana distribution. The only change from the closed-form solution above is how we estimate expectations, where now our gradients still come from the “score function” of the Gaussian.2\n2 There are some tricks to reduce the variance by subtracting a moving “baseline” average to the advantage term, which is implemented below, but we will not go into the details of this here.The score-function identity states that for any distribution \\(q_\\phi(\\mathbf{w})\\) parameterized by \\(\\phi\\): \\[\\nabla_\\phi \\mathbb{E}_{q_\\phi} [f(\\mathbf{w})] = \\mathbb{E}_{q_\\phi} [f(\\mathbf{w}) \\nabla_\\phi \\log q_\\phi(\\mathbf{w})]\\]\nwhere in our case, \\(f(\\mathbf{w}) = \\log p(Y|X, \\mathbf{w}) + \\log p(\\mathbf{w}) - \\log q_\\phi(\\mathbf{w}) = \\log p(Y,\\mathbf{w}|X) - \\log q_\\phi(\\mathbf{w})\\). (This allows us to compute gradients of the ELBO with respect to \\(\\phi\\) using samples from \\(q_\\phi\\).)\nThis score-function identity is also sometimes called the “REINFORCE” gradient estimator in the reinforcement learning literature, as it was first popularized there and uses something called the “log-derivative trick”3 to move the gradient operator inside the expectation: \\[\n\\begin{align*}\n\\nabla_\\phi \\mathbb{E}_{q_\\phi} [f(\\mathbf{w})] &= \\nabla_\\phi \\int q_\\phi(\\mathbf{w}) f(\\mathbf{w}) d\\mathbf{w}\\\\\n&= \\int f(\\mathbf{w}) \\nabla_\\phi q_\\phi(\\mathbf{w}) d\\mathbf{w} \\\\\n&= \\int f(\\mathbf{w}) q_\\phi(\\mathbf{w}) \\nabla_\\phi \\log q_\\phi(\\mathbf{w}) d\\mathbf{w} \\\\\n&= \\mathbb{E}_{q_\\phi} [f(\\mathbf{w}) \\nabla_\\phi \\log q_\\phi(\\mathbf{w})]\\\\\n\\end{align*}\n\\]\n3 Recall that \\(\\nabla_\\phi \\log(z) = \\frac{\\nabla_\\phi z}{z}\\).So putting this all together for our toy example case here, we can estimate the gradient of the ELBO as: \\[\n\\begin{align*}\n\\nabla_\\phi \\mathcal{L}(\\phi) &= \\nabla_\\phi \\mathbb{E}_{q_\\phi}\\left[\\log p(Y,\\mathbf{w}|X)\\right]\n- \\nabla_\\phi \\mathbb{E}_{q_\\phi}\\left[\\log q_\\phi(\\mathbf{w})\\right]\\\\\n&= \\mathbb{E}_{q_\\phi}\\left[\\left(\\log p(Y,\\mathbf{w}|X) - \\log q_\\phi(\\mathbf{w})\\right) \\nabla_\\phi \\log q_\\phi(\\mathbf{w})\\right]\\\\\n&\\approx \\frac{1}{S} \\sum_{s=1}^S \\left(\\log p(Y,\\mathbf{w}^{(s)}|X) - \\log q_\\phi(\\mathbf{w}^{(s)})\\right) \\nabla_\\phi \\log q_\\phi(\\mathbf{w}^{(s)}) \\quad \\text{where } \\mathbf{w}^{(s)} \\sim q_\\phi(\\mathbf{w})\n\\end{align*}\n\\]\nSo we can see that this will break down into the following steps:\n\nSample \\(S\\) weight vectors \\(\\mathbf{w}^{(s)}\\) from the variational distribution \\(q_\\phi(\\mathbf{w})\\)\nFor each sample, compute the log joint probability \\(\\log p(Y,\\mathbf{w}^{(s)}|X)\\) and the log variational probability \\(\\log q_\\phi(\\mathbf{w}^{(s)})\\). We subtract these to get what is commonly called the “advantage” term, though we can see that it is just the difference between how well the sample explains the data versus how probable it is under the variational distribution, which is also akin to something called the “likelihood ratio”.\nCompute the score function \\(\\nabla_\\phi \\log q_\\phi(\\mathbf{w}^{(s)})\\) for each sample, which in this case is intuitively the direction (in the model weights) that will increase the log likelihood of the variational distribution.\nWe multiply these gradients by the advantage term of that sample from step 2. Intuitively, what this is doing is weighting the gradient direction by how much better that sample explains the data versus how probable it is under the variational distribution. If a sample explains the data well but is improbable under \\(q_\\phi\\), we want to move \\(q_\\phi\\) in that direction to increase its probability.\nAverage these gradients over all \\(S\\) samples to get an estimate of \\(\\nabla_\\phi \\mathcal{L}(\\phi)\\)\n\n\n# Monte Carlo VI using score-function gradients for a diagonal Gaussian\n\ndef torch_log_p(z: torch.Tensor) -&gt; torch.Tensor:\n    x = z[:, 0]\n    y = z[:, 1]\n    return -0.5 * ((x**2) / 9.0 + (y - 0.1 * x**2) ** 2)\n\ndef log_q_diag(z: torch.Tensor, mu: torch.Tensor, log_sigma: torch.Tensor) -&gt; torch.Tensor:\n    diff = z - mu\n    inv_var = torch.exp(-2.0 * log_sigma)\n    quad = torch.sum(diff**2 * inv_var, dim=1)\n    log_det = 2.0 * torch.sum(log_sigma)\n    return -0.5 * (quad + log_det + 2.0 * LOG_2PI)\n\nsteps = 600\nbatch_size = 256\nlr = 0.08\nrecord_every = 1\n\nmu = torch.tensor([0.0, 0.0])\nlog_sigma = torch.log(torch.ones(2) * 2.5)\n\nmc_history = {\n    \"steps\": [],\n    \"mu\": [],\n    \"cov\": [],\n    \"elbo\": [],\n    \"grad_var_mu\": [],\n    \"grad_var_log_sigma\": []\n}\n\nfor step in range(steps):\n\n    # Sample from q(z; mu, sigma)\n    sigma = torch.exp(log_sigma)\n    eps = torch.randn(batch_size, 2)\n\n    z_samples = (mu + sigma * eps).detach()\n\n    # Compute score-function gradients under samples from q\n    log_p = torch_log_p(z_samples)\n    log_q = log_q_diag(z_samples, mu, log_sigma)\n\n    # Compute the ELBO gradient estimates (also called the advantage in RL))\n    advantage = log_p - log_q\n\n    # Compute score functions for q at the samples\n    score_mu = (z_samples - mu) / (sigma**2)\n    score_log_sigma = -1.0 + (z_samples - mu) ** 2 / (sigma**2)\n\n    # Multiply the advantage with the score functions to get gradient estimates\n    grad_mu_samples = advantage.unsqueeze(1) * score_mu\n    grad_log_sigma_samples = advantage.unsqueeze(1) * score_log_sigma\n\n    # Take the average over the MC samples\n    grad_mu = grad_mu_samples.mean(dim=0)\n    grad_log_sigma = grad_log_sigma_samples.mean(dim=0)\n\n    # Now we can do a simple gradient ascent step\n    mu = mu + lr * grad_mu\n    log_sigma = log_sigma + lr * grad_log_sigma\n\n    if step % record_every == 0 or step == steps - 1:\n        cov = np.diag(torch.exp(2.0 * log_sigma).cpu().numpy())\n        mc_history[\"steps\"].append(step)\n        mc_history[\"mu\"].append(mu.cpu().numpy())\n        mc_history[\"cov\"].append(cov)\n        mc_history[\"elbo\"].append(advantage.mean().item())\n        mc_history[\"grad_var_mu\"].append(grad_mu_samples.var(dim=0, unbiased=False).mean().item())\n        mc_history[\"grad_var_log_sigma\"].append(grad_log_sigma_samples.var(dim=0, unbiased=False).mean().item())\n\n\n\nShow Code\nmc_history[\"steps\"] = np.array(mc_history[\"steps\"])\nmc_history[\"mu\"] = np.stack(mc_history[\"mu\"])\nmc_history[\"cov\"] = np.stack(mc_history[\"cov\"])\nmc_history[\"elbo\"] = np.array(mc_history[\"elbo\"])\nmc_history[\"grad_var_mu\"] = np.array(mc_history[\"grad_var_mu\"])\nmc_history[\"grad_var_log_sigma\"] = np.array(mc_history[\"grad_var_log_sigma\"])\n\ndef plot_mc_state(frame_idx: int) -&gt; None:\n    mu_frame = mc_history[\"mu\"][frame_idx]\n    cov_frame = mc_history[\"cov\"][frame_idx]\n    step_val = mc_history[\"steps\"][frame_idx]\n    elbo_val = mc_history[\"elbo\"][frame_idx]\n\n    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\n    # adapt variable names\n    mu = mu_frame\n    history_steps = mc_history[\"steps\"]\n\n    # compute VI density on the plotting grid (assumes X_grid, Y_grid are defined as numpy meshgrid)\n    mu_x, mu_y = float(mu[0]), float(mu[1])\n    var_x = float(cov_frame[0, 0])\n    var_y = float(cov_frame[1, 1])\n    denom = 2.0 * np.pi * np.sqrt(var_x * var_y)\n    exponent = -0.5 * (((X_grid - mu_x) ** 2) / var_x + ((Y_grid - mu_y) ** 2) / var_y)\n    vi_density = np.exp(exponent) / denom\n\n    ax = axes[0]\n    ax.contour(X_grid, Y_grid, target_density_grid, levels=14, colors='#b3b3b3', linewidths=1.0, alpha=0.8)\n    contourf = ax.contourf(X_grid, Y_grid, vi_density, levels=14, cmap='Blues', alpha=0.35)\n    ax.contour(X_grid, Y_grid, vi_density, levels=14, colors='#1f77b4', linewidths=1.1, alpha=0.9)\n    ax.scatter(mu[0], mu[1], color='tab:blue', s=40, label='VI mean')\n    ax.set_title(f\"MC VI (score-function) — step {history_steps[frame_idx]})\")\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_aspect('equal', adjustable='box')\n    ax.grid(True, alpha=0.25)\n    ax.legend(loc='upper right', fontsize=9)\n\n    ax = axes[1]\n    ax.plot(mc_history[\"steps\"], mc_history[\"elbo\"], color='tab:blue', linewidth=2, label='ELBO estimate')\n    ax.axvline(step_val, color='tab:purple', linestyle='--', alpha=0.7)\n    ax.scatter(step_val, elbo_val, color='tab:purple', s=40)\n    ax.set_title('ELBO during MC VI optimisation')\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('ELBO (MC estimate)')\n    ax.grid(True, alpha=0.3)\n\n    ax2 = ax.twinx()\n    ax2.plot(mc_history[\"steps\"], mc_history[\"grad_var_mu\"], color='tab:red', linestyle='--', linewidth=1.6, label='Var[∇μ]')\n    ax2.set_ylabel('Gradient variance (μ)')\n\n    lines, labels = ax.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax.legend(lines + lines2, labels + labels2, loc='lower right', fontsize=9)\n\n    plt.tight_layout()\n    plt.show()\n\nmc_slider = widgets.IntSlider(value=len(mc_history[\"steps\"]) - 1, min=0, max=len(mc_history[\"steps\"]) - 1, step=1, description='Frame')\nmc_play = widgets.Play(interval=120, value=0, min=0, max=len(mc_history[\"steps\"]) - 1, step=1, description='Press play')\nwidgets.jslink((mc_play, 'value'), (mc_slider, 'value'))\nmc_controls = widgets.HBox([mc_play, mc_slider])\nmc_plot = widgets.interactive_output(plot_mc_state, {'frame_idx': mc_slider})\n\ndisplay(mc_controls, mc_plot)\n\nprint(f\"Final mean: {mc_history['mu'][-1]}\")\nprint(f\"Final marginal std: {np.sqrt(np.diag(mc_history['cov'][-1]))}\")\nprint(f\"Average gradient variance (μ): {mc_history['grad_var_mu'].mean():.3f}\")\n\n\n\n\n\n\n\n\nFinal mean: [0.05686871 0.33599067]\nFinal marginal std: [1.9444891 1.0056936]\nAverage gradient variance (μ): 5.516\n\n\n\n\n7.5.8 The Reparameterization Trick\nFrom the above we saw that we can now estimate VI updates even if we don’t have a closed form solution, provided that we can compute gradients w.r.t. the score function (essentially the log likelihood of \\(q\\)), which we could in principle get through something like Automatic Differentiation. However, there is a problem with this: to compute the score function gradients, we need to differentiate through samples drawn from \\(q_\\phi(\\mathbf{w})\\). That is, our expectation is over samples \\(\\mathbf{w} \\sim q_\\phi(\\mathbf{w})\\), and the score function \\(\\nabla_\\phi \\log q_\\phi(\\mathbf{w})\\) depends on \\(\\phi\\) both directly (since \\(q_\\phi\\) is parameterized by \\(\\phi\\)) and indirectly through the samples \\(\\mathbf{w}\\). This leads to stochasticity in the gradients due to the sampling process, which can result in high-variance gradient estimates.\nTo get around this, we can use something called the reparameterization trick, whose key idea is to move the stochasticity/randomness outside of the gradient computation. This is done by expressing samples from \\(q_\\phi(\\mathbf{w})\\) as a deterministic function of \\(\\phi\\) and some noise variable \\(\\boldsymbol{\\epsilon}\\) that is independent of \\(\\phi\\). For example, if \\(q_\\phi(\\mathbf{w})\\) is a Gaussian with mean \\(\\boldsymbol{\\mu}_\\phi\\) and standard deviation \\(\\boldsymbol{\\sigma}_\\phi\\), we can write: \\[\\mathbf{w} = \\boldsymbol{\\mu}_\\phi + \\boldsymbol{\\sigma}_\\phi \\odot \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, I)\\]\nWhere the expectation is not over \\(\\mathbf{w} \\sim q_\\phi(\\mathbf{w})\\) anymore, but rather over \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, I)\\).\nNote now that \\(\\mathbf{w}\\) is still a random variable, but the randomness comes solely from \\(\\boldsymbol{\\epsilon}\\), which is independent of \\(\\phi\\). Instead \\(\\phi\\) becomes a deterministic transformation of \\(\\boldsymbol{\\epsilon}\\). This allows us to rewrite the ELBO expectation as: \\[\n\\begin{align*}\n\\nabla_\\phi \\mathcal{L}(\\phi) &= \\mathbb{E}_{q_\\phi}\\left[\\left(\\log p(Y,\\mathbf{w}|X) - \\log q_\\phi(\\mathbf{w})\\right) \\nabla_\\phi \\log q_\\phi(\\mathbf{w})\\right]\\\\\n&= \\mathbb{E}_{\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, I)}\\left[\\left(\\log p(Y,\\mathbf{w}(\\boldsymbol{\\epsilon}, \\phi)|X) - \\log q_\\phi(\\mathbf{w}(\\boldsymbol{\\epsilon}, \\phi))\\right) \\nabla_\\phi \\log q_\\phi(\\mathbf{w}(\\boldsymbol{\\epsilon}, \\phi))\\right]\\\\\n&\\approx \\frac{1}{S} \\sum_{s=1}^S \\left(\\log p(Y,\\mathbf{w}(\\boldsymbol{\\epsilon}^{(s)}, \\phi)|X) - \\log q_\\phi(\\mathbf{w}(\\boldsymbol{\\epsilon}^{(s)}, \\phi))\\right) \\nabla_\\phi \\log q_\\phi(\\mathbf{w}(\\boldsymbol{\\epsilon}^{(s)}, \\phi)) \\quad \\text{where } \\boldsymbol{\\epsilon}^{(s)} \\sim \\mathcal{N}(0, I)\n\\end{align*}\n\\]\nThis subtle change has a few important implications:\n\nThe randomness is now isolated in \\(\\boldsymbol{\\epsilon}\\), which does not depend on \\(\\phi\\). This means that when we compute gradients w.r.t. \\(\\phi\\), we can treat \\(\\boldsymbol{\\epsilon}\\) as a constant. This reduces the variance of our gradient estimates significantly.\nThe gradients \\(\\nabla_\\phi \\log q_\\phi(\\mathbf{w}(\\boldsymbol{\\epsilon}, \\phi))\\) can now be computed using standard automatic differentiation techniques, as \\(\\mathbf{w}\\) is a deterministic function of \\(\\phi\\) and \\(\\boldsymbol{\\epsilon}\\).\nThis trick is particularly useful when \\(q_\\phi(\\mathbf{w})\\) is a continuous distribution like a Gaussian, but it can be extended to non-continuous distributions as well, which we may cover in a later notebook.\n\nThis approach is sometimes called the “pathwise derivative estimator” because it allows us to compute gradients by following a deterministic path through the parameter space, rather than relying on stochastic sampling, and sits in contrast to “score-function” estimators which rely on sampling directly from the distribution.\n\n# Reparameterized VI with automatic differentiation\nmu_param = torch.nn.Parameter(torch.tensor([0.0, 0.0]))\nlog_sigma_param = torch.nn.Parameter(torch.log(torch.ones(2) * 2.5))\noptimizer = Adam([mu_param, log_sigma_param], lr=0.05)\n\nsteps = 600\nbatch_size = 256\nrecord_every = 1\n\nreparam_history = {\n    \"steps\": [],\n    \"mu\": [],\n    \"cov\": [],\n    \"elbo\": [],\n    \"grad_var_mu\": [],\n    \"grad_var_log_sigma\": [],\n    \"grad_norm\": []\n}\n\nfor step in range(steps):\n    optimizer.zero_grad()\n\n    # Sample from q(z; mu, sigma) using reparameterization\n    sigma = torch.exp(log_sigma_param)\n    eps = torch.randn(batch_size, 2)\n    z = mu_param + sigma * eps\n\n    # Compute ELBO using reparameterization trick\n    log_p = torch_log_p(z)\n    log_q = -0.5 * ((eps**2).sum(dim=1) + 2.0 * torch.sum(log_sigma_param) + 2.0 * LOG_2PI)\n\n    # Compute ELBO (aka the advantage)\n    elbo = (log_p - log_q).mean()\n\n    # Call AD backward to compute gradients directly on the ELBO\n    (-elbo).backward()\n\n    grad_norm = torch.sqrt(sum(param.grad.detach().pow(2).sum() for param in [mu_param, log_sigma_param])).item()\n    optimizer.step()\n\n    if step % record_every == 0 or step == steps - 1:\n        with torch.no_grad():\n            sigma_det = torch.exp(log_sigma_param)\n            z_det = (mu_param + sigma_det * eps).detach()\n            grad_log_p = torch.stack([\n                -(z_det[:, 0] / 9.0) + 0.2 * z_det[:, 0] * (z_det[:, 1] - 0.1 * z_det[:, 0] ** 2),\n                -(z_det[:, 1] - 0.1 * z_det[:, 0] ** 2)\n            ], dim=1)\n            grad_mu_samples = grad_log_p\n            grad_log_sigma_samples = grad_log_p * (sigma_det * eps) + 1.0\n\n            cov = np.diag(torch.exp(2.0 * log_sigma_param).cpu().numpy())\n            reparam_history[\"steps\"].append(step)\n            reparam_history[\"mu\"].append(mu_param.detach().cpu().numpy())\n            reparam_history[\"cov\"].append(cov)\n            reparam_history[\"elbo\"].append(elbo.item())\n            reparam_history[\"grad_var_mu\"].append(grad_mu_samples.var(dim=0, unbiased=False).mean().item())\n            reparam_history[\"grad_var_log_sigma\"].append(grad_log_sigma_samples.var(dim=0, unbiased=False).mean().item())\n            reparam_history[\"grad_norm\"].append(grad_norm)\n\n\n\nShow Code\nreparam_history[\"steps\"] = np.array(reparam_history[\"steps\"])\nreparam_history[\"mu\"] = np.stack(reparam_history[\"mu\"])\nreparam_history[\"cov\"] = np.stack(reparam_history[\"cov\"])\nreparam_history[\"elbo\"] = np.array(reparam_history[\"elbo\"])\nreparam_history[\"grad_var_mu\"] = np.array(reparam_history[\"grad_var_mu\"])\nreparam_history[\"grad_var_log_sigma\"] = np.array(reparam_history[\"grad_var_log_sigma\"])\nreparam_history[\"grad_norm\"] = np.array(reparam_history[\"grad_norm\"])\n\ndef plot_reparam_state(frame_idx: int) -&gt; None:\n    mu_frame = reparam_history[\"mu\"][frame_idx]\n    cov_frame = reparam_history[\"cov\"][frame_idx]\n    step_val = reparam_history[\"steps\"][frame_idx]\n    elbo_val = reparam_history[\"elbo\"][frame_idx]\n\n    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\n    # adapt variable names\n    mu = mu_frame\n    history_steps = reparam_history[\"steps\"]\n\n    # compute VI density on the plotting grid (assumes X_grid, Y_grid are defined as numpy meshgrid)\n    mu_x, mu_y = float(mu[0]), float(mu[1])\n    var_x = float(cov_frame[0, 0])\n    var_y = float(cov_frame[1, 1])\n    denom = 2.0 * np.pi * np.sqrt(var_x * var_y)\n    exponent = -0.5 * (((X_grid - mu_x) ** 2) / var_x + ((Y_grid - mu_y) ** 2) / var_y)\n    vi_density = np.exp(exponent) / denom\n\n    ax = axes[0]\n    ax.contour(X_grid, Y_grid, target_density_grid, levels=14, colors='#b3b3b3', linewidths=1.0, alpha=0.8)\n    contourf = ax.contourf(X_grid, Y_grid, vi_density, levels=14, cmap='Blues', alpha=0.35)\n    ax.contour(X_grid, Y_grid, vi_density, levels=14, colors='#1f77b4', linewidths=1.1, alpha=0.9)\n    ax.scatter(mu[0], mu[1], color='tab:blue', s=40, label='VI mean')\n    ax.set_title(f\"MC VI (score-function) — step {history_steps[frame_idx]})\")\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_aspect('equal', adjustable='box')\n    ax.grid(True, alpha=0.25)\n    ax.legend(loc='upper right', fontsize=9)\n\n    ax = axes[1]\n    ax.plot(reparam_history[\"steps\"], reparam_history[\"elbo\"], color='tab:blue', linewidth=2, label='ELBO estimate')\n    ax.axvline(step_val, color='tab:purple', linestyle='--', alpha=0.7)\n    ax.scatter(step_val, elbo_val, color='tab:purple', s=40)\n    ax.set_title('ELBO during MC VI optimisation')\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('ELBO (MC estimate)')\n    ax.grid(True, alpha=0.3)\n\n    ax2 = ax.twinx()\n    ax2.plot(reparam_history[\"steps\"], reparam_history[\"grad_var_mu\"], color='tab:red', linestyle='--', linewidth=1.6, label='Var[∇μ]')\n    ax2.set_ylabel('Gradient variance (μ)')\n\n    lines, labels = ax.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax.legend(lines + lines2, labels + labels2, loc='lower right', fontsize=9)\n\n    plt.tight_layout()\n    plt.show()\n\nreparam_slider = widgets.IntSlider(value=len(reparam_history[\"steps\"]) - 1, min=0, max=len(reparam_history[\"steps\"]) - 1, step=1, description='Frame')\nreparam_play = widgets.Play(interval=120, value=0, min=0, max=len(reparam_history[\"steps\"]) - 1, step=1, description='Press play')\nwidgets.jslink((reparam_play, 'value'), (reparam_slider, 'value'))\nreparam_controls = widgets.HBox([reparam_play, reparam_slider])\nreparam_plot = widgets.interactive_output(plot_reparam_state, {'frame_idx': reparam_slider})\n\ndisplay(reparam_controls, reparam_plot)\n\nprint(f\"Final mean: {reparam_history['mu'][-1]}\")\nprint(f\"Final marginal std: {np.sqrt(np.diag(reparam_history['cov'][-1]))}\")\nprint(f\"Average gradient variance (μ): {reparam_history['grad_var_mu'].mean():.3f}\")\n\n\n\n\n\n\n\n\nFinal mean: [0.03177835 0.3603806 ]\nFinal marginal std: [1.9253347 1.0171496]\nAverage gradient variance (μ): 1.408\n\n\n\n\n7.5.9 Black-Box Variational Inference\nWe saw above how to use the reparameterization trick to help reduce variance in estimating the VI update in cases where \\(q_\\phi(\\mathbf{w})\\) is a continuous distribution, such as a Gaussian or even a simpler feedforward Neural Network. In many cases (like we will see next in VAEs) this is sufficient and we are good to go.\nHowever, there are certain cases where basic reparameterization will not work. For example, if I pick a variational family \\(q_\\phi(\\mathbf{w})\\) that is no longer a nice smooth function of the parameters \\(w\\), we will not be able to reparameterize \\(q_\\phi(\\mathbf{w})\\) into a smoot deterministic function. This is common in cases where the desired \\(q_\\phi(\\mathbf{w})\\) has discrete variables, such as clusters or categories.\nIn such cases, we actually have three options:\n\nWe can go back to score estimators like we did earlier in the notebook. We will suffer from higher variance updates but it will at least allow us to train the model even under discrete or non-continuous samples.4 This is the approach we will take below.\nWe can essentially try to “relax” discontinuities by finding a distribution that can approximate the “hard” or discrete objects we wish to study by variants that can be differentiated through. The price we will pay for this is that the optimal solution to the relaxed version of the model will be slightly biased away from the optimal solution to the original hard problem, but often this approximation error and bias is acceptable. Classical examples of these distributions include the Gumbel-Softmax or Concrete Distribution for relaxing Categorical variables, or the Birkhoff Polytope for relaxing Permutation Matrices. These extensions go beyond what I want to cover in this book, but are fascinating topics to read more about.\nThere are also classes of “Straight-Through Estimators” which essentially detach certain gradients from the computational graph such that one can sample hard/discrete samples, but then access non-hard gradients when calling automatic differentiation. These inherit similar problems to the relaxed distributions mentioned above, which is that the gradients you compute using Straight-Through estimators are only approximating, in a heuristic sense, the true gradients.\n\n4 There are some tricks to reduce the variance by subtracting a moving “baseline” average to the advantage term, which is implemented below, but we will not go into the details of this here.We will demonstrate an example below where reparameterization will not be effective by fitting a three member Gaussian Mixture Model as our \\(q_\\phi(\\mathbf{w})\\) and resorting to score-function estimators. Note that this below example takes a significantly longer time to fit/train than the examples we have used so far.\n\n# Black-box VI with a Gaussian mixture and REINFORCE gradients\ntry:\n    from scipy.stats import gaussian_kde\nexcept ImportError:\n    gaussian_kde = None\n\ndef log_mixture_prob(z: torch.Tensor, locs: torch.Tensor, log_scales: torch.Tensor, logits: torch.Tensor) -&gt; torch.Tensor:\n    diff = z.unsqueeze(1) - locs.unsqueeze(0)\n    inv_var = torch.exp(-2.0 * log_scales).unsqueeze(0)\n    quad = torch.sum(diff**2 * inv_var, dim=2)\n    log_det = 2.0 * log_scales.sum(dim=1)\n    log_weights = F.log_softmax(logits, dim=0)\n    component_log_prob = -0.5 * (quad + log_det.unsqueeze(0) + 2.0 * LOG_2PI)\n    return torch.logsumexp(log_weights.unsqueeze(0) + component_log_prob, dim=1)\n\ndef sample_mixture(locs: torch.Tensor, log_scales: torch.Tensor, logits: torch.Tensor, num_samples: int) -&gt; torch.Tensor:\n    weights = F.softmax(logits, dim=0)\n    cat = torch.distributions.Categorical(probs=weights)\n    indices = cat.sample((num_samples,))\n    sigma = torch.exp(log_scales)\n    eps = torch.randn(num_samples, 2)\n    samples = locs[indices] + sigma[indices] * eps\n    return samples.detach()\n\ndef radial_density_estimate(samples: np.ndarray, X: np.ndarray, Y: np.ndarray, bandwidth: float = 0.7, max_centers: int = 240) -&gt; np.ndarray:\n    rng_local = np.random.default_rng(0)\n    if len(samples) &gt; max_centers:\n        idx = rng_local.choice(len(samples), max_centers, replace=False)\n        centers = samples[idx]\n    else:\n        centers = samples\n    diff_x = X[None, :, :] - centers[:, 0][:, None, None]\n    diff_y = Y[None, :, :] - centers[:, 1][:, None, None]\n    density = np.exp(-(diff_x**2 + diff_y**2) / (2.0 * bandwidth**2)).sum(axis=0)\n    norm = (2.0 * math.pi * bandwidth**2) * centers.shape[0]\n    return density / norm\n\nnum_components = 3\nlogits = torch.nn.Parameter(torch.zeros(num_components))\nlocs = torch.nn.Parameter(torch.tensor([[6.0, 6.0], [0.0, 0.0], [-6.0, 6.0]]))\nlog_scales = torch.nn.Parameter(torch.log(torch.ones(num_components, 2) * 2.5))\noptimizer = Adam([logits, locs, log_scales], lr=0.05)\n\nsteps = 250\nbatch_size = 512\neval_samples = 6000\nrecord_every = 1\nbaseline = 0.0\nbaseline_beta = 0.05\n\nbbvi_history = {\n    \"steps\": [],\n    \"elbo\": [],\n    \"grad_norm\": [],\n    \"baseline\": [],\n    \"mean\": [],\n    \"cov\": [],\n    \"density\": []\n}\n\nfor step in range(steps):\n    optimizer.zero_grad()\n    sigma = torch.exp(log_scales)\n\n    weights = F.softmax(logits, dim=0)\n    cat = torch.distributions.Categorical(probs=weights)\n    indices = cat.sample((batch_size,))\n    eps = torch.randn(batch_size, 2)\n    samples = (locs[indices] + sigma[indices] * eps).detach()\n\n    log_q = log_mixture_prob(samples, locs, log_scales, logits)\n    log_p = torch_log_p(samples)\n    elbo_estimate = (log_p - log_q).mean().item()\n\n    if step == 0:\n        baseline = elbo_estimate\n    baseline = (1.0 - baseline_beta) * baseline + baseline_beta * elbo_estimate\n    advantage = (log_p - log_q - baseline).detach()\n\n    loss = -(advantage * log_q).mean()\n    loss.backward()\n    grad_norm = torch.sqrt(sum(param.grad.detach().pow(2).sum() for param in [logits, locs, log_scales])).item()\n    optimizer.step()\n\n    if step % record_every == 0 or step == steps - 1:\n        with torch.no_grad():\n            eval_samples_tensor = sample_mixture(locs, log_scales, logits, eval_samples)\n            eval_np = eval_samples_tensor.cpu().numpy()\n            mean_np = eval_np.mean(axis=0)\n            cov_np = np.cov(eval_np, rowvar=False)\n            if gaussian_kde is not None:\n                kde = gaussian_kde(eval_np.T)\n                density = kde(np.vstack([X_grid.ravel(), Y_grid.ravel()])).reshape(X_grid.shape)\n            else:\n                density = radial_density_estimate(eval_np, X_grid, Y_grid)\n        bbvi_history[\"steps\"].append(step)\n        bbvi_history[\"elbo\"].append(elbo_estimate)\n        bbvi_history[\"grad_norm\"].append(grad_norm)\n        bbvi_history[\"baseline\"].append(baseline)\n        bbvi_history[\"mean\"].append(mean_np)\n        bbvi_history[\"cov\"].append(cov_np)\n        bbvi_history[\"density\"].append(density)\n\n\n\nShow Code\nbbvi_history[\"steps\"] = np.array(bbvi_history[\"steps\"])\nbbvi_history[\"elbo\"] = np.array(bbvi_history[\"elbo\"])\nbbvi_history[\"grad_norm\"] = np.array(bbvi_history[\"grad_norm\"])\nbbvi_history[\"baseline\"] = np.array(bbvi_history[\"baseline\"])\nbbvi_history[\"mean\"] = np.stack(bbvi_history[\"mean\"])\nbbvi_history[\"cov\"] = np.stack(bbvi_history[\"cov\"])\nbbvi_history[\"density\"] = np.stack(bbvi_history[\"density\"])\n\nfinal_weights = F.softmax(logits.detach(), dim=0).cpu().numpy()\nfinal_locs = locs.detach().cpu().numpy()\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\nax = axes[0]\nax.contour(X_grid, Y_grid, target_density_grid, levels=14, colors='#b3b3b3', linewidths=1.0, alpha=0.8)\nax.contourf(X_grid, Y_grid, bbvi_history[\"density\"][-1], levels=14, cmap='Blues', alpha=0.35)\nax.contour(X_grid, Y_grid, bbvi_history[\"density\"][-1], levels=14, colors='#1f77b4', linewidths=1.1, alpha=0.9)\nscatter_sizes = 300 * final_weights\nax.scatter(final_locs[:, 0], final_locs[:, 1], s=scatter_sizes, c='tab:orange', edgecolors='black', linewidths=1.0, label='Mixture means')\nax.set_title('Gaussian mixture $q_{\\phi}$ (final snapshot)')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_aspect('equal', adjustable='box')\nax.grid(True, alpha=0.3)\nax.legend(loc='upper right', fontsize=9)\n\nax = axes[1]\nax.plot(bbvi_history[\"steps\"], bbvi_history[\"elbo\"], color='tab:blue', linewidth=2, label='ELBO estimate')\nax.set_title('Score-function optimisation trace')\nax.set_xlabel('Iteration')\nax.set_ylabel('ELBO')\nax.grid(True, alpha=0.3)\n\nax2 = ax.twinx()\nax2.plot(bbvi_history[\"steps\"], bbvi_history[\"grad_norm\"], color='tab:red', linestyle='--', linewidth=1.6, label='Gradient norm')\nax2.set_ylabel('Gradient norm')\n\nlines, labels = ax.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax.legend(lines + lines2, labels + labels2, loc='lower right', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Final mixture weights: {np.round(final_weights, 3)}\")\nprint(f\"Final mixture means: {np.round(final_locs, 3)}\")\nprint(f\"Average ELBO estimate: {bbvi_history['elbo'].mean():.3f}\")\n\n\n\n\n\n\n\n\n\nFinal mixture weights: [0.043 0.906 0.051]\nFinal mixture means: [[ 5.449  3.545]\n [ 0.021  0.365]\n [-5.234  3.275]]\nAverage ELBO estimate: 2.372\n\n\nWith this you now have a high-level overview of some of this major approaches to performing Variational Inference as well as their pros and cons. We can now move on to applying this to a more complex example of the Neural Network from earlier.\n\n\n7.5.10 Example of using VI for Neural Networks\nSince in this case the Neural Network is a continuous and differentiable function, we will leverage the reparameterization trick to allow us to automatically differentiate backwards through the network to perform the VI updates.\n\n# Variational Inference for Bayesian Neural Network\nclass BayesianNNVI(nn.Module):\n    \"\"\"\n    Bayesian Neural Network with Variational Inference.\n    \n    Each weight has a variational posterior q(w) = N(mu, sigma^2).\n    \"\"\"\n    \n    def __init__(self, hidden_size=5):\n        super().__init__()\n        self.hidden_size = hidden_size\n        \n        # Variational parameters for layer 1: input -&gt; hidden\n        self.fc1_mu_weight = nn.Parameter(torch.randn(hidden_size, 1) * 0.1)\n        self.fc1_mu_bias = nn.Parameter(torch.randn(hidden_size) * 0.1)\n        self.fc1_logsigma_weight = nn.Parameter(torch.ones(hidden_size, 1) * -3)  # Start with low variance\n        self.fc1_logsigma_bias = nn.Parameter(torch.ones(hidden_size) * -3)\n        \n        # Variational parameters for layer 2: hidden -&gt; output\n        self.fc2_mu_weight = nn.Parameter(torch.randn(1, hidden_size) * 0.1)\n        self.fc2_mu_bias = nn.Parameter(torch.randn(1) * 0.1)\n        self.fc2_logsigma_weight = nn.Parameter(torch.ones(1, hidden_size) * -3)\n        self.fc2_logsigma_bias = nn.Parameter(torch.ones(1) * -3)\n        \n    def sample_weights(self):\n        \"\"\"Sample weights using reparameterization trick.\"\"\"\n        # Layer 1\n        fc1_weight = self.fc1_mu_weight + torch.exp(self.fc1_logsigma_weight) * torch.randn_like(self.fc1_mu_weight)\n        fc1_bias = self.fc1_mu_bias + torch.exp(self.fc1_logsigma_bias) * torch.randn_like(self.fc1_mu_bias)\n        \n        # Layer 2\n        fc2_weight = self.fc2_mu_weight + torch.exp(self.fc2_logsigma_weight) * torch.randn_like(self.fc2_mu_weight)\n        fc2_bias = self.fc2_mu_bias + torch.exp(self.fc2_logsigma_bias) * torch.randn_like(self.fc2_mu_bias)\n        \n        return (fc1_weight, fc1_bias, fc2_weight, fc2_bias)\n    \n    def forward(self, x, sample=True):\n        \"\"\"Forward pass with sampled or mean weights.\"\"\"\n        x = x.view(-1, 1)\n        \n        if sample:\n            fc1_w, fc1_b, fc2_w, fc2_b = self.sample_weights()\n        else:\n            fc1_w, fc1_b = self.fc1_mu_weight, self.fc1_mu_bias\n            fc2_w, fc2_b = self.fc2_mu_weight, self.fc2_mu_bias\n        \n        # Forward propagation\n        h = F.relu(F.linear(x, fc1_w, fc1_b))\n        y = F.linear(h, fc2_w, fc2_b)\n        \n        return y.squeeze()\n    \n    def kl_divergence(self, prior_std=1.0):\n        \"\"\"Compute KL(q||p) where p is N(0, prior_std^2).\"\"\"\n        kl = 0\n        \n        # KL for each parameter: KL(N(mu, sigma^2) || N(0, prior_std^2))\n        for mu_param, logsigma_param in [\n            (self.fc1_mu_weight, self.fc1_logsigma_weight),\n            (self.fc1_mu_bias, self.fc1_logsigma_bias),\n            (self.fc2_mu_weight, self.fc2_logsigma_weight),\n            (self.fc2_mu_bias, self.fc2_logsigma_bias)\n        ]:\n            sigma = torch.exp(logsigma_param)\n            kl += 0.5 * torch.sum(\n                (mu_param**2 + sigma**2) / prior_std**2 - 1 - 2*logsigma_param + 2*np.log(prior_std)\n            )\n        \n        return kl\n    \n    def elbo(self, x, y, n_samples=5, noise_std=0.1, prior_std=1.0):\n        \"\"\"\n        Compute the Evidence Lower Bound.\n        \n        ELBO = E_q[log p(y|x,w)] - KL(q||p)\n        \"\"\"\n        x = x.view(-1, 1)\n        \n        # Expected log-likelihood (Monte Carlo estimate)\n        log_lik = 0\n        for _ in range(n_samples):\n            y_pred = self.forward(x, sample=True)\n            log_lik += -0.5 * torch.sum(((y - y_pred) / noise_std) ** 2)\n        log_lik = log_lik / n_samples\n        log_lik -= len(y) * np.log(noise_std * np.sqrt(2 * np.pi))\n        \n        # KL divergence\n        kl = self.kl_divergence(prior_std)\n        \n        # ELBO\n        elbo = log_lik - kl\n        \n        return elbo, log_lik, kl\n\n# Initialize variational model\nvi_model = BayesianNNVI(hidden_size=5)\n\n# Uncomment below if you would like a listing of all parameters and counts:\n# print(\"Variational parameters:\")\n# for name, param in vi_model.named_parameters():\n#     print(f\"  {name}: shape {param.shape}\")\n#     n_var_params = sum(p.numel() for p in vi_model.parameters())\n#     print(f\"Total variational parameters: {n_var_params}\")\n#     print(\"(2x the number of network weights for mean + variance)\")\n\n\n\nShow Code\n# Train with Variational Inference\nvi_model = BayesianNNVI(hidden_size=5)\noptimizer = torch.optim.Adam(vi_model.parameters(), lr=0.01)\n\nx_train_tensor = torch.FloatTensor(x_train)\ny_train_tensor = torch.FloatTensor(y_train)\n\nn_epochs = 3000\nn_mc_samples = 5  # Monte Carlo samples per ELBO evaluation\n\nhistory = {\n    'elbo': [],\n    'log_lik': [],\n    'kl': []\n}\n\nprint(\"Training Bayesian NN with Variational Inference...\")\nprint(f\"Epochs: {n_epochs}, MC samples: {n_mc_samples}\")\n\nfor epoch in range(n_epochs):\n    optimizer.zero_grad()\n    \n    # Compute ELBO (negative for minimization)\n    elbo, log_lik, kl = vi_model.elbo(x_train_tensor, y_train_tensor, \n                                       n_samples=n_mc_samples, \n                                       noise_std=noise_std, \n                                       prior_std=1.0)\n    loss = -elbo  # Maximize ELBO = minimize negative ELBO\n    \n    # Backprop and update\n    loss.backward()\n    optimizer.step()\n    \n    # Track metrics\n    history['elbo'].append(elbo.item())\n    history['log_lik'].append(log_lik.item())\n    history['kl'].append(kl.item())\n    \n    if (epoch + 1) % 500 == 0:\n        print(f\"Epoch {epoch+1:4d} | ELBO: {elbo.item():8.2f} | \"\n              f\"Log-lik: {log_lik.item():8.2f} | KL: {kl.item():6.2f}\")\nprint(\"Training complete!\")\n\n\nTraining Bayesian NN with Variational Inference...\nEpochs: 3000, MC samples: 5\nEpoch  500 | ELBO:  -123.28 | Log-lik:   -86.77 | KL:  36.51\nEpoch  500 | ELBO:  -123.28 | Log-lik:   -86.77 | KL:  36.51\nEpoch 1000 | ELBO:  -113.35 | Log-lik:   -80.70 | KL:  32.65\nEpoch 1000 | ELBO:  -113.35 | Log-lik:   -80.70 | KL:  32.65\nEpoch 1500 | ELBO:   -35.02 | Log-lik:     7.50 | KL:  42.52\nEpoch 1500 | ELBO:   -35.02 | Log-lik:     7.50 | KL:  42.52\nEpoch 2000 | ELBO:   -32.42 | Log-lik:    12.57 | KL:  44.99\nEpoch 2000 | ELBO:   -32.42 | Log-lik:    12.57 | KL:  44.99\nEpoch 2500 | ELBO:   -25.91 | Log-lik:    16.31 | KL:  42.22\nEpoch 2500 | ELBO:   -25.91 | Log-lik:    16.31 | KL:  42.22\nEpoch 3000 | ELBO:   -27.61 | Log-lik:    14.09 | KL:  41.70\nTraining complete!\nEpoch 3000 | ELBO:   -27.61 | Log-lik:    14.09 | KL:  41.70\nTraining complete!\n\n\n\n\nShow Code\n# Visualize VI training progress\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\nepochs_arr = np.arange(1, len(history['elbo']) + 1)\n\n# Plot 1: ELBO over time\nax = axes[0]\nax.plot(epochs_arr, history['elbo'], linewidth=2, color='blue')\nax.set_xlabel('Epoch', fontsize=12)\nax.set_ylabel('ELBO', fontsize=12)\nax.set_title('ELBO During VI Training', fontsize=13, fontweight='bold')\nax.grid(True, alpha=0.3)\n\n# Plot 2: ELBO components\nax = axes[1]\nax.plot(epochs_arr, history['log_lik'], linewidth=2, label='Expected log-likelihood', color='green')\nax.plot(epochs_arr, history['kl'], linewidth=2, label='KL divergence', color='red')\nax.set_xlabel('Epoch', fontsize=12)\nax.set_ylabel('Value', fontsize=12)\nax.set_title('ELBO Components', fontsize=13, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Final ELBO breakdown:\")\nprint(f\"  ELBO: {history['elbo'][-1]:.2f}\")\nprint(f\"  Expected log-likelihood: {history['log_lik'][-1]:.2f}\")\nprint(f\"  KL divergence: {history['kl'][-1]:.2f}\")\n\n\n\n\n\n\n\n\n\nFinal ELBO breakdown:\n  ELBO: -27.61\n  Expected log-likelihood: 14.09\n  KL divergence: 41.70\n\n\n\n\nShow Code\n# Generate VI posterior predictive samples\nx_test_tensor = torch.FloatTensor(x_test)\nvi_predictions = []\n\nwith torch.no_grad():\n    for _ in range(100):\n        y_pred = vi_model(x_test_tensor, sample=True).numpy()\n        vi_predictions.append(y_pred)\n\nvi_predictions = np.array(vi_predictions)\nvi_mean = vi_predictions.mean(axis=0)\nvi_std = vi_predictions.std(axis=0)\n\n# Compare VI vs MCMC\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plot 1: VI Posterior Predictive\nax = axes[0]\n\n# VI samples\nfor i in range(len(vi_predictions)):\n    ax.plot(x_test, vi_predictions[i], 'blue', alpha=0.15, linewidth=1)\n\n# True function and data\nax.plot(x_test, y_test_true, 'k-', linewidth=3, label='True function', zorder=10)\nax.scatter(x_train, y_train, s=100, c='red', edgecolors='black', \n           linewidth=2, zorder=15, label='Training data')\n\n# VI mean\nax.plot(x_test, vi_mean, 'blue', linewidth=3, label='VI posterior mean', linestyle='--', zorder=5)\n\n# Dummy line for legend\nax.plot([], [], 'blue', alpha=0.6, linewidth=2, label='VI posterior samples')\n\nax.set_xlabel('Input $x$', fontsize=13)\nax.set_ylabel('Output $y$', fontsize=13)\nax.set_title('Variational Inference: Posterior Predictive', fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='upper right')\nax.grid(True, alpha=0.3)\nax.set_xlim([-3.2, 3.2])\nax.set_ylim([-1.5, 1.5])\n\n# Plot 2: Direct comparison VI vs MCMC\nax = axes[1]\n\n# MCMC samples (lighter)\nfor i in range(min(50, len(mcmc_predictions))):\n    ax.plot(x_test, mcmc_predictions[i], 'orange', alpha=0.1, linewidth=1)\n\n# VI samples (darker)\nfor i in range(50):\n    ax.plot(x_test, vi_predictions[i], 'blue', alpha=0.1, linewidth=1)\n\n# Means\nax.plot(x_test, mcmc_mean, 'orange', linewidth=3, label='MCMC mean', linestyle='--')\nax.plot(x_test, vi_mean, 'blue', linewidth=3, label='VI mean', linestyle='--')\n\n# True function and data\nax.plot(x_test, y_test_true, 'k-', linewidth=3, label='True function', zorder=10)\nax.scatter(x_train, y_train, s=100, c='red', edgecolors='black', \n           linewidth=2, zorder=15, label='Training data')\n\nax.set_xlabel('Input $x$', fontsize=13)\nax.set_ylabel('Output $y$', fontsize=13)\nax.set_title('Comparison: VI vs MCMC', fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='upper right')\nax.grid(True, alpha=0.3)\nax.set_xlim([-3.2, 3.2])\nax.set_ylim([-1.5, 1.5])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNote the difference in training times between the MCMC Method and VI Method. The main thing we give up for this increase in speed is the",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "part1/introduction_to_inference.html#possible-additional-experiments",
    "href": "part1/introduction_to_inference.html#possible-additional-experiments",
    "title": "7  Introduction to Inference",
    "section": "7.6 Possible Additional Experiments",
    "text": "7.6 Possible Additional Experiments\nNow that you understand the full pipeline, you can explore below how some of the fundamental parameters of the model might affect the results:\n\n\n\n\n\n\nTipExperiment 1: Vary Hidden Layer Size\n\n\n\nTrain VI models with hidden_size = 2, 5, 10, 20. Observe: - How does model flexibility change? - Does uncertainty increase or decrease? - Is there a risk of overfitting with larger networks?\n\n\n\n\n\n\n\n\nTipExperiment 2: Effect of Training Data Size\n\n\n\nGenerate datasets with n_train = 5, 10, 20, 50. For each: - Train the VI model - Plot posterior predictive distributions - Compare the model uncertainty in data-rich vs data-sparse regions What do you notice?\n\n\n\n\n\n\n\n\nTipExperiment 3: Prior Sensitivity\n\n\n\nTry different prior standard deviations: prior_std = 0.1, 1.0, 10.0. Observe: - How does this affect the learned posterior? - Does a strong prior (small std) regularize more? - What happens with a weak prior (large std)?\n\n\nWe can do Experiment 1 together as an example, and leave the remainder for you to do independently:\n\n\nShow Code\n# Experiment 1: Vary hidden layer size\nhidden_sizes_to_test = [2, 5, 10, 20]\nresults_by_size = {}\n\nprint(\"Experiment 1: Effect of Hidden Layer Size\")\nprint(\"Training VI models with different architectures...\")\n\nfor h_size in hidden_sizes_to_test:\n    print(f\"Training with hidden_size = {h_size}...\")\n    \n    # Initialize and train\n    model_exp = BayesianNNVI(hidden_size=h_size)\n    optimizer_exp = torch.optim.Adam(model_exp.parameters(), lr=0.01)\n    \n    for epoch in range(2000):\n        optimizer_exp.zero_grad()\n        elbo, _, _ = model_exp.elbo(x_train_tensor, y_train_tensor, \n                                     n_samples=5, noise_std=noise_std, prior_std=1.0)\n        loss = -elbo\n        loss.backward()\n        optimizer_exp.step()\n    \n    # Generate predictions\n    predictions = []\n    with torch.no_grad():\n        for _ in range(100):\n            y_pred = model_exp(x_test_tensor, sample=True).numpy()\n            predictions.append(y_pred)\n    \n    predictions = np.array(predictions)\n    results_by_size[h_size] = {\n        'predictions': predictions,\n        'mean': predictions.mean(axis=0),\n        'std': predictions.std(axis=0)\n    }\n    \n    print(f\"  Final ELBO: {elbo.item():.2f}\")\n\nprint(\"Training complete!\")\n\n\nExperiment 1: Effect of Hidden Layer Size\nTraining VI models with different architectures...\nTraining with hidden_size = 2...\n  Final ELBO: -21.87\nTraining with hidden_size = 5...\n  Final ELBO: -21.87\nTraining with hidden_size = 5...\n  Final ELBO: -30.33\nTraining with hidden_size = 10...\n  Final ELBO: -30.33\nTraining with hidden_size = 10...\n  Final ELBO: -45.41\nTraining with hidden_size = 20...\n  Final ELBO: -45.41\nTraining with hidden_size = 20...\n  Final ELBO: -82.38\nTraining complete!\n  Final ELBO: -82.38\nTraining complete!\n\n\n\n\nShow Code\n# Visualize results from Experiment 1\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\naxes = axes.flatten()\n\nfor idx, h_size in enumerate(hidden_sizes_to_test):\n    ax = axes[idx]\n    \n    result = results_by_size[h_size]\n    \n    # Plot posterior samples\n    for i in range(min(50, len(result['predictions']))):\n        ax.plot(x_test, result['predictions'][i], 'blue', alpha=0.1, linewidth=1)\n    \n    # Plot mean and uncertainty bands\n    ax.plot(x_test, result['mean'], 'blue', linewidth=3, label='VI posterior mean', linestyle='--')\n    ax.fill_between(x_test, \n                     result['mean'] - result['std'], \n                     result['mean'] + result['std'],\n                     alpha=0.3, color='blue', label='±1sigma')\n    \n    # True function and data\n    ax.plot(x_test, y_test_true, 'k-', linewidth=2, label='True function', zorder=10)\n    ax.scatter(x_train, y_train, s=80, c='red', edgecolors='black', \n               linewidth=1.5, zorder=15, label='Training data')\n    \n    ax.set_xlabel('Input $x$', fontsize=12)\n    ax.set_ylabel('Output $y$', fontsize=12)\n    ax.set_title(f'Hidden Size = {h_size} ({(h_size+1) + h_size} parameters)', \n                 fontsize=13, fontweight='bold')\n    ax.legend(fontsize=10, loc='upper right')\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim([-3.2, 3.2])\n    ax.set_ylim([-1.5, 1.5])\n\nplt.suptitle('Experiment 1: Effect of Network Capacity', fontsize=15, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "part1/introduction_to_inference.html#summary-and-the-bridge-to-vaes",
    "href": "part1/introduction_to_inference.html#summary-and-the-bridge-to-vaes",
    "title": "7  Introduction to Inference",
    "section": "7.7 Summary and the bridge to VAEs",
    "text": "7.7 Summary and the bridge to VAEs\nWe saw a few key takeaways in this notebook:\nBayesian Inference Quantifies Uncertainty - Instead of a single “best” model, we maintain a distribution over models - The posterior predictive distribution \\(p(y_* | x_*, X, Y)\\) gives us predictions with uncertainty\nLinear Models Have Limitations - Bayesian linear regression has exact, closed-form posteriors (Gaussian) - This allows us to compute predictive mean and variance analytically - However, they cannot capture nonlinear relationships, as we saw in this example\nNeural Networks Provide Flexibility at the cost of an intractable posterior - Even a small 1-hidden-layer network can approximate complex functions - Prior predictive shows diverse possible functions before seeing data - Challenge: Posterior \\(p(\\mathbf{w} | X, Y)\\) is no longer tractable.\nMCMC: Accurate but Slow - Markov Chain Monte Carlo samples from the true posterior - Pros: Asymptotically exact, unbiased - Cons: Computationally expensive, slow for high-dimensional models - Useful as a reference for validation\nVariational Inference: Fast Approximation - Approximate \\(p(\\mathbf{w} | X, Y)\\) with a tractable family \\(q_\\phi(\\mathbf{w})\\) - Optimize \\(\\phi\\) to maximize the ELBO: \\[\\mathcal{L}(\\phi) = \\mathbb{E}_{q_\\phi}\\left[\\log p(Y | X, \\mathbf{w})\\right] - \\text{KL}(q_\\phi(\\mathbf{w}) || p(\\mathbf{w}))\\] - Reparameterization trick enables low-variance gradient estimates - Result: Posterior approximation in seconds instead of minutes\nFunction Space vs Weight Space - We don’t visualize high-dimensional weight distributions directly - Instead, we visualize functions sampled from the posterior - Posterior predictive is what matters for making predictions!\n\n7.7.1 Looking forward to VAEs (Variational Autoencoders)\nThe VI approach you learned here is the same mathematical foundation for VAEs. The key differences are:\n\nHere (Bayesian NN): Posterior over weights \\(p(\\mathbf{w} | X, Y)\\)\nVAEs: Posterior over latent variables \\(p(\\mathbf{z} | \\mathbf{x})\\)\n\nThe ELBO, reparameterization trick, and optimization strategy are identical.\nMoreover, in VAEs, instead of optimizing \\(q_\\phi(z)\\) separately for each datapoint \\(x\\), we learn an encoder network \\(q_\\phi(z | x)\\) that works for all \\(x\\). This is called amortized inference.\nQuestions to Reflect on\nThink about these before moving to the next notebook:\n\nWhy does VI underestimate uncertainty?\nHint: Is the Mean-field approximation where we assume independence between weights reasonable? When might it not be?\nWhen would MCMC be essential despite being slow?\nCould we use a more expressive variational family?\nWe used a Mean-field Gaussian here, but are we limited to this?",
    "crumbs": [
      "Foundational Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "part2/part2.html",
    "href": "part2/part2.html",
    "title": "Model-Specific Approaches",
    "section": "",
    "text": "These chapters look more like a “traditional” textbook, in the sense that they cover individual techniques or concepts that I thought were maximally relevant to Mechanical Engineers at the time that I wrote the book. These chapters will be most immediately useful in terms of getting up to speed with specific types of models, but they are also the most likely to become out of date quickly as newer/better models are invented.\n\nReview of Prior Course Models – CNNs, UNets, RNNs, AEs\nAdvanced Neural Models\n\nThe Attention Mechanism\nIntroduction to Transformers\nRegularization of Neural Networks\nIntroduction to Geometric Deep Learning (Message Passing, GNNs, Working with Meshes)\nFailure Mechanisms in Neural Models\n\nProbabilistic Models and Kernels\n\nIntroduction to Probabilistic Graphical Models\nExact Inference (MLE, MAP, EM)\nApproximate Inference (MCMC, VI)\nIntroduction to Probabilistic Programming\nFailure Mechanisms in Probabilistic Models\n\nEnsembles (Not included in current course for scope reasons)\n\nBagging\nBoosting",
    "crumbs": [
      "Model-Specific Approaches"
    ]
  },
  {
    "objectID": "part2/review_neural_networks.html",
    "href": "part2/review_neural_networks.html",
    "title": "8  Review of Neural Networks",
    "section": "",
    "text": "8.1 Example of PyTorch SGD for Linear Regression and a Simple Feed-Forward Network\nThis notebook reviews some basic notation and concepts related to neural networks. It will skip many details that would have been covered in your earlier Stochastics and Machine Learning course, but will set a basis for us jumping into more advanced topics in a few chapters.\nBefore we get started with PyTorch code, I encourage you first to interactive with the ConvNetJS demo, specifically the interactive 1D Regression where you can toogle on and off the different layers and plotting functions, as well as the 2D Classification demo to see how a neural network is just performing a series of feature transformations that ultimately lead to a linear classification boundary. These demos help build a good intuition for what is going on before we move onto more obtuse PyTorch code.\nShow Code\nn_samples = 30\n\n# True Function we want to estimate\ndef true_func(X): return np.cos(1.5 * np.pi * X)\n\n# Noisy Samples from the true function\nX = np.sort(2*np.random.rand(n_samples)-1)\ny = true_func(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(10,10))\n# Plot the true function:\nX_plot = np.linspace(-1.5, 1.5, 100)\nplt.plot(X_plot, true_func(X_plot), '--',label=\"True function\")\n# Plot the data samples\nplt.scatter(X,y, label=\"Samples\")\nplt.legend(loc=\"best\")\nplt.show()\nShow Code\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.4, random_state=0)\nplt.figure(figsize=(7,7))\n# Plot the data samples\nplt.scatter(X_train,y_train, label=\"Train\", c='Blue', s=20, edgecolors='none')\nplt.scatter(X_test,y_test, label=\"Test\", c='Red', s=50, edgecolors='none')\n#plt.plot(X_plot, true_func(X_plot), 'g--',label=\"True function\")\nplt.legend(loc=\"best\")\nsns.despine()\nplt.show()\nShow Code\n# Convert the data into a shape and data-type that PyTorch likes\nX_train = X_train.reshape(-1,1).astype(np.float32)\ny_train = y_train.reshape(-1,1).astype(np.float32)",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Review of Neural Networks</span>"
    ]
  },
  {
    "objectID": "part2/review_neural_networks.html#example-of-pytorch-sgd-for-linear-regression-and-a-simple-feed-forward-network",
    "href": "part2/review_neural_networks.html#example-of-pytorch-sgd-for-linear-regression-and-a-simple-feed-forward-network",
    "title": "8  Review of Neural Networks",
    "section": "",
    "text": "8.1.1 Linear Regression\n\ninput_size  = 1\noutput_size = 1\n# Linear regression model\nmodel = nn.Linear(input_size, output_size)\n\n# Loss and optimizer\ncriterion = nn.MSELoss()\nlearning_rate = 0.1 # alpha\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n\n# Train the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Convert numpy arrays to torch tensors\n    inputs = torch.from_numpy(X_train)\n    targets = torch.from_numpy(y_train)\n\n    # Forward pass\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1) % 20 == 0:\n        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n\nEpoch [20/100], Loss: 0.3992\nEpoch [40/100], Loss: 0.3596\nEpoch [60/100], Loss: 0.3553\nEpoch [80/100], Loss: 0.3548\nEpoch [100/100], Loss: 0.3548\n\n\n\n\nShow Code\n# Plot the graph\nplt.figure()\nplt.plot(X_train, y_train, 'ro', label='Data')\n#predicted = model(torch.from_numpy(X_train)).detach().numpy()\n#plt.plot(X_train, predicted, 'b+',label='Predictions')\npredicted = model(torch.from_numpy(X_plot.reshape(-1,1).astype(np.float32))).detach().numpy()\nplt.plot(X_plot, predicted, 'b', label='Prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n8.1.2 Neural Network Examples\n\n8.1.2.1 Example of Defining a Network via the full Module class\n\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(Net, self).__init__()  \n        # Fully-Connected Layer: 1 (input data) -&gt; 5 (hidden node)\n        self.fc1 = nn.Linear(input_size, hidden_size)  \n        \n        # Non-Linear Layer\n        self.sigmoid = nn.Sigmoid()\n        # You can try other kinds as well\n        # self.relu = nn.ReLU()\n        # self.elu = nn.ELU()\n        \n        \n        # Fully-Connected Layer: 5 (hidden node) -&gt; 1 (output)\n        self.fc2 = nn.Linear(hidden_size, 1) \n    \n    # Forward pass builds the model prediction from the inputs\n    def forward(self, x):                              \n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n        return out\n    \n# Build the network -- is it not trained yet\nmodel = Net(input_size=1, hidden_size=5)\n\n\n\n8.1.2.2 Example of building a model using the Sequential helper function\n\n\nShow Code\ninput_size=1\nhidden_size=4\nmodel = nn.Sequential(\n          nn.Linear(input_size, hidden_size),\n          nn.Sigmoid(),\n          nn.Linear(hidden_size, hidden_size),\n          nn.ReLU(),\n          nn.Linear(hidden_size, 1)\n        )\n\n\n\n\n8.1.2.3 Example using Python list expansions to help build deeper networks\n\n\nShow Code\ninput_size=1\nhidden_size=7\nnum_hidden_layers = 3\nactivation = nn.ReLU\n\ninput_layer = [nn.Linear(input_size, hidden_size),\n                activation()]\nhidden_layers = num_hidden_layers*[nn.Linear(hidden_size, hidden_size), \n                                   activation()]\noutput_layer = [ nn.Linear(hidden_size, 1) ] \n\n# Stack them all together\nlayers = input_layer + hidden_layers + output_layer\n\nprint(layers)\n\n# Use the * operator to \"expand\" or \"unpack\" the list\nmodel = nn.Sequential(*layers)\n\n\n[Linear(in_features=1, out_features=7, bias=True), ReLU(), Linear(in_features=7, out_features=7, bias=True), ReLU(), Linear(in_features=7, out_features=7, bias=True), ReLU(), Linear(in_features=7, out_features=7, bias=True), ReLU(), Linear(in_features=7, out_features=1, bias=True)]\n\n\n\n\n\n8.1.3 Now let’s do the actual training\n\n# What Loss function should we use? MSE!\ncriterion = nn.MSELoss()\n\n# What Optimization procedure should we use?\n\n##### Change these and let's see how it affects the model fit #################\nlearning_rate = 0.05\nweight_decay = 0.0\n##########################\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n\n\n### Train the model\n\n# Convert numpy arrays to torch tensors\ninputs = torch.from_numpy(X_train)\ntargets = torch.from_numpy(y_train)\n\nnum_epochs = 5000\nfor epoch in range(num_epochs):\n\n    ## Do Forward pass\n    # Make predictions\n    outputs = model(inputs)\n    # Compute the loss function\n    loss = criterion(outputs, targets)\n    \n    ## Update the model\n    # Reset the optimizer gradients\n    optimizer.zero_grad()\n    # Compute the gradient of the loss function\n    loss.backward()\n    # Do an optimization step\n    optimizer.step()\n    \n    # Print the loss\n    if (epoch+1) % 200 == 0:\n        print ('Epoch [{:4}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n\nEpoch [ 200/5000], Loss: 0.0064\nEpoch [ 400/5000], Loss: 0.0025\nEpoch [ 600/5000], Loss: 0.0029\nEpoch [ 800/5000], Loss: 0.0023\nEpoch [1000/5000], Loss: 0.0023\nEpoch [1200/5000], Loss: 0.0023\nEpoch [1400/5000], Loss: 0.0023\nEpoch [1600/5000], Loss: 0.0023\nEpoch [1800/5000], Loss: 0.0022\nEpoch [2000/5000], Loss: 0.0023\nEpoch [2200/5000], Loss: 0.0022\nEpoch [2400/5000], Loss: 0.0033\nEpoch [2600/5000], Loss: 0.0023\nEpoch [2800/5000], Loss: 0.0209\nEpoch [3000/5000], Loss: 0.0218\nEpoch [3200/5000], Loss: 0.0023\nEpoch [3400/5000], Loss: 0.0022\nEpoch [3600/5000], Loss: 0.0020\nEpoch [3800/5000], Loss: 0.0070\nEpoch [4000/5000], Loss: 0.0020\nEpoch [4200/5000], Loss: 0.0019\nEpoch [4400/5000], Loss: 0.0023\nEpoch [4600/5000], Loss: 0.0019\nEpoch [4800/5000], Loss: 0.0030\nEpoch [5000/5000], Loss: 0.0118\n\n\nNow let’s plot the prediction:\n\n\nShow Code\n# Plot the graph\nplt.figure()\nplt.plot(X_train, y_train, 'ro', label='Data')\npredicted = model(torch.from_numpy(X_plot.reshape(-1,1).astype(np.float32))).detach().numpy()\nplt.plot(X_plot, predicted, 'b', label='Prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect of Regularization on Simple Toy Regression Problem\n\n\n\nRevisit the above regression code and experiment with the weight decay parameter in the Adam optimizer. How does this affect the learned function? Why do you think this is?:\n\n\nYou can also play around with different activation functions, e.g., nn.ReLU(), nn.Sigmoid(), nn.Tanh(), etc., and the below plot pulls out several options from PyTorch for you to visualize the activation functions:\n\n\nShow Code\n# Common activation functions from PyTorch:\nactivations = {\n    'Identity': nn.Identity(),\n    'ReLU': nn.ReLU(),\n    'Sigmoid': nn.Sigmoid(),\n    'Tanh': nn.Tanh(),\n    'LeakyReLU': nn.LeakyReLU(),\n    'ELU': nn.ELU(),\n    'GELU': nn.GELU(),\n    'SiLU': nn.SiLU(),  # also known as Swish\n    'Softplus': nn.Softplus(),\n    'Softsign': nn.Softsign(),\n    'Hardtanh': nn.Hardtanh(),\n    'PReLU': nn.PReLU(),\n    'CELU': nn.CELU(),\n    'SELU': nn.SELU(),\n    'Mish': nn.Mish()\n}\nx = torch.linspace(-3, 3, 100)\nn = len(activations)\nncols = 4\nnrows = int(np.ceil(n/ncols))\nplt.figure(figsize=(15,10))\nfor i, (name, activation) in enumerate(activations.items()):\n    plt.subplot(nrows, ncols, i+1)\n    plt.plot(x.numpy(), activation(x).detach().numpy())\n    plt.ylim([-1.5, 3])\n    plt.title(name)\n    plt.grid()\nplt.tight_layout()\nplt.suptitle(\"Common Neural Network Activation Functions\", y=1.02, fontsize=16)\nplt.show()",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Review of Neural Networks</span>"
    ]
  },
  {
    "objectID": "part2/review_neural_networks.html#unsupervised-learning-using-autoencoders",
    "href": "part2/review_neural_networks.html#unsupervised-learning-using-autoencoders",
    "title": "8  Review of Neural Networks",
    "section": "8.2 Unsupervised Learning using Autoencoders",
    "text": "8.2 Unsupervised Learning using Autoencoders\nFor this demonstration, we will construct what is fundamentally a 1D function (t) but then embed it in a higher-dimensional space (3D) using a non-linear transformation. This will allow us to compare what a linear method (PCA) can do versus a non-linear method (Autoencoder), on a simple example.\n\n# First we create a simple line (t)\nt = np.linspace(-1,1,100)\n\n\n# Now let's make it 3D and add some (optional) noise\nnoise_level = 0.01\n# You can try out different functions below by uncommenting/commenting them\n#X = np.vstack([t,1*t**2,-1*t**3, 0.4*t**5,t**2-t**3,-0.4*t**4]).T\n#X = np.vstack([2*np.sin(3*t),1*t**2,-1*t**3, 2*np.sin(6*t+1)-t**3,2*np.cos(3*t)]).T\nX = np.vstack([np.sin(2*t),1*t**2,1*np.cos(5*t)]).T + noise_level*np.random.randn(len(t),3)\n\n\n\nShow Code\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(X[:,0], X[:,1], X[:,2])\nax.set_xlabel('$X_0$')\nax.set_ylabel('$X_1$')\nax.set_zlabel('$X_2$')\nax.elev=45\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that it results in 100 points each of which has three dimensions:\n\nX.shape\n\n(100, 3)\n\n\nWe can attempt to reduce the dimensionality of this data using PCA, but as we can already see from the 3D plot, the data is not linearly embedded in 3D space, so PCA will not be able to find a good low-dimensional representation:\n\npca = PCA(3)\nZ_PCA =pca.fit_transform(X)\nplt.figure()\nplt.scatter(Z_PCA[:,0],Z_PCA[:,1],s=15)\nplt.xlabel('PCA Dim 1')\nplt.ylabel('PCA Dim 2')\nplt.title('PCA Projection on the first two principal components')\nplt.show()\n\n\n\n\n\n\n\n\nAnd we can also see this reflected in the explained variance, which shows that we need all three original dimensions to explain the variance in the data, even though we know that the data fundamentally lies on a 1D manifold:\n\n\nShow Code\nplt.figure()\nplt.plot(pca.explained_variance_)\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance')\nplt.title('PCA Explained Variance')\nplt.xticks([0,1,2])\nplt.ylim(0,1.1*max(pca.explained_variance_))\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see below what happens if we try to truncate PCA to only two dimensions and then reconstruct back to 3D space. The reconstruction is not very good, as expected, and what do you notice about the shape of the reconstructed data?\n\n\nShow Code\n\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\npca_2d = PCA(2)\nZ_PCA = pca_2d.fit_transform(X)\nX_PCA= pca_2d.inverse_transform(Z_PCA)\nax.scatter(X[:,0], X[:,1], X[:,2],alpha=0.5)\nax.scatter(X_PCA[:,0], X_PCA[:,1], X_PCA[:,2])\n\nax.set_xlabel('$X_0$')\nax.set_ylabel('$X_1$')\nax.set_zlabel('$X_2$')\nax.elev=35\nax.azim=10\nplt.title(\"PCA Reconstruction with only two components\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNow let’s see how this works using an autoencoder. We will use a very simple architecture with just one hidden layer in the encoder and one hidden layer in the decoder, and we will use 1D latent space (since we know that the data is fundamentally 1D). You will have the option in the below code to change the number of hidden units in the encoder and decoder, as well as the number of latent dimensions, and see how this affects the results.\n\nclass Encoder(nn.Module):\n    def __init__(self, num_input, num_latent,num_hidden):\n        super().__init__()\n        self.num_input  = num_input\n        self.num_latent = num_latent\n        self.num_hidden = num_hidden\n        \n        # I encourage you to modify the architecture here by adding more layers or changing activation functions, if you wish\n        self.encode = nn.Sequential(\n            nn.Linear(self.num_input, self.num_hidden),\n            nn.ReLU(),\n            #nn.Linear(self.num_hidden, self.num_hidden),\n            #nn.ReLU(),\n            nn.Linear(self.num_hidden, self.num_latent),\n        )\n        \n    def forward(self, X):\n        encoded = self.encode(X)\n        return encoded\n    \nclass Decoder(nn.Module):\n    def __init__(self, num_input, num_latent,num_hidden):\n        super().__init__()\n        self.num_input  = num_input\n        self.num_latent = num_latent\n        self.num_hidden = num_hidden\n        \n        self.decode = nn.Sequential(\n            nn.Linear(self.num_latent, self.num_hidden),\n            nn.ReLU(),\n            #nn.Linear(self.num_hidden, self.num_hidden),\n            #nn.ReLU(),\n            nn.Linear(self.num_hidden, self.num_input)\n        )\n        \n    def forward(self, Z):\n        decoded = self.decode(Z)\n        return decoded\n    \nclass AutoEncoder(nn.Module):\n    def __init__(self, num_input,num_latent,num_hidden):\n        super().__init__()\n        self.num_input  = num_input\n        self.num_latent = num_latent\n        self.num_hidden = num_hidden\n        \n        self.encoder = Encoder(num_input  = self.num_input,\n                               num_latent = self.num_latent,\n                               num_hidden = self.num_hidden)\n        self.decoder = Decoder(num_input  = self.num_input,\n                               num_latent = self.num_latent,\n                               num_hidden = self.num_hidden)\n        \n    def forward(self, X):\n        encoded = self.encoder(X)\n        decoded = self.decoder(encoded)\n        return decoded, encoded  # &lt;- return a tuple of two values\n    \n    def transform(self,X):\n        '''Take X and encode to latent space'''\n        return self.encoder(X)\n    \n    def inverse_transform(self,Z):\n        '''Take Z and decode to X space'''\n        return self.decoder(Z)\n\n\n# Here we can set some parameters for the autoencoder that we are about to train\n# What happens if you change them?\n# e.g., increase/decrease num_latent, num_hidden, learning rate, weight decay\nnum_points, D_orig = X.shape\nnum_latent = 3\nnum_hidden = 5\nmodel = AutoEncoder(D_orig,num_latent,num_hidden)\n# Create the optimizer object:\n# Adam optimizer with learning rate and weight decay\n# I encourage you to try out different learning rates and weight decays and\n# observe their effect on the model\noptimizer = optim.AdamW(model.parameters(), \n                        lr=1e-3, \n                        weight_decay=1e-2)\n# Add a mean-squared error loss\ncriterion = nn.MSELoss()\n\n\nX_torch = torch.from_numpy(X)\nX_torch = X_torch.float()\n\n# Depending on the model architecture you use, you may need to increase or decrease this to get good training\nepochs=10000\nfor epoch in range(epochs):\n    loss = 0\n\n    optimizer.zero_grad()\n\n    # compute reconstructions\n    decoded, encoded = model(X_torch)\n\n    # compute training reconstruction loss\n    train_loss = criterion(decoded, X_torch)\n            \n    # Total Loss\n    loss = train_loss \n    \n    # compute accumulated gradients\n    loss.backward()\n\n    # perform parameter update based on current gradients\n    optimizer.step()\n\n    # display the epoch training loss\n    if epoch%500==0:\n        print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss.item()))\n\nepoch : 1/10000, loss = 0.716544\nepoch : 501/10000, loss = 0.350305\nepoch : 1001/10000, loss = 0.090813\nepoch : 1501/10000, loss = 0.010054\nepoch : 2001/10000, loss = 0.000795\nepoch : 2501/10000, loss = 0.000162\nepoch : 3001/10000, loss = 0.000120\nepoch : 3501/10000, loss = 0.000097\nepoch : 4001/10000, loss = 0.000081\nepoch : 4501/10000, loss = 0.000073\nepoch : 5001/10000, loss = 0.000062\nepoch : 5501/10000, loss = 0.000054\nepoch : 6001/10000, loss = 0.000035\nepoch : 6501/10000, loss = 0.000016\nepoch : 7001/10000, loss = 0.000006\nepoch : 7501/10000, loss = 0.000000\nepoch : 8001/10000, loss = 0.000000\nepoch : 8501/10000, loss = 0.000000\nepoch : 9001/10000, loss = 0.000000\nepoch : 9501/10000, loss = 0.000000\n\n\nNow that the model is trained, we can put the data through the encoder and decoder to pull out both the encoded (i.e., latent) representation of each point, as well as the decoded (i.e., reconstructed) version of each point, and visualize the results.\n\ndecoded, encoded = model(X_torch)\n\nFirst, let’s take a look at the reconstructed data in the original 3D space, compared to the original data:\n\n\nShow Code\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(projection='3d')\nX_P = decoded.detach().numpy()\n\nax.scatter(X[:,0], X[:,1], X[:,2],alpha=0.5)\nax.scatter(X_P[:,0], X_P[:,1], X_P[:,2])\nax.set_xlabel('$X_0$')\nax.set_ylabel('$X_1$')\nax.set_zlabel('$X_2$')\nax.elev=25\nax.azim=10\nplt.legend(['Original Data','Reconstructed Data'])\nplt.title(\"AE Reconstruction\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s see how the encoded (i.e., latent) points actually look in Z space:\n\n\nShow Code\nZ = encoded.cpu().detach().numpy()\n# Only works if num_latent &gt;=2\nif num_latent&gt;=2:\n    plt.figure()\n    plt.scatter(Z[:,0],Z[:,1])\n    plt.xlabel('z1')\n    plt.ylabel('z2')\n    plt.title('Latent Space Representation of the autoencoder')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nplt.figure()\nplt.plot(Z[:,0],marker='+')\nplt.xlabel('index')\nplt.ylabel('$z_0$')\nplt.title('Latent Dimension 0 values for each data point')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also look at a pairplot of the points in the latent space to get a sense of how the different dimensions correlate with each other:\n\n\nShow Code\ndf = pd.DataFrame(Z, columns=[f'z{i}' for i in range(Z.shape[1])])\nplt.figure()\nsns.pairplot(df)\nplt.suptitle('Latent Space Pairplot', y=1.02)\nplt.show()\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect on Autoencoder Architecture on Reconstruction Accuracy and Latent Space Behavior\n\n\n\nRevisit the above autoencoder code and experiment by changing the number of hidden units, the bottleneck (latent) dimension, and the number of layers in the encoder and decoder. How do these changes affect the reconstruction accuracy and the behavior of the learned latent space? Consider some of the below questions:\n\nWe know that the data is fundamentally 1D, but why does setting the autoencoder latent dimension to 1 not work well? Why might it be useful to have a latent dimension larger than the true dimensionality of the data for this type of model?\nWhat happens if you set the latent dimension to be three or larger? Why do you think this happens? What have we given up by doing this?\nAs you increase or decrease the number of hidden units in Autoencoder, how does this affect the reconstruction accuracy? Why do you think this is? Consider in particular the case where the number of hidden units is one or two.\nUnlike in PCA, when we re-run the autoencoder training, we get different results each time. Why do you think this is?\nUnlike PCA, the autoencoder does not guarantee that the latent dimensions are orthogonal or ordered by importance (e.g., \\(z_0\\) being more important than \\(z_1\\), etc.). Do you see any evidence of this in the learned latent space? Why do you think this is?\nThe Autoencoder used a ReLU activation function in the hidden layers. How does this manifest in the way that the network reconstructs the data? (Note, this may only be visible if you set the latent dimension to 1 and the number of hidden units to a small number, e.g., 2 or 3).\nWe can see that when we set the latent dimension to two or greater, the autoencoder can reconstruct the data well, but it is not capturing the intrinsic dimensionality of the data. One way we might do this is by adding L1 regularization to the latent space coordinates, which is commented out in the code above. Try adding in this regularization. Does it fix the problem?",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Review of Neural Networks</span>"
    ]
  },
  {
    "objectID": "part2/review_neural_networks.html#least-volume-regularization",
    "href": "part2/review_neural_networks.html#least-volume-regularization",
    "title": "8  Review of Neural Networks",
    "section": "8.3 Least Volume Regularization",
    "text": "8.3 Least Volume Regularization\nOne of the problems that we saw in Autoencoders is that the latent space does not have any particular structure, and in particular, it is not guaranteed to be ordered, in the same sense as PCA. It was also difficult for us to determine the exact “size” of the latent dimension, since, as we saw, setting the latent dimension to 1 did not work well, even though we knew that the data was fundamentally 1D, due to training variability and the fact that the autoencoder could get trapped in local minima. One possible solution to this is to add a regularization term that encourages the latent space to be small in some sense. One such regularization is called Least Volume Regularization, which encourages the latent space to have a small volume by penalizing the volume of the encoded points in the latent space.\n\n\nShow Code\nclass _Combo(nn.Module):\n    def forward(self, input):\n        return self.model(input)\n\nclass LinearCombo(_Combo):\n    def __init__(self, in_features, out_features, activation=nn.LeakyReLU(0.2)):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(in_features, out_features),\n            activation\n        )\n\nclass MLP(nn.Module):\n    \"\"\"Regular fully connected network generating features.\n\n    Args:\n        in_features: The number of input features.\n        out_feature: The number of output features.\n        layer_width: The widths of the hidden layers.\n        combo: The layer combination to be stacked up.\n\n    Shape:\n        - Input: `(N, H_in)` where H_in = in_features.\n        - Output: `(N, H_out)` where H_out = out_features.\n    \"\"\"\n    def __init__(\n        self, in_features: int, out_features:int, layer_width: list,\n        combo = LinearCombo\n        ):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.layer_width = list(layer_width)\n        self.model = self._build_model(combo)\n\n    def forward(self, input):\n        return self.model(input)\n\n    def _build_model(self, combo):\n        model = nn.Sequential()\n        idx = -1\n        for idx, (in_ftr, out_ftr) in enumerate(self.layer_sizes[:-1]):\n            model.add_module(str(idx), combo(in_ftr, out_ftr))\n        model.add_module(str(idx+1), nn.Linear(*self.layer_sizes[-1])) # type:ignore\n        return model\n\n    @property\n    def layer_sizes(self):\n        return list(zip([self.in_features] + self.layer_width,\n        self.layer_width + [self.out_features]))\n\n\n\nambient_dim = X.shape[1]\n\n# Change the below latent dimension to see what happens to the embedded points\nlatent_dim = 3\n\nwidth = ambient_dim * 16\nencoder = MLP(ambient_dim, latent_dim, [width] * 4)\ndecoder = MLP(latent_dim, ambient_dim, [width] * 4)\n\nopt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n\n\n# Set X to a torch tensor:\nX_torch = torch.from_numpy(X).float()\n\nfor i in range(5000):\n    opt.zero_grad()\n    z = encoder(X_torch)\n    rec_loss = F.mse_loss(decoder(z), X_torch)\n    loss = rec_loss\n    # If you want, you can even add an L1 penalty on the latent space\n    # to try to encourage sparsity by uncommenting the line below:\n    #loss += 1e-3 * torch.mean(torch.abs(z))\n\n    loss.backward()\n    opt.step()\n    if (i+1) % 100 == 0:\n        print(f'Epoch {i:4}: rec = {rec_loss:.5g}')\n\nEpoch   99: rec = 0.39166\nEpoch  199: rec = 0.31308\nEpoch  299: rec = 0.048501\nEpoch  399: rec = 0.023062\nEpoch  499: rec = 0.015194\nEpoch  599: rec = 0.010049\nEpoch  699: rec = 0.0068286\nEpoch  799: rec = 0.0048156\nEpoch  899: rec = 0.0035384\nEpoch  999: rec = 0.0026096\nEpoch 1099: rec = 0.0018915\nEpoch 1199: rec = 0.001313\nEpoch 1299: rec = 0.00084967\nEpoch 1399: rec = 0.00053819\nEpoch 1499: rec = 0.00027124\nEpoch 1599: rec = 0.00014737\nEpoch 1699: rec = 0.00010135\nEpoch 1799: rec = 8.2536e-05\nEpoch 1899: rec = 7.4167e-05\nEpoch 1999: rec = 6.9067e-05\nEpoch 2099: rec = 6.5697e-05\nEpoch 2199: rec = 6.2941e-05\nEpoch 2299: rec = 6.0584e-05\nEpoch 2399: rec = 5.8694e-05\nEpoch 2499: rec = 5.6992e-05\nEpoch 2599: rec = 5.5272e-05\nEpoch 2699: rec = 5.3765e-05\nEpoch 2799: rec = 5.2397e-05\nEpoch 2899: rec = 5.1138e-05\nEpoch 2999: rec = 5.0006e-05\nEpoch 3099: rec = 4.8935e-05\nEpoch 3199: rec = 4.7868e-05\nEpoch 3299: rec = 4.6667e-05\nEpoch 3399: rec = 4.5643e-05\nEpoch 3499: rec = 4.4691e-05\nEpoch 3599: rec = 4.3831e-05\nEpoch 3699: rec = 4.3016e-05\nEpoch 3799: rec = 4.2142e-05\nEpoch 3899: rec = 4.1268e-05\nEpoch 3999: rec = 4.0287e-05\nEpoch 4099: rec = 3.9385e-05\nEpoch 4199: rec = 3.8568e-05\nEpoch 4299: rec = 3.7773e-05\nEpoch 4399: rec = 3.7031e-05\nEpoch 4499: rec = 3.6334e-05\nEpoch 4599: rec = 3.5686e-05\nEpoch 4699: rec = 3.5056e-05\nEpoch 4799: rec = 3.4465e-05\nEpoch 4899: rec = 3.388e-05\nEpoch 4999: rec = 3.332e-05\n\n\nNow that the autoencoder has been trained, let’s take a look at the standard deviation of the embedded points (i.e., in \\(z\\)). We can sort the latent dimensions according to which dimensions have the highest standard deviation in Z.\n\n\nShow Code\nencoder.eval()\ndecoder.eval()\n\n# Embed the data into Z using the trained encoder\nwith torch.no_grad():\n    z = encoder(X_torch)\n# Now let's sort the latent codes by which ones have the\n# largest standard deviation in Z:\nidx = z.std(0).argsort(descending=True)\n\nplt.figure()\nplt.bar(np.arange(z.std(0).size(-1)), z.std(0)[idx])\nplt.title('latent STDs (autoencoder)')\nplt.show()\n\n\n\n\n\n\n\n\n\nAs with before, we can plot some of the data points in Z space:\n\n\nShow Code\nplt.figure()\nplt.scatter(z[:, idx[0]].cpu().detach().numpy(), z[:, idx[1]].cpu().detach().numpy(), s=10)\nplt.gca().set_aspect('equal')\nplt.xlabel('$z_0$')\nplt.ylabel('$z_1$')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also plot the covariance among the latent codes of the embedded data:\n\n\nShow Code\ncov = z.T[idx].cov().detach().cpu().numpy()\nplt.figure()\nplt.matshow(cov, cmap='Reds')\nfor (i, j), var in np.ndenumerate(cov):\n    plt.gca().text(j, i, '{:.3e}'.format(var), ha='center', va='center')\nplt.title('Latent Covariance Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also get a general idea about how well we are reconstructing the original data by comparing the ground truth values versus predicted (i.e., encoded then decoded) data points – this is often called a parity plot:\n\n\nShow Code\nX_ = decoder(z).detach()\n\nfor i in range(X_torch.size(-1)):\n    plt.figure(figsize=(3,3))\n    plt.scatter(X[:, i], X_[:, i], s=3)\n    plt.gca().set_aspect('equal')\n    plt.xlabel('groundtruth')\n    plt.ylabel('reconstruction')\n    plt.title('$x_{}$'.format(i))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLastly, we can visualize a pairplot of the latent space to see how the different dimensions correlate with each other:\n\n\nShow Code\ndf = pd.DataFrame(z.cpu().detach().numpy(), columns=[f'z{i}' for i in range(z.shape[1])])\nplt.figure()\nsns.pairplot(df)\nplt.suptitle('Latent Space Pairplot', y=1.02)\nplt.show()\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Review of Neural Networks</span>"
    ]
  },
  {
    "objectID": "part2/review_neural_networks.html#how-might-we-select-the-right-order-in-an-autoencoder-building-the-case-for-least-volume-analysis-lva",
    "href": "part2/review_neural_networks.html#how-might-we-select-the-right-order-in-an-autoencoder-building-the-case-for-least-volume-analysis-lva",
    "title": "8  Review of Neural Networks",
    "section": "8.4 How might we select the right order in an Autoencoder? Building the case for Least Volume Analysis (LVA)",
    "text": "8.4 How might we select the right order in an Autoencoder? Building the case for Least Volume Analysis (LVA)\nOur explorations above exposed both the advantages and disadvantages of using non-linear maps to attempt to embed and capture the underlying distribution and topology of the data.\nBelow we describe the basic principle of “Least Volume” regularization in Autoencoders and demonstrate how it can be useful in providing automated order selection in over-parameterized Autoencoders. It can allow us to capture relevant topological structure, but with minimal dimension.\n\nImage above from: Qiuyi Chen and Mark Fuge, “Least Volume Analysis”\nIn general, the idea behind least volume is that we want to encourage the latent space to take up as little volume as possible, while still being able to reconstruct the data well. This can be achieved by adding a regularization term to the loss function that penalizes the volume of the latent space. A simple way to do this is to penalize the geometric mean of the standard deviation of the latent dimensions, which encourages the latent space to be small in all dimensions. Specifically, we can minimize the product of all elements of the latent code’s standard deviation vector \\(\\prod \\sigma\\), which is equivalent to minimizing the exponential of the mean of the log of the standard deviation vector:\nvol_loss = torch.exp(torch.log(z.std(0) + η).mean())\nWe add a small constant η to avoid numerical issues when any one of the standard deviation’s in any dimension approaches zero – that is when the autoencoder eliminates a dimension, and thus \\(\\prod \\sigma\\) would have a zero in the product. This loss term can be added to the reconstruction loss, weighted by a hyperparameter λ, to form the total loss.\nIn principle, while this loss can encourage the latent space to reduce its volume, there is one catch: the autoencoder could simply scale up the weights in the encoder and decoder to make the latent space arbitrarily small, while still being able to reconstruct the data well. To prevent this, we have to prevent the decoder from being able to arbitrarily increase its weights, and one easy way to enforce this is through spectral normalization on the weights of the decoder, which constrains the Lipschitz constant of the decoder to be at most 1. By preventing the decoder from scaling up its weights too much, and the encoder cannot easily defeat the volume penalty by isotropically shrinking the weights, and thus the only way to achieve a good volume penalty is to actually reduce dimensions.\nFun Fact: It turns out that in the case of an Autoencoder that only uses Linear layers, and with no activation functions, the least volume penalty is equivalent to PCA. That is, PCA can be seen as a special case of Least Volume Autoencoder. For more details on the mathematical proof, see Proposition 15 in the original paper.\nBelow code implements the spectral normalized decoder:\n\nfrom torch.nn.utils.parametrizations import spectral_norm\n\nclass SNLinearCombo(_Combo):\n    def __init__(self, in_features, out_features, activation=nn.LeakyReLU(0.2)):\n        super().__init__()\n        self.model = nn.Sequential(\n            spectral_norm(nn.Linear(in_features, out_features)),\n            activation\n        )\n\nclass SNMLP(MLP):\n    def __init__(\n        self, in_features: int, out_features: int, layer_width: list,\n        combo=SNLinearCombo):\n        super().__init__(in_features, out_features, layer_width, combo)\n\n    def _build_model(self, combo):\n        model = nn.Sequential()\n        idx = -1\n        for idx, (in_ftr, out_ftr) in enumerate(self.layer_sizes[:-1]):\n            model.add_module(str(idx), combo(in_ftr, out_ftr))\n        # Note here is the main difference: the last layer also has spectral normalization\n        # This was not the case in the previous MLP definition\n        model.add_module(str(idx+1), spectral_norm(nn.Linear(*self.layer_sizes[-1])))\n        return model\n\n\nwidth = ambient_dim * 16\n# Note in particular the lack of the bottleneck choice below\n# That is, we don't need to actually pick a bottleneck dimension -- LVA automatically determines this, like PCA\nencoder = MLP(ambient_dim, ambient_dim, [width] * 4)\n# Note also the change in the decoder to have spectral normalization\ndecoder = SNMLP(ambient_dim, ambient_dim, [width] * 4)\n\nopt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n\n\nη, λ = 0.01, 0.01\n\nfor i in range(20000):\n    opt.zero_grad()\n    z = encoder(X_torch)\n    rec_loss = F.mse_loss(decoder(z), X_torch)\n    # Note below the least volume loss\n    vol_loss = torch.exp(torch.log(z.std(0) + η).mean())\n    loss = rec_loss + λ * vol_loss\n    loss.backward()\n    opt.step()\n\n\n    if (i+1) % 1000 == 0:\n        # Print floats with 5 significant digits and fill epoch with leading spaces if under 4 digits\n        print(f'Epoch {i:4}: rec = {rec_loss:.5g}, vol = {vol_loss:.5g}')\n\nEpoch  999: rec = 0.015768, vol = 0.20888\nEpoch 1999: rec = 0.0093937, vol = 0.2092\nEpoch 2999: rec = 0.0075273, vol = 0.21866\nEpoch 3999: rec = 0.0061228, vol = 0.26539\nEpoch 4999: rec = 0.0049839, vol = 0.23421\nEpoch 5999: rec = 0.0042767, vol = 0.1931\nEpoch 6999: rec = 0.0035521, vol = 0.17556\nEpoch 7999: rec = 0.0034988, vol = 0.22726\nEpoch 8999: rec = 0.0025694, vol = 0.1861\nEpoch 9999: rec = 0.0028511, vol = 0.202\nEpoch 10999: rec = 0.0022147, vol = 0.17631\nEpoch 11999: rec = 0.0021955, vol = 0.17034\nEpoch 12999: rec = 0.0020662, vol = 0.19222\nEpoch 13999: rec = 0.0019016, vol = 0.20411\nEpoch 14999: rec = 0.0015431, vol = 0.15869\nEpoch 15999: rec = 0.0014341, vol = 0.18272\nEpoch 16999: rec = 0.0015622, vol = 0.18977\nEpoch 17999: rec = 0.0013786, vol = 0.17417\nEpoch 18999: rec = 0.0017483, vol = 0.15937\nEpoch 19999: rec = 0.0014537, vol = 0.20069\n\n\n\n\nShow Code\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(projection='3d')\nX_P = decoder(encoder(X_torch)).detach().numpy()\n#X_P = decoded.detach().numpy()\n\nax.scatter(X[:,0], X[:,1], X[:,2],alpha=0.5)\nax.scatter(X_P[:,0], X_P[:,1], X_P[:,2])\nax.set_xlabel('$X_0$')\nax.set_ylabel('$X_1$')\nax.set_zlabel('$X_2$')\nax.elev=25\nax.azim=10\nplt.legend(['Original Data','Reconstructed Data'])\nplt.title(\"AE Reconstruction\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nencoder.eval()\ndecoder.eval()\n\nwith torch.no_grad():\n    z = encoder(X_torch)\nidx = z.std(0).argsort(descending=True)\n\nplt.scatter(z[:, idx[0]].cpu().detach().numpy(), z[:, idx[1]].cpu().detach().numpy(), s=10)\nplt.gca().set_aspect('equal')\nplt.xlabel('$z_0$')\nplt.ylabel('$z_1$')\nplt.show()\n\nplt.scatter(z[:, idx[0]].cpu().detach().numpy(), z[:, idx[2]].cpu().detach().numpy(),s=10)\nplt.gca().set_aspect('equal')\nplt.xlabel('$z_0$')\nplt.ylabel('$z_2$')\nplt.show()\n\n# Plot the latent STDs by magnitude in the sorted order:\nplt.figure()\nplt.bar(np.arange(z.std(0).size(-1)), z.std(0)[idx])\nplt.title('latent STDs (autoencoder)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting the latent code covariances:\n\n\nShow Code\ncov = z.T[idx].cov().detach().cpu().numpy()\nplt.matshow(cov, cmap='cool')\nfor (i, j), var in np.ndenumerate(cov):\n    plt.gca().text(j, i, '{:.2e}'.format(var), ha='center', va='center')\nplt.title('Latent Covariance Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow Code\ndf = pd.DataFrame(z.cpu().detach().numpy(), columns=[f'z{i}' for i in range(z.shape[1])])\nplt.figure()\nsns.pairplot(df)\nplt.suptitle('Latent Space Pairplot', y=1.02)\nplt.show()\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nLet’s check again the reconstruction errors:\n\n\nShow Code\nX_ = decoder(z).detach()\n\nfor i in range(X_torch.size(-1)):\n    plt.figure(figsize=(3,3))\n    plt.scatter(X[:, i], X_[:, i], s=3)\n    plt.gca().set_aspect('equal')\n    plt.xlabel('groundtruth')\n    plt.ylabel('reconstruction')\n    plt.title('$x_{}$'.format(i))\n    plt.show()",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Review of Neural Networks</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html",
    "href": "part2/gen_models/intro_to_GANS.html",
    "title": "9  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "9.1 What are Generative Models really doing?\nIn this chapter, we will build geometric intuition for push-forward generative models, which are essentially trying to transport probability distributions from one space to another. We will start with simple linear maps to gain a sense of how they modify a simple 2D data space, then scale up to a small MLP GAN on a ring-of-Gaussians toy dataset. We will also talk about common metrics to evaluate generative models. Even though GANs may be comparatively simple compared to more advanced generative models that we will explore later, they will nevertheless be useful in building intuition for how push-forward generative models work.\nFundamentally, generative models are really just functions that transform a probability distribution from one space to another – you can think of them as “distribution transformers” or, move intuitively, as moving probability mass around in space. As an analogy, consider that you are sitting in a sandbox with a smooth mound of sand in front of you – the 2D Gaussian that we will use below is not too far from this, actually. A generative model is kind of like your hands, which you can use to push the sand around, creating hills and valleys, and moving the sand from one place to another. In this way, we are essentially moving the probability mass (sand, in this analogy) from a simple distribution (the smooth mound of sand) to a more complex distribution (the hills and valleys that you create with your hands). Different models that we will explore later (e.g., VAEs, normalizing flows, diffusion models) have different ways of doing this, but the core goal is the same, and they essentially all try to do one or more of three operations:\nAs we go forward, we will see how different types of models are better or worse at these different operations, and, in some cases, how they can be combined to create more powerful models.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#what-are-generative-models-really-doing",
    "href": "part2/gen_models/intro_to_GANS.html#what-are-generative-models-really-doing",
    "title": "9  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "\\(f(z) \\rightarrow x\\): Mapping points from one space (typically called the latent space) to another (typically, the data space).\n\\(f^{-1}(x) \\rightarrow z\\): Mapping points from data space back to latent space.\n\\(p(x) \\Leftrightarrow  p(z)\\): Mapping probability densities from data space to latent space (and vice versa).",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#basic-deterministic-push-forward-models-from-latent-space-to-data-space",
    "href": "part2/gen_models/intro_to_GANS.html#basic-deterministic-push-forward-models-from-latent-space-to-data-space",
    "title": "9  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "9.2 Basic Deterministic Push-Forward Models: From Latent Space to Data Space",
    "text": "9.2 Basic Deterministic Push-Forward Models: From Latent Space to Data Space\nA push-forward generative model defines a mapping \\(x = f(z)\\), where \\(z\\) is sampled from a simple latent distribution (often a standard Gaussian, though it need not be) and \\(f\\) is a deterministic function (e.g., linear map or neural network).\nWe’ll start with a 2D latent \\(z \\sim N(0, I)\\) (i.e., a standard Gaussian) and inspect how different choices of \\(f\\) reshape the distribution in data space. In this case, the latent space is 2D and the data space is also 2D, so we can visualize both spaces directly, however, in general, the latent space is often lower-dimensional than the data space.\n\n\nShow Code\ndef plot_pushforward(z: np.ndarray, x: np.ndarray, title: str = \"Linear push-forward\"):\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    axes[0].scatter(z[:, 0], z[:, 1], s=8, alpha=0.35, color='tab:gray')\n    axes[0].set_title('Latent samples z ~ N(0, I)')\n    axes[0].set_xlabel('z1')\n    axes[0].set_ylabel('z2')\n    axes[0].axis('equal')\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].scatter(x[:, 0], x[:, 1], s=8, alpha=0.5, color='tab:blue')\n    axes[1].set_title(title)\n    axes[1].set_xlabel('x1')\n    axes[1].set_ylabel('x2')\n    axes[1].axis('equal')\n    axes[1].grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n\n\n# Linear push-forward demo with interactive sliders\ndef linear_pushforward_demo(A: np.ndarray, b: np.ndarray, n: int = 1000, seed: int = RNG_SEED):\n    rng = np.random.default_rng(seed)\n    z = rng.standard_normal(size=(n, 2))\n    x = (z @ A.T) + b\n    return z, x\n\n# Default transform\nA0 = np.array([[1.2, 0.4],[0.0, 0.8]], dtype=float)\nb0 = np.array([0.0, 0.0], dtype=float)\nz_lin, x_lin = linear_pushforward_demo(A0, b0, n=2000)\nplot_pushforward(z_lin, x_lin, title='x = A z + b')\n\nWe can see from above that the simple linear map has slightly “moved” or shifted the location of the probability mass, just like if you were sculpting sand in your sandbox. You can play below with some of the sliders to manipulate the simple 2x2 weight matrix and 2x1 bias vector to see how this works interactively:\n\n\nShow Code\nif interact is not None:\n    def _interactive_pushforward(a11=1.2, a12=0.4, a21=0.0, a22=0.8, bx=0.0, by=0.0):\n        A = np.array([[a11, a12],[a21, a22]], dtype=float)\n        b = np.array([bx, by], dtype=float)\n        z, x = linear_pushforward_demo(A, b, n=2000)\n        plot_pushforward(z, x, title=f'x = A z + b')\n    interact(\n        _interactive_pushforward,\n        a11=FloatSlider(min=-2.0, max=2.0, step=0.05, value=1.2, description='a11'),\n        a12=FloatSlider(min=-2.0, max=2.0, step=0.05, value=0.4, description='a12'),\n        a21=FloatSlider(min=-2.0, max=2.0, step=0.05, value=0.0, description='a21'),\n        a22=FloatSlider(min=-2.0, max=2.0, step=0.05, value=0.8, description='a22'),\n        bx=FloatSlider(min=-2.0, max=2.0, step=0.05, value=0.0, description='b1'),\n        by=FloatSlider(min=-2.0, max=2.0, step=0.05, value=0.0, description='b2'),\n    )",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#toy-dataset-ring-of-gaussians",
    "href": "part2/gen_models/intro_to_GANS.html#toy-dataset-ring-of-gaussians",
    "title": "9  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "9.3 Toy Dataset: Ring of Gaussians",
    "text": "9.3 Toy Dataset: Ring of Gaussians\nOK, now that we have some intuition for how simple linear maps can move probability mass around in space, let’s try a more complex example where it is not so clear that a simple linear map will be sufficient. Below, we will create a common toy dataset consisting of a “ring of Gaussians” – that is, several Gaussian blobs arranged in a circle. You can see from above that no combination of weights in a simple linear map will be able to move our original probability mass from a standard Gaussian to this ring of Gaussians, so we will need something more powerful. A simple starting point for this is something called a Generative Adversarial Network (GAN), which we will explore next.\n\n\nShow Code\nX_ring, y_ring = create_ring_gaussians()\nplt.figure(figsize=(5.5,5.5))\nsc = plt.scatter(X_ring[:,0], X_ring[:,1], c=y_ring, cmap='tab10', s=10, alpha=0.6)\nplt.colorbar(sc, label='Mode index')\nplt.title('Toy Dataset: Colored Gaussian Ring')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.axis('equal')\nplt.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()\n\nring_latent_limits = ((-3.5, 3.5), (-3.5, 3.5))\nring_data_limits = _compute_axis_limits(X_ring)",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#what-is-a-generative-adversarial-network-gan",
    "href": "part2/gen_models/intro_to_GANS.html#what-is-a-generative-adversarial-network-gan",
    "title": "9  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "9.4 What is a Generative Adversarial Network (GAN)?",
    "text": "9.4 What is a Generative Adversarial Network (GAN)?\nA Generative Adversarial Network (GAN) is a framework for training generative models using two competing neural networks: a generator and a discriminator. The generator, \\(G\\), learns to map random noise from a latent space (e.g., \\(z \\sim p_z(z)\\) or \\(f(z)=x\\), where \\(G\\) functions as \\(f\\) here) to data space, aiming to produce samples that resemble the real data. It is tempting to want to train just the Generator by minimizing Mean Squared Error between \\(z\\) and \\(f(z)=x\\), except we have one big problem for right now – we do not know apriori which samples of \\(x\\) correspond to which samples of \\(z\\), and so it is not straightforward to compute this MSE.1 Instead, what we will do is train a separate network, the Discriminator, that can help us push the samples produced by the Generator closer to the real data distribution.\n1 Indeed, we will return to this idea in later notebooks once we have introduced the concept of Optimal Transport, which solves this mapping problem, but for now, let’s assume that we don’t know how to do this.Specifically, the discriminator, \\(D\\), tries to distinguish between real data samples and those produced by the generator and our loss function will encourage the discriminator to become better and better at this task. In turn, the generator will be trained to produce samples that the discriminator classifies as real – that is to try to fool the disciminator.\nThe two networks are trained simultaneously in something called a minimax game:\n\nThe generator tries to “fool” the discriminator by generating realistic samples.\nThe discriminator tries to correctly classify real vs. generated samples.\n\n\n9.4.1 Mathematical Formulation\nThe standard GAN objective, as introduced by Goodfellow et al. (2014), is:\n\\[\n\\min_G \\max_D \\; V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\left[ \\log D(x) \\right] + \\mathbb{E}_{z \\sim p_z(z)} \\left[ \\log (1 - D(G(z))) \\right]\n\\]\n\n\\(p_{\\text{data}}(x)\\): Distribution of real data.\n\\(p_z(z)\\): Prior distribution over latent variables (often standard normal).\n\\(G(z)\\): Generator’s output given latent input \\(z\\).\n\\(D(x)\\): Discriminator’s estimate of the probability that \\(x\\) is real.\n\nThe generator and discriminator are typically neural networks trained with stochastic gradient descent. The generator improves by producing samples that the discriminator cannot distinguish from real data (the second term on the right-hand side of the equation above), while the discriminator improves by getting better at distinguishing real from fake (the first term on the right-hand side of the equation above).",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#a-simple-generative-adversarial-network-gan",
    "href": "part2/gen_models/intro_to_GANS.html#a-simple-generative-adversarial-network-gan",
    "title": "9  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "9.5 A Simple Generative Adversarial Network (GAN)",
    "text": "9.5 A Simple Generative Adversarial Network (GAN)\nNow we will define a small GAN and see how it moves probability mass around after passing samples through the Generator. You can feel free to modify the architecture below if you like and see how that impacts the below results, but for now we will use a single hidden layer MLP with LeakyReLU activation for both the Generator and Discriminator.\n\nclass MLPGenerator(nn.Module):\n    def __init__(self, noise_dim: int = 2, hidden_dim: int = 256, out_dim: int = 2):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Linear(noise_dim, hidden_dim), nn.LeakyReLU(),\n            nn.Linear(hidden_dim, hidden_dim), nn.LeakyReLU(),\n            nn.Linear(hidden_dim, out_dim),\n)\n    def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n        return self.main(z)\n\nclass MLPDiscriminator(nn.Module):\n    def __init__(self, input_dim: int = 2, hidden_dim: int = 256):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim), nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, hidden_dim), nn.LeakyReLU(0.2),\n            nn.Linear(hidden_dim, 1),\n)\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.main(x).squeeze(-1)\n\ndef build_generator(noise_dim=2, hidden_dim=256):\n    return MLPGenerator(noise_dim=noise_dim, hidden_dim=hidden_dim).to(device)\n\ndef build_discriminator(hidden_dim=256):\n    return MLPDiscriminator(hidden_dim=hidden_dim).to(device)",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#basic-training-loop",
    "href": "part2/gen_models/intro_to_GANS.html#basic-training-loop",
    "title": "9  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "9.6 Basic Training Loop",
    "text": "9.6 Basic Training Loop\nWith the models defined, we will now set up the training loop to optimize both the generator and discriminator.\n\ndef train_vanilla_gan(\n    data: np.ndarray, *, noise_dim: int = 2, batch_size: int = 256, epochs: int = 120,\n    lr_g: float = 2e-4, lr_d: float = 2e-4, hidden_dim: int = 256, print_every: int = 40) -&gt; tuple[nn.Module, nn.Module, GanHistory]:\n    # Load the data into a DataLoader for batching and make PyTorch happy\n    loader = make_loader(data, batch_size)\n\n    # Set up the basic networks\n    G = build_generator(noise_dim=noise_dim, hidden_dim=hidden_dim)\n    D = build_discriminator(hidden_dim=hidden_dim)\n    # Instantiate the optimizers for each model\n    opt_g = optim.Adam(G.parameters(), lr=lr_g, betas=(0.5, 0.999))\n    opt_d = optim.Adam(D.parameters(), lr=lr_d, betas=(0.5, 0.999))\n    bce = nn.BCEWithLogitsLoss()\n    # Record the loss history for plotting later\n    hist = GanHistory([], [], [], [], [])\n\n    # Now we do the training loop for # epochs defined in `epochs`\n    for ep in range(epochs):\n        d_losses=[]\n        g_losses=[]\n        real_scores=[]\n        fake_scores=[]\n        for (xb,) in loader:\n            # Send the data to the GPU, if using.\n            xb = xb.to(device)\n\n            # Take a Discriminator step\n            z = torch.randn(xb.size(0), noise_dim, device=device)\n            with torch.no_grad():\n                x_fake = G(z)\n            opt_d.zero_grad()\n            d_real = D(xb)\n            d_fake = D(x_fake)\n            loss_d = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))\n            loss_d.backward()\n            opt_d.step()\n            d_losses.append(float(loss_d.detach().cpu().item()))\n            real_scores.append(d_real.mean().item())\n            fake_scores.append(d_fake.mean().item())\n\n            # Take a Generator step\n            z = torch.randn(xb.size(0), noise_dim, device=device)\n            opt_g.zero_grad()\n            xg = G(z)\n            dg = D(xg)\n            loss_g = bce(dg, torch.ones_like(dg))\n            loss_g.backward()\n            opt_g.step()\n            g_losses.append(float(loss_g.detach().cpu().item()))\n        \n        # We'll record some epoch metrics\n        with torch.no_grad():\n            z_eval = torch.randn(2048, noise_dim, device=device)\n            samples = G(z_eval)\n            div = compute_diversity_metric(samples)\n\n        # Now we'll record the metrics for plotting later and reporting\n        hist.d_loss.append(float(np.mean(d_losses)))\n        hist.g_loss.append(float(np.mean(g_losses)))\n        hist.diversity.append(div)\n        hist.real_scores.append(float(np.mean(real_scores)))\n        hist.fake_scores.append(float(np.mean(fake_scores)))\n        if (ep+1) % max(1, print_every) == 0 or ep==0:\n            print(f\"Epoch {ep+1:03d}/{epochs} | D {hist.d_loss[-1]:.3f} | G {hist.g_loss[-1]:.3f} | Div {div:.3f}\")\n    return G, D, hist\n\nYou can feel free to modify some of the training elements here, such as the epoch length or learning rates of the Generator or Discriminator, respectively.\n\nG, D, H = train_vanilla_gan(\n    X_ring, epochs=120, batch_size=256, \n    lr_g=2e-4, lr_d=2e-4, \n    hidden_dim=256, noise_dim=2, \n    print_every=40)\n\nOK, now the model is trained, so let’s look at some basic visualizations of how it did. In the below plots, which we will re-use for other models later on, we will show four things:\n\nUpper Left: Any training losses for the model, as a function of training epoch. In the case of a GAN, this will plot both the Generator and Discriminator losses. This plot type allows us to assess something about the convergence and stability of training.\nUpper Right: The Sample Diversity, as a function of Epoch. This is computing and plotting the variance of \\(\\mathbf{x}\\) as a function of epoch, where \\(\\mathbf{x}\\) are samples drawn from the Generator at each epoch. This plot type allows us one way to assess whether the model is suffering from mode collapse (i.e., low diversity in the generated samples) or not.\nLower Right: A plot of the real data samples (in light grey) and samples drawn from the Generator (in reddish-orange). This plot type allows us to visually assess how well the model is capturing the data distribution. This will be easy to compare in this simple 2D case, but will be harder in higher dimensions, and in those cases you might have to resort to just comparing selected samples or overall distribution summary statistics.\nLower Right: This plot will vary depending on the specific model we are studying, but in this case it will show the specific scores of the Discriminator (often called the “Critic”) with respect to its classification accuracy on real vs. fake samples. In the context of a GAN model, this allows us to assess how well a Generator is fooling the discriminator, since a well-trained Generator should produce samples that the Discriminator classifies as real, and thus both the “Real Score” and “Fake Score” should have around 50% accuracy. (i.e., the Discriminator is effectively guessing randomly).\n\n\n\nShow Code\nplot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (Vanilla GAN)')\n\n\nIn addition to the above training diagnostics, it can also be useful to plot interpolations in latent space to see how smoothly the Generator can move between different modes of the data distribution. In the case of a well-trained GAN, we would expect that interpolating between two points in latent space should produce a smooth transition in data space, moving through intermediate samples that also look realistic. In this case, this means hopping smoothly between the different Gaussian blobs in our ring-of-Gaussians dataset, and not jumping back-and-forth randomly between them.\n\n\nShow Code\nplot_latent_interpolation(G, noise_dim=2, title_suffix=' (Vanilla GAN)')\n\n\nTo give you some interactive control on visualizing the forward mapping \\(f(z) \\rightarrow x\\), use the sliders below to move a single latent vector (left-side plot) and observe the corresponding generated point in data space (right-side plot). By moving around in \\(z\\) you can try to align the generated point with a particular mode on the ring.\n\n\nShow Code\nif interact is not None:\n    def _move_latent(z1: float = 0.0, z2: float = 0.0):\n        \"\"\"\n        Visualize the effect of moving a latent vector z = [z1, z2] through the generator.\n        Left: latent space (z1, z2) with current point highlighted.\n        Right: generated data point in data space, overlaid on the real data.\n        \"\"\"\n        z = torch.tensor([[z1, z2]], dtype=torch.float32, device=device)\n        with torch.no_grad():\n            x = G(z).cpu().numpy()[0]\n\n        fig, axes = plt.subplots(1, 2, figsize=(11, 5))\n\n        # Left: latent space\n        axes[0].scatter(0, 0, s=40, color='gray', alpha=0.2, label='Origin')\n        axes[0].scatter(z1, z2, s=120, color='crimson', edgecolors='k', linewidths=0.5, label='Current z')\n        axes[0].set_title('Latent Space (z)')\n        axes[0].set_xlabel('z1')\n        axes[0].set_ylabel('z2')\n        axes[0].set_xlim(ring_latent_limits[0])\n        axes[0].set_ylim(ring_latent_limits[1])\n        axes[0].grid(True, alpha=0.3)\n        axes[0].legend(loc='upper left', frameon=False)\n\n        # Right: data space\n        axes[1].scatter(X_ring[:,0], X_ring[:,1], s=10, alpha=0.15, color='gray', label='Real')\n        axes[1].scatter([x[0]], [x[1]], s=120, color='crimson', edgecolors='k', linewidths=0.5, label='Generated')\n        axes[1].set_title('Generated Data Point')\n        axes[1].set_xlabel('x1')\n        axes[1].set_ylabel('x2')\n        axes[1].set_xlim(ring_data_limits[0])\n        axes[1].set_ylim(ring_data_limits[1])\n        axes[1].grid(True, alpha=0.3)\n        axes[1].legend(loc='upper left', frameon=False)\n\n        plt.tight_layout()\n        plt.show()\n    interact(\n        _move_latent,\n        z1=FloatSlider(min=-3.0, max=3.0, step=0.05, value=0.0, description='z1'),\n        z2=FloatSlider(min=-3.0, max=3.0, step=0.05, value=0.0, description='z2'),\n    )\nelse:\n    print('ipywidgets not available; skipping latent sliders.')",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/intro_to_GANS.html#summary-and-next-steps",
    "href": "part2/gen_models/intro_to_GANS.html#summary-and-next-steps",
    "title": "9  Introduction to Push-Forward Generative Models – Generative Adversarial Networks (GANs)",
    "section": "9.7 Summary and Next Steps:",
    "text": "9.7 Summary and Next Steps:\nOK, so we have seen how a basic GAN is set up and trained, and covered some diagnostic plots that give us insight into how the training went. This gave us the following sets of useful tools:\n\nWe could learn a forward mapping function \\(f(z) \\rightarrow x\\) that could push forward a given sample in \\(z\\) to a corresponding sample in \\(x\\). This could learn distributions that were far more complex than a simple linear map could achieve above.\nThis \\(f\\) is a deterministic function – i.e., the same \\(z\\) will always produce the same \\(x\\).\nWe could produce latent interpolations in \\(z\\) that produced smooth transitions in \\(x\\) and seemed to capture the overall clustering behavior of certain distributions.\n\nUnfortunately, GANs did not give us any of the following properties, which will turn out to be useful later and motivate other types of generative models:\n\nWe did not learn an inverse mapping \\(f^{-1}(x) \\rightarrow z\\). This means that we cannot easily encode real data samples into the latent space, which would be useful for many applications. For example, if I wanted to slightly modify \\(x\\) – e.g., \\(x^\\prime = x+\\delta x\\) – and compute the corresponding latent coordinate \\(z^\\prime = z+\\delta z\\), GANS do not provide a built-it mechanism to do this.2 We will see how both Variational Autoencoders (VAEs) and Normalizing Flows (NFs) address this problem.\nWe also cannot yet easily talk about or compute probability mappings between \\(p(x)\\) and \\(p(z)\\), which would be useful for computing likelihoods of data samples or for evaluating how well the model fits the data distribution, both of which are quite useful. Again, we will see later how both VAEs and NFs address this problem.\n\n2 Of course, I could try to get at this another way, for example, by applying automatic differentiation to the Generator to compute \\(\\frac{\\partial f}{\\partial z}\\) and then use that to estimate \\(\\delta z \\approx (\\frac{\\partial f}{\\partial z})^{-1} \\delta x\\), but this requires extra work on our part and this is not always straightforward, especially if the Jacobian is ill-conditioned.Before we move beyond GANs, however, we will first explore some of the complex training dynamics and other subtle issues that arise when applying them in practice. This will help you build up your intuition and experience in how to assess problems in generative models and will serve as a strong basis upon which to launch into more complex models later. It will also give us an opportunity to introduce the concept of Optimal Transport, which will be useful in its own right applications later on, and will also help us understand some of the limitations of GANs should you encounter their use in the future.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Push-Forward Generative Models -- Generative Adversarial Networks (GANs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/GAN_pitfalls.html",
    "href": "part2/gen_models/GAN_pitfalls.html",
    "title": "10  GAN Training Pitfalls",
    "section": "",
    "text": "10.1 Experiment 1: When the Discriminator Overpowers the Generator\nThis notebook will highlight some of the original problems with GAN models, some of the techniques you can use to diagnose model errors, and also introduce common rememdies for simple feed-forward GAN models. The next notebook will introduce the concept of Optimal Transport, and show how this mitigates some of the problems caused by the original GAN formulation as a minimax game.\nAs with the last notebook, we will use the simple Ring of Gaussian Toy example to illustrate this for pedagogical purposes. In later notebooks, we will see how these problems can manifest in more complex settings.\nLet’s start by loading the dataset again:\nWe will now run a series of experiments to show you common problems and how they manifest in the plotting visualizations.\nIn this experiment, we will see what happens with the Discriminator part of the GAN significantly overpowers the Generator. This can occur because of several reasons:\nWe can induce this below by just significantly increasing the learning rate of the Discriminator relative to the Generator. Consider the following questions and experiment:\nShow Code\nclass MLPGenerator(nn.Module):\n    def __init__(self, noise_dim: int = 2, hidden_dim: int = 256, out_dim: int = 2, spectral_normalization: bool = False):\n        super().__init__()\n        # Create linear layers and optionally apply spectral normalization\n        lin1 = nn.Linear(noise_dim, hidden_dim)\n        lin2 = nn.Linear(hidden_dim, hidden_dim)\n        lin3 = nn.Linear(hidden_dim, out_dim)\n        if spectral_normalization:\n            from torch.nn.utils import spectral_norm\n            lin1 = spectral_norm(lin1)\n            lin2 = spectral_norm(lin2)\n            lin3 = spectral_norm(lin3)\n        self.main = nn.Sequential(\n            lin1, nn.LeakyReLU(),\n            lin2, nn.LeakyReLU(),\n            lin3,\n        )\n\n    def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n        return self.main(z)\n\nclass MLPDiscriminator(nn.Module):\n    def __init__(self, input_dim: int = 2, hidden_dim: int = 256, spectral_normalization: bool = False):\n        super().__init__()\n        # Optionally apply spectral normalization to linear layers to stabilize training\n        layers = []\n        lin1 = nn.Linear(input_dim, hidden_dim)\n        lin2 = nn.Linear(hidden_dim, hidden_dim)\n        lin3 = nn.Linear(hidden_dim, 1)\n        if spectral_normalization:\n            from torch.nn.utils import spectral_norm\n            lin1 = spectral_norm(lin1)\n            lin2 = spectral_norm(lin2)\n            lin3 = spectral_norm(lin3)\n        layers.extend([lin1, nn.LeakyReLU(0.2), lin2, nn.LeakyReLU(0.2), lin3])\n        self.main = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.main(x).squeeze(-1)\n\n\ndef build_generator(noise_dim=2, hidden_dim=256, spectral_normalization: bool = False):\n    return MLPGenerator(noise_dim=noise_dim, hidden_dim=hidden_dim, spectral_normalization=spectral_normalization).to(device)\n\n\ndef build_discriminator(hidden_dim=256, spectral_normalization: bool = False):\n    return MLPDiscriminator(hidden_dim=hidden_dim, spectral_normalization=spectral_normalization).to(device)\nShow Code\ndef train_vanilla_gan(\n    data: np.ndarray, *, noise_dim: int = 2, batch_size: int = 256, epochs: int = 120,\n    lr_g: float = 2e-4, lr_d: float = 2e-4, hidden_dim: int = 256, print_every: int = 40,\n    spectral_normalization: bool = False, checkpoint_interval: int = 0) -&gt; tuple[nn.Module, nn.Module, GanHistory]:\n    # Load the data into a DataLoader for batching and make PyTorch happy\n    loader = make_loader(data, batch_size)\n\n    # Set up the basic networks\n    G = build_generator(noise_dim=noise_dim, hidden_dim=hidden_dim)\n    D = build_discriminator(hidden_dim=hidden_dim, spectral_normalization=spectral_normalization)\n    # Instantiate the optimizers for each model\n    opt_g = optim.Adam(G.parameters(), lr=lr_g, betas=(0.5, 0.999))\n    opt_d = optim.Adam(D.parameters(), lr=lr_d, betas=(0.5, 0.999))\n    bce = nn.BCEWithLogitsLoss()\n    # Record the loss history for plotting later\n    hist = GanHistory([], [], [], [], [])\n\n    # Helper to capture a snapshot\n    def _capture_snapshot(ep_idx: int):\n        with torch.no_grad():\n            z_eval = torch.randn(2048, noise_dim, device=device)\n            samples = G(z_eval).cpu().numpy()\n        # Save a shallow copy of generator state and a small sample for visualization\n        state = {k: v.cpu().clone() for k, v in G.state_dict().items()}\n        small_samples = samples[np.random.choice(samples.shape[0], size=min(512, samples.shape[0]), replace=False)]\n        hist.snapshots.append({\"epoch\": ep_idx, \"state_dict\": state, \"samples\": small_samples})\n\n    # Optionally capture an initial snapshot\n    if checkpoint_interval and checkpoint_interval &gt; 0:\n        _capture_snapshot(0)\n\n    # Now we do the training loop for # epochs defined in `epochs`\n    for ep in range(epochs):\n        d_losses=[]\n        g_losses=[]\n        real_scores=[]\n        fake_scores=[]\n        for (xb,) in loader:\n            # Send the data to the GPU, if using.\n            xb = xb.to(device)\n\n            # Take a Discriminator step\n            z = torch.randn(xb.size(0), noise_dim, device=device)\n            with torch.no_grad():\n                x_fake = G(z)\n            opt_d.zero_grad()\n            d_real = D(xb)\n            d_fake = D(x_fake)\n            loss_d = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))\n            loss_d.backward()\n            opt_d.step()\n            d_losses.append(float(loss_d.detach().cpu().item()))\n            real_scores.append(d_real.mean().item())\n            fake_scores.append(d_fake.mean().item())\n\n            # Take a Generator step\n            z = torch.randn(xb.size(0), noise_dim, device=device)\n            opt_g.zero_grad()\n            xg = G(z)\n            dg = D(xg)\n            loss_g = bce(dg, torch.ones_like(dg))\n            loss_g.backward()\n            opt_g.step()\n            g_losses.append(float(loss_g.detach().cpu().item()))\n        \n        # We'll record some epoch metrics\n        with torch.no_grad():\n            z_eval = torch.randn(2048, noise_dim, device=device)\n            samples = G(z_eval)\n            div = compute_diversity_metric(samples)\n\n        # Now we'll record the metrics for plotting later and reporting\n        hist.d_loss.append(float(np.mean(d_losses)))\n        hist.g_loss.append(float(np.mean(g_losses)))\n        hist.diversity.append(div)\n        hist.real_scores.append(float(np.mean(real_scores)))\n        hist.fake_scores.append(float(np.mean(fake_scores)))\n\n        # Optionally capture a checkpoint snapshot\n        if checkpoint_interval and checkpoint_interval &gt; 0 and ((ep + 1) % checkpoint_interval == 0):\n            _capture_snapshot(ep + 1)\n\n        if (ep+1) % max(1, print_every) == 0 or ep==0:\n            print(f\"Epoch {ep+1:03d}/{epochs} | D {hist.d_loss[-1]:.3f} | G {hist.g_loss[-1]:.3f} | Div {div:.3f}\")\n    return G, D, hist\nG, D, H = train_vanilla_gan(\n    X_ring, epochs=120, batch_size=256,\n    lr_g=1e-5,\n    lr_d=1e-3,\n    hidden_dim=256, noise_dim=2, \n    checkpoint_interval=10,\n    print_every=40)\n\nEpoch 001/120 | D 0.537 | G 1.542 | Div 0.009\nEpoch 040/120 | D 0.331 | G 3.036 | Div 0.307\nEpoch 040/120 | D 0.331 | G 3.036 | Div 0.307\nEpoch 080/120 | D 0.388 | G 2.256 | Div 0.062\nEpoch 080/120 | D 0.388 | G 2.256 | Div 0.062\nEpoch 120/120 | D 0.394 | G 2.226 | Div 0.062\nEpoch 120/120 | D 0.394 | G 2.226 | Div 0.062\nShow Code\nplot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (GAN w/ D &gt;&gt; G)')\nplot_latent_interpolation(G, noise_dim=2, title_suffix=' (GAN w/ D &gt;&gt; G)')\nWe can also take a look at this interactively, by viewing various snapshots during training:\nShow Code\n# Interactive checkpoint browser (requires ipywidgets)\nif interact is None:\n    print(\"ipywidgets not available. Install ipywidgets to use the interactive checkpoint browser.\")\nelse:\n    from ipywidgets import IntSlider, Output, VBox, Label\n\n    out = Output()\n\n    def show_checkpoint(idx: int):\n        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n            with out:\n                print(\"No checkpoints available. Rerun training with checkpoint_interval&gt;0 to populate snapshots.\")\n            return\n        cp = H.snapshots[idx]\n        # Load generator state into a temporary generator instance\n        G_temp = build_generator(noise_dim=2, hidden_dim=256)\n        # load state dict (ensure tensors are moved to device)\n        state = {k: v.to(device) for k, v in cp[\"state_dict\"].items()}\n        G_temp.load_state_dict(state)\n\n        with out:\n            out.clear_output(wait=True)\n            # Plot diagnostics and latent interpolation for this checkpoint\n            print(f\"Checkpoint: index={idx} | epoch={cp['epoch']}\")\n            plot_model_diagnostics(H, X_ring, G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\")\n            plot_latent_interpolation(G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\", real_samples=X_ring, latent_limits=ring_latent_limits, data_limits=ring_data_limits)\n\n    def make_slider():\n        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n            print(\"No checkpoints available. Rerun training with checkpoint_interval&gt;0 to populate snapshots.\")\n            return\n        max_idx = len(H.snapshots) - 1\n        slider = IntSlider(value=0, min=0, max=max_idx, step=1, description='Checkpoint')\n        def on_change(change):\n            if change['name'] == 'value':\n                show_checkpoint(change['new'])\n        slider.observe(on_change)\n        display(VBox([Label('Checkpoint browser'), slider, out]))\n        # show initial\n        show_checkpoint(0)\n\n    make_slider()",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GAN Training Pitfalls</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/GAN_pitfalls.html#experiment-1-when-the-discriminator-overpowers-the-generator",
    "href": "part2/gen_models/GAN_pitfalls.html#experiment-1-when-the-discriminator-overpowers-the-generator",
    "title": "10  GAN Training Pitfalls",
    "section": "",
    "text": "The Discriminator is a much larger (or higher capacity) network than the Generator, and thus is able to move its classification decision boundary in more complex ways to distinguish real from fake samples, and the Generator doesn’t have sufficient capacity to compensate for this.\nThe Discriminator trains much faster than the Generator, and thus can move its decision boundary faster than the Generator can keep up.\nThe Discriminator is initialized (either on purpose or by random chance) such that it has an initial significant advantage over the Generator. In this case, even if the Generator has high capacity and can train quickly, it may not be able to recover from this initial disadvantage, as the discriminator make lock it into a portion of the sample space from which local gradient descent on the Generator cannot recover.\n\n\n\n\n\n\n\n\nTipExperiment: What happens when the Discriminator overpowers the Generator?\n\n\n\nBelow you can change the relative learning rates of the Discriminator versus the Generator:\n\nWhat happens when the Discriminator is able to train significantly faster than the Generator? What about the other way around?\nHow can we tell from the various loss plots that the Discriminator is overpowering the Generator? How does this behavior manifest itself in the generated samples?\nWhy is it that the generator is unable to recover from this situation, even though it has the capacity to represent the data distribution? What about the training procedure and dynamics of the minimax game prevents the Generator from catching up?\nIn the latent interpolation plots, as the GAN spans its latent set of coordinates, what happens to the generated samples?",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GAN Training Pitfalls</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/GAN_pitfalls.html#experiment-2-when-the-generator-overpowers-the-discriminator",
    "href": "part2/gen_models/GAN_pitfalls.html#experiment-2-when-the-generator-overpowers-the-discriminator",
    "title": "10  GAN Training Pitfalls",
    "section": "10.2 Experiment 2: When the Generator Overpowers the Discriminator",
    "text": "10.2 Experiment 2: When the Generator Overpowers the Discriminator\nAbove we saw what happens when the Discriminator overpowers the Generator. But what about the other way around? The next experiment will help illuminate what can go wrong.\n\n\n\n\n\n\nTipExperiment: What happens when the Generator overpowers the Discriminator?\n\n\n\nBelow you can change the relative learning rates of the Discriminator versus the Generator to give the Generator a significant advantage:\n\nWhat happens when we turn the tables on the Discriminator, and give the Generator comparatively more power?\nHow are the loss plots this time significantly different than what you saw in the first experiment?\nAs you look through the training curves as well as snapshots during training, what do you notice about the generated samples? Why would an underpowered Discriminator lead to this behavior? (Hint: Play out conceptually in your head how the generator would respond in cases where the Discriminator is very weak or slow.)\n\n\n\n\nG, D, H = train_vanilla_gan(\n    X_ring, epochs=120, batch_size=256,\n    lr_g=1e-4,\n    lr_d=1e-5,\n    hidden_dim=256, noise_dim=2, \n    checkpoint_interval=10,\n    print_every=40)\n\nEpoch 001/120 | D 1.347 | G 0.704 | Div 0.030\nEpoch 040/120 | D 1.346 | G 0.751 | Div 0.015\nEpoch 040/120 | D 1.346 | G 0.751 | Div 0.015\nEpoch 080/120 | D 1.368 | G 0.716 | Div 0.650\nEpoch 080/120 | D 1.368 | G 0.716 | Div 0.650\nEpoch 120/120 | D 1.370 | G 0.731 | Div 0.343\nEpoch 120/120 | D 1.370 | G 0.731 | Div 0.343\n\n\n\n\nShow Code\nplot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (GAN w/ G &gt;&gt; D)')\nplot_latent_interpolation(G, noise_dim=2, title_suffix=' (GAN w/ G &gt;&gt; D)')\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n\n\n\n\n\n\n\n\n\nYou can also view this interactively at different training snapshots below, which might help illuminate some behavior:\n\n\nShow Code\n# Interactive checkpoint browser (requires ipywidgets)\nif interact is None:\n    print(\"ipywidgets not available. Install ipywidgets to use the interactive checkpoint browser.\")\nelse:\n    from ipywidgets import IntSlider, Output, VBox, Label\n\n    out = Output()\n\n    def show_checkpoint(idx: int):\n        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n            with out:\n                print(\"No checkpoints available. Rerun training with checkpoint_interval&gt;0 to populate snapshots.\")\n            return\n        cp = H.snapshots[idx]\n        # Load generator state into a temporary generator instance\n        G_temp = build_generator(noise_dim=2, hidden_dim=256)\n        # load state dict (ensure tensors are moved to device)\n        state = {k: v.to(device) for k, v in cp[\"state_dict\"].items()}\n        G_temp.load_state_dict(state)\n\n        with out:\n            out.clear_output(wait=True)\n            # Plot diagnostics and latent interpolation for this checkpoint\n            print(f\"Checkpoint: index={idx} | epoch={cp['epoch']}\")\n            plot_model_diagnostics(H, X_ring, G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\")\n            plot_latent_interpolation(G_temp, noise_dim=2, title_suffix=f\" (Checkpoint ep {cp['epoch']})\", real_samples=X_ring, latent_limits=ring_latent_limits, data_limits=ring_data_limits)\n\n    def make_slider():\n        if not hasattr(H, \"snapshots\") or len(H.snapshots) == 0:\n            print(\"No checkpoints available. Rerun training with checkpoint_interval&gt;0 to populate snapshots.\")\n            return\n        max_idx = len(H.snapshots) - 1\n        slider = IntSlider(value=0, min=0, max=max_idx, step=1, description='Checkpoint')\n        def on_change(change):\n            if change['name'] == 'value':\n                show_checkpoint(change['new'])\n        slider.observe(on_change)\n        display(VBox([Label('Checkpoint browser'), slider, out]))\n        # show initial\n        show_checkpoint(0)\n\n    make_slider()",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GAN Training Pitfalls</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/GAN_pitfalls.html#experiment-3-effect-of-discriminator-spectral-normalization",
    "href": "part2/gen_models/GAN_pitfalls.html#experiment-3-effect-of-discriminator-spectral-normalization",
    "title": "10  GAN Training Pitfalls",
    "section": "10.3 Experiment 3: Effect of Discriminator Spectral Normalization",
    "text": "10.3 Experiment 3: Effect of Discriminator Spectral Normalization\nPart of the issue in Experiment 1 (where the Discriminator overpowers the Generator) is that the Discriminator can make very sudden and extreme changes to its decision boundary, which the Generator cannot keep up with. One way to mitigate this is to use Spectral Normalization on the Discriminator weights, which constrains the Lipschitz constant of the Discriminator function, and thus prevents it from making extreme changes to its decision boundary.\n\n\n\n\n\n\nTipExperiment: How does Spectral Normalization affect the Discriminator?\n\n\n\nBelow we will activate spectral_normalization=True in the Discriminator, and see how this affects the training dynamics when the Discriminator is given a significant learning rate advantage over the Generator.\n\nKeeping the learning rates from Experiment 1, what do you now notice about the Discriminator’s training losses?\nDo different learning rates modulate this effect? You can repeat a version of Experiment 2 here if you are curious.\nIn what ways has applying this spectral normalization improved the model performance? In what ways has it made it worse? Why do you think this is? (Hint: Think about what Spectral Normalization controls within a network, and what effect that would have if applied to the Discriminator in a mini-max game against the Generator.)\nLooking only at the various training curves, do you notice the effect of the Spectral normalization? Where do the effect of the Spectral Normalization show itself within the diagnostics?\n\n\n\n\n\nShow Code\nG, D, H = train_vanilla_gan(\n    X_ring, epochs=240, batch_size=256,\n    lr_g=1e-4,\n    lr_d=5e-4,\n    spectral_normalization=True,\n    hidden_dim=256, noise_dim=2, \n    print_every=40)\n\n\nEpoch 001/240 | D 1.299 | G 0.731 | Div 0.143\nEpoch 040/240 | D 1.371 | G 0.703 | Div 4.572\nEpoch 040/240 | D 1.371 | G 0.703 | Div 4.572\nEpoch 080/240 | D 1.374 | G 0.700 | Div 4.610\nEpoch 080/240 | D 1.374 | G 0.700 | Div 4.610\nEpoch 120/240 | D 1.376 | G 0.700 | Div 4.528\nEpoch 160/240 | D 1.378 | G 0.697 | Div 4.569\nEpoch 200/240 | D 1.381 | G 0.697 | Div 4.568\nEpoch 240/240 | D 1.383 | G 0.699 | Div 4.630\n\n\n\n\nShow Code\nplot_model_diagnostics(H, X_ring, G, noise_dim=2, title_suffix=' (GAN w/ D_spec_norm &gt;&gt; G)')\nplot_latent_interpolation(G, noise_dim=2, title_suffix=' (GAN w/ D_spec_norm &gt;&gt; G)')\n\n\n\n\n\n\n\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GAN Training Pitfalls</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/GAN_pitfalls.html#summary-and-next-steps",
    "href": "part2/gen_models/GAN_pitfalls.html#summary-and-next-steps",
    "title": "10  GAN Training Pitfalls",
    "section": "10.4 Summary and Next Steps",
    "text": "10.4 Summary and Next Steps\nWe have seen above how the mini-max formulation of a Generative Adversarial Network leads to unwanted training dynamics that need to be carefully tuned if the GAN is to perform well. In large part, we can see that this is the result of the two networks fighting each other. But is this fight really necessary? After all, the Generator is trying to learn the data distribution, and the Discriminator is just a tool to help it do so, because we didn’t have a clear way to map the generated samples to the real samples in a way that directly allowed us to directly optimize something like an MSE.\nIn the next notebook, we will see how we can reformulate this problem in terms of Optimal Transport, which will allow us to sidestep some of these issues and remove some of the problems cased by the mini-max game.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GAN Training Pitfalls</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/OT.html",
    "href": "part2/gen_models/OT.html",
    "title": "11  Optimal Transport for Generative Models",
    "section": "",
    "text": "11.1 What is Optimal Transport?\nIn the previous notebooks, we explored how Generative Adversarial Networks (GANs) use a minimax game between a Generator and Discriminator to learn data distributions. However, we saw that this adversarial training can lead to instability, mode collapse, and other training difficulties. In this notebook, we’ll explore an alternative approach based on Optimal Transport (OT), which provides a more direct way to measure the distance between probability distributions.\nThe key insight is this: instead of training a discriminator to distinguish real from fake samples, we can directly minimize the distance between the generated distribution and the real data distribution using optimal transport metrics. This often leads to more stable training and better coverage of the data distribution.\nLet’s go back to our earlier analogy where we were imagining probability distributions as two piles of sand, and our goal is to transform one pile into the other. Optimal transport addresses the question: What is the most efficient way to move the sand from one configuration to another?\nMore formally, given two probability distributions \\(z\\) and \\(x\\), optimal transport finds a transport plan \\(\\pi\\) that moves mass from \\(z\\) to \\(x\\) while minimizing the total transport cost. There are many possible definitions of cost here, and it is common to think of a distance as a form of cost, with the Wasserstein distance (also called the Earth Mover’s Distance) as a common one with the technical form: \\[\nW_p(z, x) = \\left( \\inf_{\\pi \\in \\Pi(z, x)} \\int \\|x - y\\|^p \\, d\\pi(x, y) \\right)^{1/p}\n\\]\nwhere \\(\\Pi(z, x)\\) is the set of all joint distributions with marginals \\(z\\) and \\(x\\).\nOK, so far so good in principle – I just need to find the transport plan that minimizes some p-norm over all joint distributions of \\(z\\) and \\(x\\). However, in practice, this is not so straightforward, since finding a minimum over all possible joint distributions of \\(z\\) and \\(x\\) is not so computationally tractable.1\nInstead, of computing the Wasserstein distance directly, we will compute an approximate version of it that regularizes the transport plan, and is called computing the Sinkhorn Divergence. Going over the specific implementation details of the Sinkhorn Divergence (which rely on Sinkhorn Iteration and knowledge of doubly stochastic matrices) are beyond the scope of what I want to cover in these notebooks, but interested students can check out Computational Optimal Transport by Gabriel Peyré and Marco Cuturi for further details.\nThe important thing to know in the context of a course at this level is that the Sinkhorn Divergence can only approximate the true Wasserstein distance, and that it does so via what is often called a “blur” parameter. This parameter is essentially a smoothing term that determines how much we penalize the complexity of the transport map. Some small amount of blur will help us compute gradients and use the Sinkhorn Divergence in ML model training, but too much of this will prevent a model from capturing fine details in the data distribution. You will see an interactive example of this next before we move on to using OT for GAN training.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal Transport for Generative Models</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/OT.html#what-is-optimal-transport",
    "href": "part2/gen_models/OT.html#what-is-optimal-transport",
    "title": "11  Optimal Transport for Generative Models",
    "section": "",
    "text": "1 At a high level, this is due to a combinatoric assignment problem in something called the coupling matrix, where you are trying to match generated and real datapoints to one another and optimize for the lowest distance. Because this assignment matrix is ultimately a binary matrix, this makes it not easily differentiable.\n\n\n11.1.1 Simple Optimal Transport Example\nLet’s start with a familiar and concrete example that we have been using in the prior GAN notebooks. We’ll take a simple 2D Gaussian and compute its optimal transport to our ring of Gaussians dataset. We’ll visualize the transport map by computing the transport vectors (i.e., in what direction we move the probability mass) and also demonstrate how moving in those directions shifts our 2D Gaussian towards the ring.\n\n\nShow Code\n# Load the ring dataset\nX_ring, y_ring = create_ring_gaussians(n_samples=2000)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nsc = ax.scatter(X_ring[:, 0], X_ring[:, 1], c=y_ring, cmap='tab10', s=15, alpha=0.6)\nplt.colorbar(sc, label='Mode index')\nax.set_title('Target: Ring of Gaussians')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.axis('equal')\nax.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNow what we will do below is place a simple 2D Gaussian distribution centered at the origin, and then compute the Sinkhorn Divergence (loss) between each point in the real dataset and each point in the simple 2D Gaussian. Using Automatic Differentiation, we can then compute how to move each point in the generated distribution to minimize this loss. We will plot a sample of these gradient vectors so that you can see what the transport map looks like and also take a (very large) step in that direction for each point, so you can see the visual effect of the transport.\nIn this case, we are only taking a single, giant step along the transport map for pedagogical purposes, but in reality (and as we will do later), you would move slowly along the transport map over many iterations to gradually morph the generated distribution into the real data distribution.\n\n# Create a simple 2D Gaussian source distribution\nn_source = 2000\nsource_samples = np.random.randn(n_source, 2).astype(np.float32) * 0.5\n\n# Convert to torch tensors\nsource_torch = torch.from_numpy(source_samples).to(device)\ntarget_torch = torch.from_numpy(X_ring).to(device)\n\n# Compute optimal transport using Sinkhorn algorithm\n# The blur parameter controls entropic regularization (larger = more regularization)\nsinkhorn_loss = SamplesLoss(\"sinkhorn\", p=2, blur=0.01, scaling=0.9)\n\n# Compute transport plan by taking gradient\nsource_torch.requires_grad_(True)\nloss = sinkhorn_loss(source_torch, target_torch)\nloss.backward()\n\n\n# This is effectively a giant step size so the we can visualize the gradients\n# and see a meaningful change in the distribution.\n# In reality, we would take much smaller steps than this and do it over iterations\nmagnitude_scaling = 2000\n\n# The gradient points in the direction of optimal transport\ntransport_direction = source_torch.grad.detach().cpu().numpy()\ntransported_points = source_samples + transport_direction*magnitude_scaling\n\n\n\nShow Code\n# Visualize the transport\nfig, axes = plt.subplots(3, 1, figsize=(10,16))\n\n# Source distribution\naxes[0].scatter(source_samples[:, 0], source_samples[:, 1], \n                s=15, alpha=0.5, c='tab:blue')\naxes[0].scatter(X_ring[:, 0], X_ring[:, 1], \n                s=8, alpha=0.2, c='lightgray', label='Target')\naxes[0].set_title('Source: 2D Gaussian')\naxes[0].set_xlabel('$x_1$')\naxes[0].set_ylabel('$x_2$')\naxes[0].axis('equal')\naxes[0].grid(True, alpha=0.2)\n\n# Transport vectors\n# Sample a subset for visualization clarity\nidx_subset = np.random.choice(n_source, size=25, replace=False)\naxes[1].scatter(X_ring[:, 0], X_ring[:, 1], \n                s=8, alpha=0.2, c='lightgray', label='Target')\naxes[1].quiver(source_samples[idx_subset, 0], source_samples[idx_subset, 1],\n                magnitude_scaling/1.5*transport_direction[idx_subset, 0], \n                magnitude_scaling/1.5*transport_direction[idx_subset, 1],\n                angles='xy', scale_units='xy', scale=0.5, width=0.005, \n                color='tab:orange', alpha=0.5)\naxes[1].set_title('Optimal Transport Vectors')\naxes[1].set_xlabel('$x_1$')\naxes[1].set_ylabel('$x_2$')\naxes[1].axis('equal')\naxes[1].grid(True, alpha=0.2)\naxes[1].legend()\n\n# Transported distribution\naxes[2].scatter(X_ring[:, 0], X_ring[:, 1], \n                s=8, alpha=0.2, c='lightgray', label='Target')\naxes[2].scatter(transported_points[:, 0], transported_points[:, 1], \n                s=15, alpha=0.5, c='tab:red', label='Transported')\naxes[2].set_title('After Transport')\naxes[2].set_xlabel('$x_1$')\naxes[2].set_ylabel('$x_2$')\naxes[2].axis('equal')\naxes[2].grid(True, alpha=0.2)\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Sinkhorn divergence: {loss.item():.4f}\")\n\n\n\n\n\n\n\n\n\nSinkhorn divergence: 2.7932\n\n\nIn the visualization above, you can see how optimal transport naturally moves mass from the source Gaussian to the target ring distribution. The transport vectors (middle plot) show the direction and magnitude of how each point should move to minimize the total transport cost.\n\n\n\n\n\n\nTipExperiment: Effect of Sinkhorn Divergence Parameters\n\n\n\nThe Sinkhorn divergence has several key parameters that affect the transport:\n\nblur (\\(\\epsilon\\)): Controls the amount of entropic regularization. Larger values make the transport “smoother” but less accurate.\np: The p-norm used for measuring distances. This is typically p=1 for Manhattan distance (sum of absolute differences) or p=2 for Euclidean distance (standard L2 norm)).\n\nUse the slider below to gain intuition about changing the effects of these three parameters: - What effect does moving from a small blur to a large blur have? - How does the transport pattern differ between p=1 and p=2?\n\n\n\n\nShow Code\nif geomloss_available and widgets_available:\n    def explore_pnorm(p: int = 2, blur: float = 0.05):\n        # Create source distribution\n        source_samples = np.random.randn(1000, 2).astype(np.float32) * 0.5\n        source_torch = torch.from_numpy(source_samples).to(device)\n        target_torch = torch.from_numpy(X_ring[:1000]).to(device)\n        \n        # Compute transport\n        sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=p, blur=blur, scaling=0.9)\n        source_torch.requires_grad_(True)\n        loss = sinkhorn_loss(source_torch, target_torch)\n        loss.backward()\n        \n        magnitude_scaling = 1500\n        \n        transport_direction = source_torch.grad.detach().cpu().numpy()\n        transported_points = source_samples + transport_direction * magnitude_scaling\n        \n        # Visualize\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n        \n        # Transport vectors\n        idx_subset = np.random.choice(1000, size=150, replace=False)\n        axes[0].scatter(X_ring[:, 0], X_ring[:, 1], \n                       s=8, alpha=0.15, c='lightgray', label='Target')\n        axes[0].quiver(source_samples[idx_subset, 0], source_samples[idx_subset, 1],\n                      magnitude_scaling/1.5*transport_direction[idx_subset, 0], \n                      magnitude_scaling/1.5*transport_direction[idx_subset, 1],\n                      angles='xy', scale_units='xy', scale=1, width=0.003, \n                      color='tab:orange', alpha=0.7)\n        axes[0].set_title(f'Transport Vectors (p={p}, blur={blur:.3f})')\n        axes[0].set_xlabel('$x_1$')\n        axes[0].set_ylabel('$x_2$')\n        axes[0].axis('equal')\n        axes[0].grid(True, alpha=0.2)\n        axes[0].legend()\n        \n        # Result\n        axes[1].scatter(X_ring[:, 0], X_ring[:, 1], \n                       s=8, alpha=0.2, c='lightgray', label='Target')\n        axes[1].scatter(transported_points[:, 0], transported_points[:, 1], \n                       s=15, alpha=0.5, c='tab:red', label='Transported')\n        axes[1].set_title(f'After Transport (loss={loss.item():.4f})')\n        axes[1].set_xlabel('$x_1$')\n        axes[1].set_ylabel('$x_2$')\n        axes[1].axis('equal')\n        axes[1].grid(True, alpha=0.2)\n        axes[1].legend()\n        \n        plt.tight_layout()\n        plt.show()\n    \n    interact(explore_pnorm, \n             p=IntSlider(min=1, max=2, step=1, value=2, description='p-norm'),\n             blur=FloatSlider(min=0.01, max=0.7, step=0.01, value=0.05, description='Blur (ε)'))\nelif not geomloss_available:\n    print(\"GeomLoss not available. Please install it to run this experiment.\")\nelse:\n    print(\"ipywidgets not available. Please install it for interactive controls.\")",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal Transport for Generative Models</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/OT.html#building-an-entropic-ot-gan",
    "href": "part2/gen_models/OT.html#building-an-entropic-ot-gan",
    "title": "11  Optimal Transport for Generative Models",
    "section": "11.2 Building an Entropic OT GAN",
    "text": "11.2 Building an Entropic OT GAN\nNow that we are beginning to understand optimal transport and how it might be useful, let’s try to use it to train a generative model. Instead of using a discriminator (as in standard GANs), we’ll directly minimize the Sinkhorn divergence between generated samples and real data. We can do this because the Sinkhorn divergence is now substituting for the original role of the Discriminator (i.e., to move the generator closer to real-world data), and so the Discriminator is no longer necessary.2\n2 One could argue that we are no longer really doing a “GAN” here because we do not have an “Adversary” now that the Discriminator is gone, so it is perhaps misleading to call it this, but since earlier papers refer to this style of Generative Model as an Entropic GAN, I will be consistent with them, even if it isn’t the best name in my view.This approach has several advantages:\n\nNo discriminator needed: We only need the network for the Generator now, which is simpler and we fewer total parameters to train. We don’t need to worry about two learning rates or different capacities in each network, like we saw in the GAN Pitfalls notebook.\nMore stable to train: Since we no longer have a minimax game to balance, we do not have to contend with oscillatory behavior of the optimizer and potentially getting trapped in a loop. This can also equate to faster training times as a result, if the learning rate is suitably tuned.\nBetter coverage: OT naturally encourages covering all modes of the data distribution, rather than hoping that a Discriminator pushes the Generator to cover all modes.\nMeaningful gradients: Because OT computes a pairwise distance among all data points, so long as the gradient of our distance/cost measure remains finite and non-zero at far away distances, we can still perform useful gradient descent steps in the generative model. This is not the case for some other cost functions (such as the KL Divergence) where gradients might vanish if the two distributions are not close enough.\n\n\nclass GeneratorMLP(nn.Module):\n    \"\"\"Simple MLP generator for 2D data.\"\"\"\n    def __init__(self, noise_dim: int = 2, hidden_dim: int = 256, out_dim: int = 2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(noise_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, out_dim)\n        )\n    \n    def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n        return self.net(z)\n\n\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass SinkhornHistory:\n    \"\"\"Training history for Sinkhorn-based generator.\"\"\"\n    loss: List[float]\n    diversity: List[float]\n\ndef train_sinkhorn_generator(\n    data: np.ndarray,\n    noise_dim: int = 2,\n    batch_size: int = 256,\n    epochs: int = 200,\n    lr: float = 1e-3,\n    hidden_dim: int = 256,\n    blur: float = 0.05,\n    p: int = 2,\n    print_every: int = 50\n) -&gt; tuple:\n    \"\"\"\n    Train a generator using Sinkhorn divergence.\n    \n    Args:\n        data: Real data samples (numpy array)\n        noise_dim: Dimension of latent noise\n        batch_size: Batch size for training\n        epochs: Number of training epochs\n        lr: Learning rate\n        hidden_dim: Hidden dimension for generator\n        blur: Entropic regularization parameter\n        p: p-norm for distance metric\n        print_every: Print progress every N epochs\n    \n    Returns:\n        Tuple of (trained generator, training history)\n    \"\"\"\n    if not geomloss_available:\n        raise ImportError(\"GeomLoss is required for Sinkhorn training. Install with: pip install geomloss\")\n    \n    # Setup data loader\n    loader = make_loader(data, batch_size)\n    \n    # Initialize generator and optimizer\n    G = GeneratorMLP(noise_dim=noise_dim, hidden_dim=hidden_dim, out_dim=2).to(device)\n    optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    # Setup Sinkhorn loss\n    sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=p, blur=blur, scaling=0.9, debias=True)\n    \n    # Training history\n    history = SinkhornHistory(loss=[], diversity=[])\n    \n    print(f\"Training Sinkhorn OT Generator for {epochs} epochs...\")\n    print(f\"Parameters: blur={blur}, p={p}, lr={lr}, hidden_dim={hidden_dim}\")\n    \n    for epoch in range(epochs):\n        epoch_losses = []\n        \n        for (real_batch,) in loader:\n            real_batch = real_batch.to(device)\n            \n            # Generate fake samples\n            z = torch.randn(batch_size, noise_dim, device=device)\n            fake_batch = G(z)\n            \n            # Compute Sinkhorn divergence\n            loss = sinkhorn_loss(fake_batch, real_batch)\n            \n            # Update generator\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            epoch_losses.append(float(loss.detach().cpu().item()))\n        \n        # Record metrics\n        mean_loss = float(np.mean(epoch_losses))\n        \n        # Compute diversity\n        with torch.no_grad():\n            z_eval = torch.randn(2048, noise_dim, device=device)\n            samples = G(z_eval)\n            diversity = compute_diversity_metric(samples)\n        \n        history.loss.append(mean_loss)\n        history.diversity.append(diversity)\n        \n        if (epoch + 1) % print_every == 0 or epoch == 0:\n            print(f\"Epoch {epoch+1:03d}/{epochs} | Loss: {mean_loss:.4f} | Diversity: {diversity:.4f}\")\n    return G, history\n\nLet’s train the OT generator on our ring dataset:\n\n\nShow Code\nif geomloss_available:\n    # Train the generator\n    G_ot, H_ot = train_sinkhorn_generator(\n        X_ring,\n        epochs=200,\n        batch_size=256,\n        noise_dim=2,\n        hidden_dim=256,\n        blur=0.02,\n        p=2,\n        lr=1e-3,\n        print_every=50\n    )\n    \n    # Generate and visualize samples\n    with torch.no_grad():\n        z_test = torch.randn(2000, 2, device=device)\n        generated_samples = G_ot(z_test).cpu().numpy()\n    \n    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n    ax.scatter(X_ring[:, 0], X_ring[:, 1], \n              s=10, alpha=0.2, c='lightgray', label='Real')\n    ax.scatter(generated_samples[:, 0], generated_samples[:, 1], \n              s=12, alpha=0.5, c='tab:orange', label='Generated (OT)')\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.set_title('Entropic OT GAN: Real vs Generated Samples')\n    ax.legend()\n    ax.axis('equal')\n    ax.grid(True, alpha=0.2)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"GeomLoss not available. Please install it to train the OT generator.\")\n\n\nTraining Sinkhorn OT Generator for 200 epochs...\nParameters: blur=0.02, p=2, lr=0.001, hidden_dim=256\nEpoch 001/200 | Loss: 2.8801 | Diversity: 1.4708\nEpoch 001/200 | Loss: 2.8801 | Diversity: 1.4708\nEpoch 050/200 | Loss: 0.1830 | Diversity: 4.3724\nEpoch 050/200 | Loss: 0.1830 | Diversity: 4.3724\nEpoch 100/200 | Loss: 0.1967 | Diversity: 4.4298\nEpoch 100/200 | Loss: 0.1967 | Diversity: 4.4298\nEpoch 150/200 | Loss: 0.1828 | Diversity: 4.2631\nEpoch 150/200 | Loss: 0.1828 | Diversity: 4.2631\nEpoch 200/200 | Loss: 0.1656 | Diversity: 4.4816\nEpoch 200/200 | Loss: 0.1656 | Diversity: 4.4816\n\n\n\n\n\n\n\n\n\nLet’s use our standard diagnostic plots from the GAN notebooks to evaluate the OT generator’s performance:\n\n\nShow Code\nplot_model_diagnostics(\n    H_ot, \n    X_ring, \n    G_ot, \n    noise_dim=2, \n    title_suffix=' (Sinkhorn OT)'\n)\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nplot_latent_interpolation(\n    G_ot, \n    noise_dim=2, \n    title_suffix=' (Sinkhorn OT)',\n    real_samples=X_ring\n)\n\n\nIgnoring fixed y limits to fulfill fixed data aspect with adjustable data limits.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal Transport for Generative Models</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/OT.html#summary-and-next-steps",
    "href": "part2/gen_models/OT.html#summary-and-next-steps",
    "title": "11  Optimal Transport for Generative Models",
    "section": "11.3 Summary and Next Steps",
    "text": "11.3 Summary and Next Steps\nIn this notebook, we explored how Optimal Transport provides an alternative to adversarial training for generative models:\n\nOptimal Transport Intuition: OT finds the most efficient way to transform one distribution into another, providing meaningful gradients even when distributions don’t overlap.\nSinkhorn Divergence: By adding entropic regularization, we make OT computationally tractable while preserving most benefits. The blur parameter controls the trade-off between precision and smoothness.\nEntropic OT GANs: We can train generative models by directly minimizing Sinkhorn divergence, eliminating the need for a discriminator and the associated minimax game.\nAdvantages over Traditional GANs:\n\nSimpler architecture (no discriminator)\nMore stable training (no adversarial dynamics)\nBetter mode coverage (OT naturally spreads mass)\nMeaningful loss values (directly related to distribution distance)\n\nTrade-offs: OT-based training requires careful tuning of the blur parameter and can be computationally expensive for very large datasets. In particular, OT-based distances are very good when the source and target distributions are already well aligned, but less so when the transport distance is high (since then the optimal map computed by Sinkhorn Iteration may not be as discriminative). As a consequence, many real-world applications of OT in a generative model context use OT as a fine-tuning step after doing an alignment or registration step (e.g., when matching a mesh of an organ to data from a CT scan).\n\nSo, at this point we have addressed one of the key weaknesses of GANs – the instability of adversarial training – by removing the discriminator entirely and replacing it with a direct measure of distribution distance. However, there is still one big missing piece to be desired by Entropic GAN-style models. While we have a better forward map now of \\(f(z) \\rightarrow x\\) via the generator, we still do not have an inverse map \\(f^{-1}(x) \\rightarrow z\\) that would allow us to encode real data points back into the latent space. This is something that Variational Autoencoders (VAEs) and Normalizing Flows provide, and is where we turn our attention next. We will see that the general concept of OT will raise its head again after Normalizing Flows in the context of something called “Flow Matching”, and we will return to this in a few notebooks.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal Transport for Generative Models</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/VAEs.html",
    "href": "part2/gen_models/VAEs.html",
    "title": "12  Variational Autoencoders (VAEs)",
    "section": "",
    "text": "13 Variational Autoencoders (VAEs)\nIn the previous notebooks, we explored push-forward generative models (GANs) and then moved to optimal transport approaches. These models excel at generating samples by learning a mapping \\(f(z) \\rightarrow x\\) from latent space to data space. However, they have a significant limitation: they don’t provide a way to go backwards – to encode a real data point \\(x\\) back into the latent space \\(z\\).\nVariational Autoencoders (VAEs) address this limitation by learning both directions simultaneously:\nWe saw this kind of encoder-decoder structure before when we discussed (non-variational) Autoencoders, but only in the context of deterministic mappings for dimension reduction. Now, we will extend this idea to probabilistic mappings that allow us to both generate new data and infer latent representations with explicit likelihoods.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variational Autoencoders (VAEs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/VAEs.html#learning-objectives",
    "href": "part2/gen_models/VAEs.html#learning-objectives",
    "title": "12  Variational Autoencoders (VAEs)",
    "section": "13.1 Learning Objectives",
    "text": "13.1 Learning Objectives\n\nUnderstand the architecture and motivation behind Variational Autoencoders\nLearn about the Evidence Lower Bound (ELBO) and why it’s used for training\nExplore the role of the KL divergence regularization term\nBuild and train a simple VAE on 2D data\nVisualize the learned latent space and understand the encoder-decoder relationship\nCompare VAEs to GANs and understand their respective strengths\n\n\n\nShow Code\n# Setup and Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom dataclasses import dataclass\nfrom typing import List\n\n# Optional interactive widgets\ntry:\n    from ipywidgets import interact, FloatSlider, IntSlider\n    widgets_available = True\nexcept Exception:\n    interact = None\n    FloatSlider = IntSlider = None\n    widgets_available = False\n\n# Import shared utilities from the local module\nfrom gen_models_utilities import (\n    device, create_ring_gaussians,\n    make_loader, compute_diversity_metric, plot_model_diagnostics, plot_latent_interpolation\n)\n\nplt.style.use('seaborn-v0_8-muted')\nsns.set_context('talk')\n\nprint(f\"Using device: {device}\")\n\n\nUsing device: cuda",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variational Autoencoders (VAEs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/VAEs.html#what-is-a-variational-autoencoder",
    "href": "part2/gen_models/VAEs.html#what-is-a-variational-autoencoder",
    "title": "12  Variational Autoencoders (VAEs)",
    "section": "13.2 What is a Variational Autoencoder?",
    "text": "13.2 What is a Variational Autoencoder?\nTo understand VAEs, let’s start with regular autoencoders. An autoencoder is a neural network trained to compress data into a lower-dimensional representation (encoding) and then reconstruct the original data from that compressed form (decoding). The key insight is that if we can successfully compress and reconstruct data, the compressed representation must capture the essential features of the original dataset, in a possibly more interpretable form.\nHowever, regular autoencoders have a problem: the latent space can be irregular and discontinuous. Two similar data points might end up far apart in latent space, making it hard to generate new samples or interpolate smoothly. This is not a problem as far as a regular Autoencoder is concerned, since its only goal in life is to reconstruct the training data well (it doesn’t care about interpolation or what the latent space might mean).\nVariational Autoencoders solve this by imposing structure on the latent space. Instead of encoding a data point \\(x\\) to a single point \\(z\\), a VAE encodes it to a probability distribution over \\(z\\). This is typically a Gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\\[\nq_\\phi(z|x) = \\mathcal{N}(z; \\mu_\\phi(x), \\sigma_\\phi^2(x))\n\\]\nwhere \\(\\phi\\) represents the encoder parameters (typically those of a neural network).\nThe decoder then samples from this distribution to reconstruct the data: \\[p_\\theta(x|z) = \\mathcal{N}(x; \\mu_\\theta(z), \\sigma_\\theta^2(z))\\]\nwhere \\(\\theta\\) represents the decoder parameters.\nWith this, we actually have two types of probabilistic models, one that can compute the likelihood of a data point (\\(x\\)) given a latent code (\\(z\\)), and a second that can infer a distribution over latent codes (\\(z\\)) given a data point (\\(x\\)).\n\n13.2.1 The ELBO: Why VAEs Work\nVAEs are trained by maximizing the Evidence Lower Bound (ELBO), which decomposes into two terms:\n\\[\n\\text{ELBO} = \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_\\text{Reconstruction term} - \\underbrace{D_{KL}(q_\\phi(z|x) \\| p(z))}_\\text{Regularization term}\n\\]\nLet’s break this down:\n\nReconstruction term: This measures how well the decoder can reconstruct the original data from the latent representation. It’s similar to a standard autoencoder loss.\nKL divergence term: This regularizes the latent space by encouraging \\(q_\\phi(z|x)\\) to be close to a prior distribution \\(p(z)\\) (typically a standard normal \\(\\mathcal{N}(0, I)\\)). This ensures the latent space is well-behaved and continuous.\n\nThe KL term has a nice closed-form solution when both distributions are Gaussian:\n\\[\nD_{KL}(q_\\phi(z|x) \\| p(z)) = \\frac{1}{2}\\sum_{i=1}^{d} (\\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1)\n\\]\nLet’s step back a moment and think about what this term is encouraging. \\(p(z)\\) is fixed, so the only thing we are changing when we optimize the KL divergence is \\(q_\\phi\\) (via the Neural Network weights \\(\\phi\\)). What is \\(\\phi\\) trying to do? Given an \\(x\\) from our training data, \\(\\phi\\) will transform it to \\(z\\) using the encoder’s neural network, and this will constitute a distribution over \\(z\\) influenced by \\(x\\), specifically, \\(z \\sim \\mathcal{N}(z; \\mu_\\phi(x), \\sigma_\\phi^2(x))\\). So this will produce a tiny little Gaussian blob in \\(z\\) for that given \\(x\\), and the KL divergence will penalize this \\(q_\\phi(z|x)\\) from being “far” (in a KL sense) from \\(p(z)\\). However, this doesn’t mean that \\(q_\\phi(z|x)\\) will look at all like \\(p(z)\\), rather, \\(p(z)\\) is just sort of gently encouraging \\(q_\\phi(z|x)\\) to remaining close by to \\(p(z)\\). In fact, \\(q_\\phi(z|x)\\) can’t look too much like \\(p(z)\\) because then \\(q_\\phi(z|x)\\) would be conditionally independent from \\(x\\) and this would make it hard for the decoder (\\(p_\\theta(x|z)\\)) to reproduce \\(x\\) well.\n\n\n13.2.2 The Reparameterization Trick\nTo train VAEs with backpropagation, we need gradients to flow through the sampling operation. The reparameterization trick achieves this by rewriting the sampling as:\n\\[\nz = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n\\]\nNow the randomness is moved to \\(\\epsilon\\), and gradients can flow through \\(\\mu\\) and \\(\\sigma\\).",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variational Autoencoders (VAEs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/VAEs.html#building-a-simple-vae",
    "href": "part2/gen_models/VAEs.html#building-a-simple-vae",
    "title": "12  Variational Autoencoders (VAEs)",
    "section": "13.3 Building a Simple VAE",
    "text": "13.3 Building a Simple VAE\nLet’s implement a VAE for our familiar 2D ring dataset. We’ll keep the architecture simple so we can visualize and understand what’s happening.\n\n\nShow Code\n# Load the ring dataset\nX_ring, y_ring = create_ring_gaussians(n_samples=2000)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nsc = ax.scatter(X_ring[:, 0], X_ring[:, 1], c=y_ring, cmap='tab10', s=15, alpha=0.6)\nplt.colorbar(sc, label='Mode index')\nax.set_title('Target: Ring of Gaussians')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.axis('equal')\nax.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n13.3.1 VAE Architecture\nOur VAE will consist of two networks:\n\nEncoder: Takes 2D data and outputs the parameters (\\(\\mu\\), \\(\\log\\sigma^2\\)) of a Gaussian distribution in latent space\nDecoder: Takes a latent code \\(z\\) and reconstructs the 2D data point\n\nFor this simple example, we’ll use 2D latent space so we can visualize it directly.\n\nclass Encoder(nn.Module):\n    \"\"\"Encoder network that maps data to latent distribution parameters.\"\"\"\n    def __init__(self, x_dim: int = 2, hidden_dim: int = 128, z_dim: int = 2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(x_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        # Output mean and log-variance of the latent distribution\n        self.mu_layer = nn.Linear(hidden_dim, z_dim)\n        self.logvar_layer = nn.Linear(hidden_dim, z_dim)\n    \n    def forward(self, x: torch.Tensor):\n        h = self.net(x)\n        mu = self.mu_layer(h)\n        logvar = self.logvar_layer(h)\n        return mu, logvar\n\n\nclass Decoder(nn.Module):\n    \"\"\"Decoder network that maps latent codes back to data space.\"\"\"\n    def __init__(self, z_dim: int = 2, hidden_dim: int = 128, x_dim: int = 2):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(z_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, x_dim)\n        )\n    \n    def forward(self, z: torch.Tensor):\n        return self.net(z)\n\n\nclass VAE(nn.Module):\n    \"\"\"Variational Autoencoder combining encoder and decoder.\"\"\"\n    def __init__(self, x_dim: int = 2, z_dim: int = 2, hidden_dim: int = 128):\n        super().__init__()\n        self.encoder = Encoder(x_dim, hidden_dim, z_dim)\n        self.decoder = Decoder(z_dim, hidden_dim, x_dim)\n        self.z_dim = z_dim\n    \n    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor):\n        \"\"\"Reparameterization trick: z = mu + sigma * epsilon\"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    \n    def forward(self, x: torch.Tensor):\n        # Encode\n        mu, logvar = self.encoder(x)\n        # Sample from latent distribution\n        z = self.reparameterize(mu, logvar)\n        # Decode\n        x_recon = self.decoder(z)\n        return x_recon, mu, logvar, z\n    \n    def encode(self, x: torch.Tensor):\n        \"\"\"Encode data to latent space (returns mean of distribution).\"\"\"\n        mu, _ = self.encoder(x)\n        return mu\n    \n    def decode(self, z: torch.Tensor):\n        \"\"\"Decode latent codes to data space.\"\"\"\n        return self.decoder(z)\n\n\n\n13.3.2 The VAE Loss Function\nThe VAE loss combines reconstruction error and KL divergence. For reconstruction, we’ll use mean squared error (MSE), which corresponds to assuming a Gaussian likelihood with a mean at the decoder value (\\(\\hat{x} = \\mu_\\theta(z)\\)) and a variance \\(\\sigma_\\theta^2\\) that could be output by the decoder:\n\\[\n\\mathcal{L} = \\underbrace{\\frac{1}{2\\sigma_\\theta^2}\\|x - \\hat{x}\\|^2}_\\text{Reconstruction} + \\underbrace{D_{KL}(q_\\phi(z|x) \\| p(z))}_\\text{KL divergence}\n\\]\nIn a simplified case, we could set \\(\\sigma_\\theta^2\\) to be a constant value (\\(\\sigma^2\\)) in which case the only job of the decoder would be to output a single vector (the mean value \\(\\hat{x} = \\mu_\\theta(z)\\)) and then the constant \\(\\sigma^2\\) would effectively control the trade-off between reconstruction accuracy and regularization strength.\n\ndef vae_loss(x: torch.Tensor, x_recon: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor, \n             beta: float = 1.0, reconstruction_variance: float = 0.1):\n    \"\"\"\n    Compute VAE loss = Reconstruction loss + beta * KL divergence.\n    \n    Args:\n        x: Original data\n        x_recon: Reconstructed data\n        mu: Mean of latent distribution\n        logvar: Log-variance of latent distribution\n        beta: Weight for KL term (beta=1 is standard VAE, beta&gt;1 is beta-VAE)\n        reconstruction_variance: Assumed variance of reconstruction distribution\n    \"\"\"\n    # Reconstruction loss (MSE scaled by assumed variance)\n    recon_loss = torch.sum((x - x_recon) ** 2, dim=1) / (2 * reconstruction_variance)\n    \n    # KL divergence between q(z|x) and p(z) = N(0,I)\n    # KL(N(mu, sigma^2) || N(0,1)) = 0.5 * sum(mu^2 + sigma^2 - log(sigma^2) - 1)\n    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n    \n    # Total loss (negative ELBO)\n    return (recon_loss + beta * kl_div).mean(), recon_loss.mean(), kl_div.mean()\n\n\n\n13.3.3 Training the VAE\nNow let’s train our VAE on the ring dataset. We’ll track both the reconstruction loss and KL divergence separately to understand how the model learns.\n\n@dataclass\nclass VAEHistory:\n    \"\"\"Training history for VAE.\"\"\"\n    loss: List[float]\n    recon_loss: List[float]\n    kl_loss: List[float]\n    diversity: List[float]\n\n\ndef train_vae(\n    data: np.ndarray,\n    x_dim: int = 2,\n    z_dim: int = 2,\n    hidden_dim: int = 128,\n    batch_size: int = 256,\n    epochs: int = 200,\n    lr: float = 1e-3,\n    beta: float = 1.0,\n    reconstruction_variance: float = 0.1,\n    print_every: int = 50\n):\n    \"\"\"\n    Train a VAE on the provided data.\n    \n    Args:\n        data: Training data (numpy array)\n        x_dim: Dimension of data space\n        z_dim: Dimension of latent space\n        hidden_dim: Hidden layer dimension\n        batch_size: Batch size for training\n        epochs: Number of training epochs\n        lr: Learning rate\n        beta: Weight for KL term (beta-VAE parameter)\n        reconstruction_variance: Assumed variance of reconstruction distribution\n        print_every: Print progress every N epochs\n    \n    Returns:\n        Trained VAE model and training history\n    \"\"\"\n    # Setup data loader\n    loader = make_loader(data, batch_size)\n    \n    # Initialize model and optimizer\n    vae = VAE(x_dim=x_dim, z_dim=z_dim, hidden_dim=hidden_dim).to(device)\n    optimizer = optim.Adam(vae.parameters(), lr=lr)\n    \n    # Training history\n    history = VAEHistory(loss=[], recon_loss=[], kl_loss=[], diversity=[])\n    \n    print(f\"Training VAE for {epochs} epochs...\")\n    print(f\"Parameters: z_dim={z_dim}, beta={beta}, recon_var={reconstruction_variance}, lr={lr}\")\n    \n    vae.train()\n    for epoch in range(epochs):\n        epoch_losses = []\n        epoch_recon = []\n        epoch_kl = []\n        \n        for (batch,) in loader:\n            batch = batch.to(device)\n            \n            # Forward pass\n            x_recon, mu, logvar, z = vae(batch)\n            \n            # Compute loss\n            loss, recon, kl = vae_loss(batch, x_recon, mu, logvar, beta, reconstruction_variance)\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            epoch_losses.append(loss.item())\n            epoch_recon.append(recon.item())\n            epoch_kl.append(kl.item())\n        \n        # Record metrics\n        mean_loss = np.mean(epoch_losses)\n        mean_recon = np.mean(epoch_recon)\n        mean_kl = np.mean(epoch_kl)\n        \n        # Compute diversity (sample from prior and decode)\n        with torch.no_grad():\n            z_samples = torch.randn(2048, z_dim, device=device)\n            x_samples = vae.decode(z_samples)\n            diversity = compute_diversity_metric(x_samples)\n        \n        history.loss.append(mean_loss)\n        history.recon_loss.append(mean_recon)\n        history.kl_loss.append(mean_kl)\n        history.diversity.append(diversity)\n        \n        if (epoch + 1) % print_every == 0 or epoch == 0:\n            print(f\"Epoch {epoch+1:03d}/{epochs} | Loss: {mean_loss:.3f} | \"\n                  f\"Recon: {mean_recon:.3f} | KL: {mean_kl:.3f} | Div: {diversity:.3f}\")\n    return vae, history\n\n\n\nShow Code\n# Train the VAE\nvae, history = train_vae(\n    X_ring,\n    x_dim=2,\n    z_dim=2,\n    hidden_dim=128,\n    epochs=400,\n    batch_size=256,\n    lr=1e-3,\n    beta=1.0,\n    reconstruction_variance=0.1,\n    print_every=50\n)\n\n\nTraining VAE for 400 epochs...\nParameters: z_dim=2, beta=1.0, recon_var=0.1, lr=0.001\nEpoch 001/400 | Loss: 43.550 | Recon: 43.310 | KL: 0.240 | Div: 0.074\nEpoch 001/400 | Loss: 43.550 | Recon: 43.310 | KL: 0.240 | Div: 0.074\nEpoch 050/400 | Loss: 4.296 | Recon: 0.964 | KL: 3.332 | Div: 3.860\nEpoch 050/400 | Loss: 4.296 | Recon: 0.964 | KL: 3.332 | Div: 3.860\nEpoch 100/400 | Loss: 3.996 | Recon: 0.917 | KL: 3.079 | Div: 3.879\nEpoch 100/400 | Loss: 3.996 | Recon: 0.917 | KL: 3.079 | Div: 3.879\nEpoch 150/400 | Loss: 3.904 | Recon: 0.842 | KL: 3.062 | Div: 3.965\nEpoch 150/400 | Loss: 3.904 | Recon: 0.842 | KL: 3.062 | Div: 3.965\nEpoch 200/400 | Loss: 3.813 | Recon: 0.813 | KL: 3.001 | Div: 4.029\nEpoch 200/400 | Loss: 3.813 | Recon: 0.813 | KL: 3.001 | Div: 4.029\nEpoch 250/400 | Loss: 3.797 | Recon: 0.986 | KL: 2.810 | Div: 4.013\nEpoch 250/400 | Loss: 3.797 | Recon: 0.986 | KL: 2.810 | Div: 4.013\nEpoch 300/400 | Loss: 3.734 | Recon: 0.822 | KL: 2.912 | Div: 4.059\nEpoch 300/400 | Loss: 3.734 | Recon: 0.822 | KL: 2.912 | Div: 4.059\nEpoch 350/400 | Loss: 3.773 | Recon: 0.873 | KL: 2.900 | Div: 4.172\nEpoch 350/400 | Loss: 3.773 | Recon: 0.873 | KL: 2.900 | Div: 4.172\nEpoch 400/400 | Loss: 3.758 | Recon: 0.824 | KL: 2.935 | Div: 4.051\nEpoch 400/400 | Loss: 3.758 | Recon: 0.824 | KL: 2.935 | Div: 4.051\n\n\n\n\n13.3.4 Visualizing Training Progress\nLet’s look at how the different loss components evolved during training:\n\n\nShow Code\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nepochs = np.arange(1, len(history.loss) + 1)\n\n# Total loss\naxes[0].plot(epochs, history.loss, color='tab:blue', linewidth=2)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Total Loss (Negative ELBO)')\naxes[0].set_title('Total VAE Loss')\naxes[0].set_yscale('log')\naxes[0].grid(True, alpha=0.3)\n\n# Reconstruction vs KL\naxes[1].plot(epochs, history.recon_loss, label='Reconstruction', linewidth=2, color='tab:orange')\naxes[1].plot(epochs, history.kl_loss, label='KL Divergence', linewidth=2, color='tab:green')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Loss')\naxes[1].set_title('Loss Components')\naxes[1].set_yscale('log')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Diversity\naxes[2].plot(epochs, history.diversity, color='teal', linewidth=2)\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('Sample Variance')\naxes[2].set_title('Sample Diversity')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variational Autoencoders (VAEs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/VAEs.html#exploring-the-learned-latent-space",
    "href": "part2/gen_models/VAEs.html#exploring-the-learned-latent-space",
    "title": "12  Variational Autoencoders (VAEs)",
    "section": "13.4 Exploring the Learned Latent Space",
    "text": "13.4 Exploring the Learned Latent Space\nOne of the most powerful aspects of VAEs is that we can visualize the latent space directly (since we’re using 2D latent codes). Let’s see how the VAE has organized the ring data in latent space.\n\n\nShow Code\n# Encode all data points to latent space\nvae.eval()\nwith torch.no_grad():\n    X_tensor = torch.from_numpy(X_ring).float().to(device)\n    z_encoded = vae.encode(X_tensor).cpu().numpy()\n    \n    # Generate samples from prior\n    z_prior = torch.randn(2000, 2, device=device)\n    x_generated = vae.decode(z_prior).cpu().numpy()\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Original data space\nsc0 = axes[0].scatter(X_ring[:, 0], X_ring[:, 1], c=y_ring, cmap='tab10', s=15, alpha=0.6)\naxes[0].set_title('Original Data Space')\naxes[0].set_xlabel('$x_1$')\naxes[0].set_ylabel('$x_2$')\naxes[0].axis('equal')\naxes[0].grid(True, alpha=0.2)\n\n# Latent space (colored by original mode)\nsc1 = axes[1].scatter(z_encoded[:, 0], z_encoded[:, 1], c=y_ring, cmap='tab10', s=15, alpha=0.6)\naxes[1].set_title('Learned Latent Space')\naxes[1].set_xlabel('$z_1$')\naxes[1].set_ylabel('$z_2$')\naxes[1].axis('equal')\naxes[1].grid(True, alpha=0.2)\n\n# Generated samples\naxes[2].scatter(X_ring[:, 0], X_ring[:, 1], c='lightgray', s=8, alpha=0.3, label='Real')\naxes[2].scatter(x_generated[:, 0], x_generated[:, 1], c='tab:red', s=12, alpha=0.5, label='Generated')\naxes[2].set_title('Generated Samples')\naxes[2].set_xlabel('$x_1$')\naxes[2].set_ylabel('$x_2$')\naxes[2].axis('equal')\naxes[2].grid(True, alpha=0.2)\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n13.4.1 Visualizing \\(q_\\phi(z|x)\\)\nThe encoder learns to map each input data point \\(x\\) to a distribution in the latent space, not just a single point. Let’s pick two points from our dataset and visualize the corresponding Gaussian distributions \\(q_\\phi(z|x)\\) that the encoder has learned for them.\n\n# Select two points from the dataset\npoint_1_idx, point_2_idx = 150, 1050\nx_1 = torch.from_numpy(X_ring[point_1_idx]).float().to(device)\nx_2 = torch.from_numpy(X_ring[point_2_idx]).float().to(device)\n\n# Get the parameters of q(z|x) for these two points\nvae.eval()\nwith torch.no_grad():\n    mu_1, logvar_1 = vae.encoder(x_1.unsqueeze(0))\n    mu_2, logvar_2 = vae.encoder(x_2.unsqueeze(0))\n\n# Create distributions\nstd_1 = torch.exp(0.5 * logvar_1)\ndist_1 = torch.distributions.MultivariateNormal(mu_1.squeeze(), torch.diag(std_1.squeeze().pow(2)))\n\nstd_2 = torch.exp(0.5 * logvar_2)\ndist_2 = torch.distributions.MultivariateNormal(mu_2.squeeze(), torch.diag(std_2.squeeze().pow(2)))\n\n\n\nShow Code\n\n# Create a grid for the latent space plot\nz_range = 3.0\nz_grid_pts = 100\nz1_grid = torch.linspace(-z_range, z_range, z_grid_pts, device=device)\nz2_grid = torch.linspace(-z_range, z_range, z_grid_pts, device=device)\nZ1, Z2 = torch.meshgrid(z1_grid, z2_grid, indexing='xy')\nz_grid_tensor = torch.stack([Z1.flatten(), Z2.flatten()], dim=1)\n\n# Evaluate densities on the grid\nlog_prob_1 = dist_1.log_prob(z_grid_tensor)\nlog_prob_2 = dist_2.log_prob(z_grid_tensor)\nprob_1 = log_prob_1.exp().reshape(z_grid_pts, z_grid_pts).cpu().numpy()\nprob_2 = log_prob_2.exp().reshape(z_grid_pts, z_grid_pts).cpu().numpy()\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(13, 6))\ncolor1, color2 = 'tab:red', 'tab:purple'\n\n# Left plot: Original data space\naxes[0].scatter(X_ring[:, 0], X_ring[:, 1], c='lightgray', s=10, alpha=0.5, label='Dataset')\naxes[0].scatter(x_1[0].cpu(), x_1[1].cpu(), color=color1, s=100, ec='black', label=f'Point 1 (idx {point_1_idx})')\naxes[0].scatter(x_2[0].cpu(), x_2[1].cpu(), color=color2, s=100, ec='black', label=f'Point 2 (idx {point_2_idx})')\naxes[0].set_title('Selected Points in Data Space')\naxes[0].set_xlabel('$x_1$')\naxes[0].set_ylabel('$x_2$')\naxes[0].axis('equal')\naxes[0].grid(True, alpha=0.2)\naxes[0].legend()\n\n# Right plot: Latent space distributions\naxes[1].contour(Z1.cpu().numpy(), Z2.cpu().numpy(), prob_1, colors=color1, linewidths=2)\naxes[1].contour(Z2.cpu().numpy(), Z1.cpu().numpy(), prob_2, colors=color2, linewidths=2)\n# Also plot the full latent space encoding for context\naxes[1].scatter(z_encoded[:, 0], z_encoded[:, 1], c=y_ring, cmap='tab10', s=5, alpha=0.2)\naxes[1].set_title('Learned $q(z|x)$ for Selected Points')\naxes[1].set_xlabel('$z_1$')\naxes[1].set_ylabel('$z_2$')\naxes[1].axis('equal')\naxes[1].grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n13.4.2 Visualizing the Latent Space Manifold\nSince both our data and latent space are 2D, we can create a grid in latent space and see what the decoder produces at each point. This helps us understand how the VAE has organized the latent space.\n\n# Create a grid in latent space\nn_points = 20\nz_range = 2.0\nz1 = np.linspace(-z_range, z_range, n_points)\nz2 = np.linspace(-z_range, z_range, n_points)\nZ1, Z2 = np.meshgrid(z1, z2)\nz_grid = np.stack([Z1.flatten(), Z2.flatten()], axis=1).astype(np.float32)\n\n# Decode the grid\nwith torch.no_grad():\n    z_grid_tensor = torch.from_numpy(z_grid).to(device)\n    x_grid = vae.decode(z_grid_tensor).cpu().numpy()\n\n\n\nShow Code\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(13, 6))\n\n# Latent space grid\naxes[0].scatter(z_grid[:, 0], z_grid[:, 1], c='tab:blue', s=20, alpha=0.5)\naxes[0].scatter(z_encoded[:, 0], z_encoded[:, 1], c='lightgray', s=3, alpha=0.3)\naxes[0].set_title('Grid in Latent Space')\naxes[0].set_xlabel('$z_1$')\naxes[0].set_ylabel('$z_2$')\naxes[0].axis('equal')\naxes[0].grid(True, alpha=0.2)\n\n# Corresponding points in data space\naxes[1].scatter(X_ring[:, 0], X_ring[:, 1], c='lightgray', s=8, alpha=0.2, label='Real data')\nsc = axes[1].scatter(x_grid[:, 0], x_grid[:, 1], c=z_grid[:, 0], cmap='viridis', s=30, alpha=0.7, \n                     edgecolors='black', linewidths=0.5, label='Decoded grid')\nplt.colorbar(sc, ax=axes[1], label='$z_1$ coordinate')\naxes[1].set_title('Decoded Grid in Data Space')\naxes[1].set_xlabel('$x_1$')\naxes[1].set_ylabel('$x_2$')\naxes[1].axis('equal')\naxes[1].grid(True, alpha=0.2)\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect of re-weighting the KL Divergence term\n\n\n\nWe can see above that reconstruction_variance essentially sets the variances of the Gaussian outputs for the reconstructed data points. - Mathematically, what does setting this to a small variance imply for the loss function? What about a big variance? - Try modifying the reconstruction_variance above from a small number to a large one, e.g., 0.01 to 5.0. What happens to the output of the VAE? Why is this? (Note: if you go below about 0.1 then you may have to increase the learning rate of number of epochs unless it converges.)",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variational Autoencoders (VAEs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/VAEs.html#comparing-vaes-to-gans-and-ot-models",
    "href": "part2/gen_models/VAEs.html#comparing-vaes-to-gans-and-ot-models",
    "title": "12  Variational Autoencoders (VAEs)",
    "section": "13.5 Comparing VAEs to GANs and OT Models",
    "text": "13.5 Comparing VAEs to GANs and OT Models\nNow that we understand VAEs, let’s compare them to the other generative models we’ve studied:\n\n\n\n\n\n\n\n\n\nAspect\nGAN\nOT GAN\nVAE\n\n\n\n\nArchitecture\nGenerator + Discriminator\nGenerator only\nEncoder + Decoder\n\n\nTraining\nAdversarial (minimax)\nDirect divergence minimization\nMaximize ELBO\n\n\nStability\nCan be unstable\nMore stable\nGenerally stable\n\n\nForward map\nYes (\\(z \\rightarrow x\\))\nYes (\\(z \\rightarrow x\\))\nYes (\\(z \\rightarrow x\\))\n\n\nInverse map\nNo\nNo\nYes (\\(x \\rightarrow z\\))\n\n\nMode coverage\nProne to mode collapse\nGood coverage\nGood coverage\n\n\nInterpretability\nLimited\nLimited\nGood (latent space mirrors \\(p(z)\\))\n\n\n\n\n13.5.1 When to use VAEs:\n\nWhen you need an encoder: If you want to map real data to latent codes (for analysis, interpolation, or compression)\nFor smooth interpolation: The regularized latent space makes interpolation meaningful\nWhen interpretability matters: VAEs often learn more interpretable latent representations\nWhen training stability is important: VAEs are generally easier to train than GANs\n\n\n\n13.5.2 When to use GANs or OT models:\n\nWhen sample quality is paramount: GANs often produce sharper, more realistic samples\nWhen you only need generation: If you don’t need to encode real data\nFor specific domains: GANs can excel at images, videos, and other perceptual data compared to VAEs (at least historically)",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variational Autoencoders (VAEs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/VAEs.html#summary-and-looking-forward",
    "href": "part2/gen_models/VAEs.html#summary-and-looking-forward",
    "title": "12  Variational Autoencoders (VAEs)",
    "section": "13.6 Summary and Looking Forward",
    "text": "13.6 Summary and Looking Forward\nIn this notebook, we explored Variational Autoencoders (VAEs), which provide a principled approach to learning bidirectional mappings between data and latent space:\n\n13.6.1 Key Takeaways:\n\nVAEs learn distributions: Unlike deterministic autoencoders, VAEs encode data as distributions in latent space, enabling smooth interpolation and principled sampling.\nThe ELBO objective: VAEs maximize a lower bound on the data likelihood, balancing reconstruction accuracy with latent space regularization.\nKL divergence regularization: This term ensures the latent space is well-behaved (close to a standard normal), making sampling and interpolation meaningful.\nBidirectional mapping: VAEs provide both encoding (\\(x \\rightarrow z\\)) and decoding (\\(z \\rightarrow x\\)), enabling applications like representation learning and data analysis.\n\n\n\n13.6.2 Limitations and Extensions:\nWhile VAEs are powerful, they have some limitations: - Blurry samples: The MSE reconstruction loss can lead to averaged, blurry outputs - Posterior collapse: In high dimensions, the KL term can dominate, causing the model to ignore the encoder - Limited expressiveness: The Gaussian assumption may be too restrictive for complex distributions\nModern extensions address these issues (not covered here): - β-VAE: Adjustable β for better disentanglement - Normalizing Flow VAEs: Use more expressive posterior distributions - Vector Quantized VAEs (VQ-VAE): Discrete latent codes for sharper reconstructions - Hierarchical VAEs: Multiple layers of latent variables for richer representations\n\n\n13.6.3 Next Steps:\nVAEs created an important step beyond GAN models in that they allowed us to model probability distributions over both the output data samples from the generator \\(p_\\theta(z|x)\\) but also the possible latent codes via an encoder \\(q_\\phi(z|x)\\). However, the VAE was only able to predict the latent codes using an approximation method (variational inference) by assuming a \\(q\\) and minimizing the ELBO.\nWhat if it were possible to learn both \\(p(x|z)\\) and \\(q(z|x)\\) exactly? In the next notebook, we’ll explore Normalizing Flows, which provide another way to learn bidirectional mappings with exact likelihood computation. Unlike VAEs, flows are deterministic and invertible, allowing for exact inference without variational approximations. This makes them particularly useful when we need precise likelihood estimates or want to avoid the approximation errors inherent in VAEs.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Variational Autoencoders (VAEs)</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/normalizing_flows.html",
    "href": "part2/gen_models/normalizing_flows.html",
    "title": "13  Normalizing Flows",
    "section": "",
    "text": "13.1 Learning Objectives\nIn the previous chapter we trained a Variational Autoencoder (VAE) to learn a latent representation that could reconstruct and generate samples. VAEs rely on a probabilistic encoder-decoder pair and optimize the Evidence Lower Bound (ELBO). In this notebook we take a different perspective: we build models that exactly transform a simple base distribution (e.g., a Gaussian) into our target data distribution using a sequence of invertible mappings known as normalizing flows.\nWe will reuse the same 2D ring-of-Gaussians dataset that we have been using in prior notebooks so far, so that we can compare models directly and focus on the geometric intuition behind these transformations.\nShow Code\n# Setup and Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions import MultivariateNormal, Normal\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\n\ntry:\n    from ipywidgets import interact, FloatSlider, IntSlider\n    widgets_available = True\nexcept Exception:\n    interact = None\n    FloatSlider = IntSlider = None\n    widgets_available = False\n\nfrom gen_models_utilities import (\n    device,\n    create_ring_gaussians,\n    make_loader,\n    compute_diversity_metric\n)\n\nplt.style.use('seaborn-v0_8-muted')\nsns.set_context('talk')\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\nprint(f'Using device: {device}')\n\n\nUsing device: cuda",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/normalizing_flows.html#learning-objectives",
    "href": "part2/gen_models/normalizing_flows.html#learning-objectives",
    "title": "13  Normalizing Flows",
    "section": "",
    "text": "Understand how normalizing flows differ from VAEs in the way they model probability densities\nApply the change-of-variables formula to relate base and transformed distributions\nExperiment with invertible transformations in 1D and 2D to build intuition\nImplement and train a RealNVP-style normalizing flow on the ring dataset\nSummarize the advantages and limitations of flows and motivate continuous or stochastic extensions",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/normalizing_flows.html#vaes-vs.-normalizing-flows",
    "href": "part2/gen_models/normalizing_flows.html#vaes-vs.-normalizing-flows",
    "title": "13  Normalizing Flows",
    "section": "13.2 VAEs vs. Normalizing Flows",
    "text": "13.2 VAEs vs. Normalizing Flows\nVAEs approximate the data likelihood by maximizing an evidence lower bound, balancing a reconstruction term with a KL regularizer on the latent posterior. Normalizing flows instead learn an exactly normalizable density by composing invertible transformations that map a simple base distribution to the target data distribution using the change-of-variables formula.\nThe key trade-off is between tractable posterior inference (VAE) and exact likelihood modeling with potentially sharper samples (flow). Both rely on simple base distributions, but flows must ensure every transformation remains invertible with a tractable Jacobian determinant.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/normalizing_flows.html#key-idea-the-change-of-variables-formula",
    "href": "part2/gen_models/normalizing_flows.html#key-idea-the-change-of-variables-formula",
    "title": "13  Normalizing Flows",
    "section": "13.3 Key Idea: The Change-of-Variables Formula",
    "text": "13.3 Key Idea: The Change-of-Variables Formula\nSuppose we want to transform a simple base random variable \\(z\\) with known density \\(p_Z(z)\\) into a new variable \\(x = f(z)\\). If \\(f\\) is invertible and differentiable, then the density of \\(x\\) is given by the Change-of-Variables formula:\n\\[\np_X(x) = p_Z(f^{-1}(x)) \\left\\lvert \\det \\frac{\\partial f^{-1}(x)}{\\partial x} \\right\\rvert = p_Z(z) \\left\\lvert \\det \\frac{\\partial f(z)}{\\partial z} \\right\\rvert^{-1}.\n\\]\nThis formula is for a single function \\(f\\). In Normalizing Flows, the idea is to instead construct \\(f\\) as a composition of many simple, easily invertible transformations, each of which can be computed using the Change-of-Variables formula above. If we can do this, then computing the log-density reduces to summing the log-determinants of the Jacobians of those transformations. Let’s ground this idea in an accessible one-dimensional example before moving to higher dimensions.\nSpecifically, we will consider first a simple tanh-based transformation, which is both invertible and differentiable. The forward function \\(f\\) and its inverse \\(f^{-1}\\) are given by: \\[\nf(z) = \\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}, \\quad f^{-1}(x) = \\tanh^{-1}(x) = \\frac{1}{2} \\ln\\left(\\frac{1+x}{1-x}\\right).\n\\]\nLet’s see what this transformation does to a simple base distribution, such as a standard normal.\n\n# 1D change-of-variables example with a tanh transform\nnum_samples = 10_000\nbase_normal = Normal(loc=torch.tensor(0.0, device=device), scale=torch.tensor(1.0, device=device))\n\n# We can compute all of the below functions analytically for a given simple transformation:\ndef forward_transform(u: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Smooth, monotonic transform mapping R -&gt; (-1, 1).\"\"\"\n    return torch.tanh(u)\n\ndef inverse_transform(y: torch.Tensor) -&gt; torch.Tensor:\n    clipped = torch.clamp(y, min=-0.999_999, max=0.999_999)\n    return 0.5 * torch.log((1 + clipped) / (1 - clipped))\n\ndef log_abs_det_jacobian(y: torch.Tensor) -&gt; torch.Tensor:\n    return -torch.log1p(-y.pow(2))\n\nwith torch.no_grad():\n    z_samples = base_normal.sample((num_samples,))\n    x_samples = forward_transform(z_samples)\n    x_grid = torch.linspace(-0.999, 0.999, 500, device=device)\n    z_grid = inverse_transform(x_grid)\n    log_px = base_normal.log_prob(z_grid) + log_abs_det_jacobian(x_grid)\n\n\n\nShow Code\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\naxes[0].hist(x_samples.cpu().numpy(), bins=80, density=True, alpha=0.65, color='tab:blue', label='Samples')\naxes[0].plot(x_grid.cpu().numpy(), torch.exp(log_px).cpu().numpy(), color='black', linewidth=2, label='Analytic density')\naxes[0].set_title('Density after tanh transform')\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('p(x)')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(z_samples.cpu().numpy(), x_samples.cpu().numpy(), '.', alpha=0.3, markersize=3, color='tab:orange')\naxes[1].set_title('Mapping z → x')\naxes[1].set_xlabel('z ~ N(0,1)')\naxes[1].set_ylabel('x = tanh(z)')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this simple transformation, we can see how it modifies the density of a standard normal distribution, and also how the tranformation is invertible, since we can go both ways. We can draw a sample from \\(z\\) and pass it through the forward transformation to get \\(x\\), and we can also take a value of \\(x\\) and map it back to \\(z\\) using the inverse transformation. We can see from the mapping that the transformation is indeed invertible.\n\n13.3.1 Building Intuition Through Simple Transforms\nBefore we move onto more complex normalizing flows, it will help us to build geometric intuition for individual transformations. We will:\n\nLook at several common 1D functions that appear in flow architectures to understand how they bend the real line.\nApply those transforms to a base Gaussian and inspect how the probability density changes.\nExplore higher-dimensional analogues (rotations, couplings, planar flows) to see how they reshape contours while keeping the transformation invertible.\n\nKeep the change-of-variables formula in mind: steep regions of a transform squeeze probability mass, while flat regions stretch it.\n\n# Visualizing common 1D flow transformations before touching the density\nimport torch.nn.functional as F\n\nz_plot = torch.linspace(-3.0, 3.0, 400, device=device)\n\ntransform_fns = {\n    \"Identity\": lambda z: z,\n    \"Affine (scale=2.5, shift=1)\": lambda z: 2.5 * z + 1.0,\n    \"Sigmoid\": torch.sigmoid,\n    \"Tanh\": torch.tanh,\n    \"Leaky ReLU (α=0.1)\": lambda z: torch.where(z &gt;= 0, z, 0.1 * z),\n    \"Softplus shift\": lambda z: F.softplus(z) - np.log(2.0),\n}\n\n\n\nShow Code\nfig, ax = plt.subplots(figsize=(9, 5))\nfor label, fn in transform_fns.items():\n    ax.plot(z_plot.cpu().numpy(), fn(z_plot).cpu().numpy(), linewidth=2, label=label)\nax.set_title('How candidate flow transforms warp the real line')\nax.set_xlabel('Input z')\nax.set_ylabel('Transformed output f(z)')\nax.grid(True, alpha=0.3)\nax.legend(loc='upper left', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nNote how in the above all warpings are monotonic functions, ensuring invertibility. Now let’s see how they modify a simple Gaussian base density.\n\n\nShow Code\n# Applying 1D transforms to a Gaussian base distribution\nsamples_1d = 6_000\nbase_1d = Normal(loc=torch.tensor(0.0, device=device), scale=torch.tensor(1.0, device=device))\n\ndef derivative(label: str, z: torch.Tensor) -&gt; torch.Tensor:\n    if label == \"Identity\":\n        return torch.ones_like(z)\n    if label.startswith(\"Affine\"):\n        return torch.full_like(z, 2.5)\n    if label == \"Sigmoid\":\n        s = torch.sigmoid(z)\n        return s * (1 - s)\n    if label == \"Tanh\":\n        t = torch.tanh(z)\n        return 1 - t.pow(2)\n    if label.startswith(\"Leaky\"):\n        return torch.where(z &gt;= 0, torch.ones_like(z), torch.full_like(z, 0.1))\n    if label == \"Softplus shift\":\n        return torch.sigmoid(z)\n    raise ValueError(f\"Unknown transform: {label}\")\n\nwith torch.no_grad():\n    z_samples = base_1d.sample((samples_1d,))\n    transformed_samples = {}\n    log_abs_det = {}\n    for label, fn in transform_fns.items():\n        y = fn(z_samples)\n        transformed_samples[label] = y.cpu().numpy()\n        jac = derivative(label, z_samples)\n        log_abs_det[label] = torch.log(jac.abs() + 1e-8).cpu().numpy()\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\naxes = axes.flat\nfor ax, (label, values) in zip(axes, transformed_samples.items()):\n    ax.hist(values, bins=80, density=True, alpha=0.7, color='tab:blue')\n    ax.set_title(f\"{label}\\n⟨log|f'|⟩ = {log_abs_det[label].mean():.2f}\")\n    ax.set_xlabel('Transformed value')\n    ax.set_ylabel('Density')\n    ax.grid(True, alpha=0.25)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n13.3.2 More Complex Transforms\nOK, so far so good, and the 1D change of variables is fairly intuitive. But how do we extend this to higher dimensions? For higher dimensions, the change-of-variables formula generalizes to: \\[\np_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left\\lvert \\det \\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right\\rvert = p_Z(\\mathbf{z}) \\left\\lvert \\det \\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}} \\right\\rvert^{-1}.\n\\]\nwhere \\(\\mathbf{x}, \\mathbf{z} \\in \\mathbb{R}^D\\) are now \\(D\\)-dimensional vectors, and the Jacobian (J) \\(\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}\\) is a \\(D \\times D\\) matrix of partial derivatives: \\[\n\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}} = \\begin{bmatrix}\n\\frac{\\partial f_1(\\mathbf{z})}{\\partial z_1} & \\frac{\\partial f_1(\\mathbf{z})}{\\partial z_2} & \\cdots & \\frac{\\partial f_1(\\mathbf{z})}{\\partial z_D} \\\\\n\\frac{\\partial f_2(\\mathbf{z})}{\\partial z_1} & \\frac{\\partial f_2(\\mathbf{z})}{\\partial z_2} & \\cdots & \\frac{\\partial f_2(\\mathbf{z})}{\\partial z_D} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_D(\\mathbf{z})}{\\partial z_1} & \\frac{\\partial f_D(\\mathbf{z})}{\\partial z_2} & \\cdots & \\frac{\\partial f_D(\\mathbf{z})}{\\partial z_D}\n\\end{bmatrix}\n\\]\nSo, if we unpack the new formula, we see that the density at a point \\(\\mathbf{x}\\) depends on the density at the corresponding point \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\) in the base distribution, scaled by the volume change induced by the transformation \\(f\\) at that point, as captured by the determinant of the Jacobian. As such, in normalizing flows, most of the effort goes into designing transformations \\(f\\) (or compositions of transformations) that are:\n\nInvertible: So we can compute \\(f^{-1}(\\mathbf{x})\\).\nTractable Jacobian Determinant: So we can efficiently compute \\(\\log|\\det \\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}|\\).\nExpressive: So that the composition of multiple such transformations can model complex target distributions.\n\n\n13.3.2.1 A Refresher on Determinants\nBefore going into specific transformations or architectures, let’s briefly refresh our understanding of determinants, in general.1 We will see that many of the common transformations in Normalizing Flows are designed to take advantage of one or more of the following properties of determinants:\n1 For some common matrix identities, I can recommend reviewing the Matrix Cookbook\nProduct Rule: For two square matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) of the same size, \\(\\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A}) \\cdot \\det(\\mathbf{B})\\). This means that for a composition of functions \\(f = f_2 \\circ f_1\\), the Jacobian determinant is the product of the individual determinants: \\(\\det(J_f) = \\det(J_{f_2}) \\cdot \\det(J_{f_1})\\).\nMatrix Inversion: The determinant of the inverse of a matrix is the reciprocal of the determinant: \\(\\det(\\mathbf{A}^{-1}) = 1/\\det(\\mathbf{A})\\). This is useful when computing the density of the transformed variable using the inverse transformation.\nEigenvalues: The determinant of a matrix can also be computed as the product of its eigenvalues. This property is sometimes used in flow architectures that involve spectral methods or transformations based on eigen-decompositions.\nTriangular Matrices: The determinant of a triangular matrix (upper or lower) is simply the product of its diagonal elements: \\(\\det J = \\prod_{i=1}^d \\frac{\\partial f_i}{\\partial x_i}\\) and the log-determinant is just the sum of those diagonal terms. This property is often exploited in flow architectures to design transformations with triangular Jacobians, making the determinant computation efficient. This is because, for a matrix of size \\(D \\times D\\), computing the determinant directly is \\(O(D^3)\\), while for triangular matrices this reduces to \\(O(D)\\). We can see that a diagonal matrix is a special case of a triangular matrix, as thus has the same formula for the determinant.\nBlock Matrices: For block diagonal matrices, the determinant is the product of the determinants of the blocks. This allows for designing transformations that operate on subsets of dimensions independently, simplifying the Jacobian structure.\nOrthogonal Matrices: For orthogonal matrices (where \\(\\mathbf{A}^T\\mathbf{A} = \\mathbf{I}\\)), the determinant is either +1 or -1. This property is useful in designing volume-preserving transformations, since we can make sure that the determinant of the Jacobian is one, and thus simplify density computations or ensure that volume doesn’t change. This is also useful in special matrices like Permutation matrices, which are orthogonal, and we will see later are useful for shuffling dimensions in coupling layers.\nVolume Preserving Matrices: Some transformations are designed to be volume-preserving, meaning that they do not change the volume of regions in space. For such transformations, the determinant of the Jacobian is one. These will have important analogues to continuous flows which we will see in later noterbooks, where this property corresponds to divergence-free vector fields (\\(\\nabla \\cdot f = 0\\)).\nLow-Rank Updates: For matrices that can be expressed as a low-rank update to another matrix, the determinant can be computed using the matrix determinant lemma. This is particularly useful in certain flow architectures that involve low-rank transformations, such as planar flows. Specifically, if \\(\\mathbf{A}\\) is an invertible matrix and \\(\\mathbf{u}, \\mathbf{v}\\) are column vectors, then: \\(\\det(\\mathbf{A} + \\mathbf{u}\\mathbf{v}^T) = \\det(\\mathbf{A}) \\cdot (1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{u})\\). In the special case where \\(\\mathbf{A}\\) is the identity matrix, this simplifies to: \\(\\det(\\mathbf{I} + \\mathbf{u}\\mathbf{v}^T) = 1 + \\mathbf{v}^T \\mathbf{u}\\).\n\nWith these properties in mind, we can now explore some common transformations used in normalizing flows.\n\n\n13.3.2.2 Affine Transforms\nThese are the simplest layers, applying a linear transformation followed by a shift: \\(f(\\mathbf{z}) = \\mathbf{A}\\mathbf{z} + \\mathbf{b}\\). The Jacobian is simply \\(\\mathbf{A}\\), so its log-determinant is \\(\\log|\\det \\mathbf{A}|\\). If \\(\\mathbf{A}\\) is a rotation matrix, the transform is volume-preserving (\\(\\log|\\det|=0\\)). If \\(\\mathbf{A}\\) is diagonal, it applies axis-aligned scaling.\n\n\n13.3.2.3 Planar Flows\nThese layers stretch and compress the space along a specific direction (a hyperplane). The transformation is \\(f(\\mathbf{z}) = \\mathbf{z} + \\mathbf{u}h(\\mathbf{w}^T\\mathbf{z} + b)\\), where \\(\\mathbf{u}, \\mathbf{w}\\) are vectors, \\(b\\) is a scalar, and \\(h\\) is a smooth nonlinearity like \\(\\tanh\\). This creates contractions or expansions perpendicular to the hyperplane \\(\\mathbf{w}^T\\mathbf{z} + b=0\\). Because this is a low-rank update, the log-determinant can be efficiently computed as \\(\\log|1 + \\mathbf{u}^T\\mathbf{v}(\\mathbf{z})|\\), where \\(\\mathbf{v}(\\mathbf{z}) = h'(\\mathbf{w}^T\\mathbf{z}+b)\\mathbf{w}\\).\n\n\n13.3.2.4 Radial Flows\nThese layers modify the space around a reference point \\(\\mathbf{z}_0\\): \\(f(\\mathbf{z}) = \\mathbf{z} + \\beta \\frac{\\mathbf{z}-\\mathbf{z}_0}{\\alpha+r}\\), where \\(r=||\\mathbf{z}-\\mathbf{z}_0||\\) is a radius from the reference point and \\(\\alpha &gt; 0, \\beta \\in \\R\\) are fixed or learned scalars parameterizing the function. This can create or fill holes in the distribution around \\(\\mathbf{z}_0\\) by pushing or pulling points in \\(\\mathbf{z}\\) toward/away from \\(\\mathbf{z}_0\\). Its Jacobian is given by: \\[\n\\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}} = \\mathbf{I} + h(r)\\mathbf{I} + h'(r) \\frac{(\\mathbf{z}-\\mathbf{z}_0)(\\mathbf{z}-\\mathbf{z}_0)^T}{r}\n\\]\nwhere \\(h(r) = \\frac{\\beta}{\\alpha + r}\\) and \\(h'(r) = -\\frac{\\beta}{(\\alpha + r)^2}\\). As in planar flows, this is a low-rank update, so we can use the matrix determinant lemma to compute the determinant efficiently: \\[\n\\det \\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}} = (1 + h(r))^{D-1} (1 + h(r) + h'(r) r) = \\left(1 + \\frac{\\beta}{\\alpha + r}\\right)^{D-1} \\left(1 + \\frac{\\beta \\alpha}{(\\alpha + r)^2}\\right)\n\\]\n\n\n13.3.2.5 Affine Coupling Layers\nWe are not only restricted to transforming the entire base distribution in one function. We can also split the base distribution into two parts, \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\). One part is used to parameterize an affine transformation on the other:\n\\[\n\\begin{aligned}\n\\mathbf{y}_1 &= \\mathbf{z}_1 \\\\\n\\mathbf{y}_2 &= \\mathbf{z}_2 \\odot \\exp(s(\\mathbf{z}_1)) + t(\\mathbf{z}_1)\n\\end{aligned}\n\\]\nHere, \\(s\\) (scale) and \\(t\\) (translate) are neural networks. Let’s assume for this simple example that \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\) are each half of the dimensions of \\(\\mathbf{z}\\). The inverse transformation is straightforward: \\[\n\\begin{aligned}\n\\mathbf{z}_1 &= \\mathbf{y}_1 \\\\\n\\mathbf{z}_2 &= (\\mathbf{y}_2 - t(\\mathbf{y}_1)) \\odot \\exp(-s(\\mathbf{y}_1))\n\\end{aligned}\n\\]\nWe can also see that this approach endowed the Jacobian with a triangular structure: \\[\n\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{z}} = \\begin{bmatrix}\n\\mathbf{I} & \\mathbf{0} \\\\\n\\frac{\\partial \\mathbf{y}_2}{\\partial \\mathbf{z}_1} & \\text{diag}(\\exp(s(\\mathbf{z}_1)))\n\\end{bmatrix}\n\\]\nSince the Jacobian is triangular, this makes its determinant just the product of the diagonal elements: \\(\\sum_j s(\\mathbf{z}_1)_j\\). This is expressive, efficient, and easily invertible. Of course the cost of doing this is that we can only transform half of the dimensions in one layer, leaving the rest unchanged. To address this, we can stack multiple coupling layers, alternating which half of the dimensions are transformed in each layer. Additionally, we can add permutation layers (which are orthogonal and volume-preserving) between coupling layers to shuffle the dimensions, ensuring that all dimensions get transformed over multiple layers.\n\n\nShow Code\n# Exploring 2D invertible transforms that inspire flow layers\nsamples_2d = 4_000\nbase_2d = MultivariateNormal(loc=torch.zeros(2), covariance_matrix=torch.eye(2))\n\nwith torch.no_grad():\n    z2 = base_2d.sample((samples_2d,))\n\n    rotation = torch.tensor([[np.cos(np.deg2rad(45.0)), -np.sin(np.deg2rad(45.0))],\n                               [np.sin(np.deg2rad(45.0)), np.cos(np.deg2rad(45.0))]], dtype=torch.float32)\n    scale = torch.tensor([[0.35, 0.0],[0.0, 2.0]], dtype=torch.float32)\n    shear = torch.tensor([[1.0, 0.6],[0.0, 1.0]], dtype=torch.float32)\n\n    w = torch.tensor([0.6, -0.4])\n    u = torch.tensor([0.4, 0.5])\n    b = torch.tensor(0.1)\n\n    z0 = torch.tensor([1.2, -0.8])\n    alpha = torch.tensor(1.0)\n    beta = torch.tensor(0.8)\n\n    def planar_flow(x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        linear = x @ w + b\n        h = torch.tanh(linear)\n        psi = (1 - torch.tanh(linear).pow(2)).unsqueeze(-1) * w\n        det_term = 1 + (psi * u).sum(dim=1)\n        y = x + u * h.unsqueeze(-1)\n        return y, det_term\n\n    def radial_flow(x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        diff = x - z0\n        r = diff.norm(dim=1, keepdim=True) + 1e-6\n        h = 1.0 / (alpha + r)\n        h_prime = -1.0 / (alpha + r).pow(2)\n        y = x + beta * h * diff\n        det_term = (1 + beta * h.squeeze()) * (1 + beta * h.squeeze() + beta * h_prime.squeeze() * r.squeeze())\n        return y, det_term\n\n    def affine_coupling(x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        x1 = x[:, :1]\n        x2 = x[:, 1:]\n        s = 0.75 * torch.tanh(x1)\n        t = 0.5 * torch.sin(x1)\n        y1 = x1\n        y2 = x2 * torch.exp(s) + t\n        y = torch.cat([y1, y2], dim=1)\n        det_term = torch.exp(s.squeeze())\n        return y, det_term\n\n    transforms_2d = {}\n    log_abs_det_2d = {}\n\n    y_rot = z2 @ rotation.T\n    transforms_2d[\"Rotation 45°\"] = y_rot.numpy()\n    log_abs_det_2d[\"Rotation 45°\"] = np.zeros(samples_2d)\n\n    y_scale = z2 @ scale.T\n    transforms_2d[\"Anisotropic scaling\"] = y_scale.numpy()\n    log_abs_det_2d[\"Anisotropic scaling\"] = np.full(samples_2d, np.log(abs(torch.det(scale).item())))\n\n    y_shear = z2 @ shear.T\n    transforms_2d[\"Shear\"] = y_shear.numpy()\n    log_abs_det_2d[\"Shear\"] = np.zeros(samples_2d)\n\n    y_planar, det_planar = planar_flow(z2)\n    transforms_2d[\"Planar flow\"] = y_planar.numpy()\n    log_abs_det_2d[\"Planar flow\"] = torch.log(det_planar.abs() + 1e-8).numpy()\n\n    y_radial, det_radial = radial_flow(z2)\n    transforms_2d[\"Radial flow\"] = y_radial.numpy()\n    log_abs_det_2d[\"Radial flow\"] = torch.log(det_radial.abs() + 1e-8).numpy()\n\n    y_coupling, det_coupling = affine_coupling(z2)\n    transforms_2d[\"Affine coupling\"] = y_coupling.numpy()\n    log_abs_det_2d[\"Affine coupling\"] = torch.log(det_coupling.abs() + 1e-8).numpy()\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\naxes = axes.flat\nfor ax, (label, points) in zip(axes, transforms_2d.items()):\n    ax.scatter(points[:, 0], points[:, 1], s=6, alpha=0.35)\n    ax.set_title(f\"{label}\\n⟨log|det J|⟩ = {log_abs_det_2d[label].mean():.2f}\")\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.axis('equal')\n    ax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n13.3.3 From Transforms to Complex Flows and Probabilities\nIn practice, we can stack many such layers of different types to build a deep normalizing flow that can model complex target distributions. Each layer contributes to the overall transformation, and we can compute the log-density of the transformed variable by summing the log-determinants of each layer’s Jacobian. Each transform above comes with a Jacobian determinant that tells us how probability mass re-scales. In one dimension, the recipe is \\[\n\\log p_Y(y) = \\log p_Z(f^{-1}(y)) + \\log\\left|\\frac{\\mathrm{d} f^{-1}(y)}{\\mathrm{d} y}\\right| = \\log p_Z(z) - \\log\\left|f'(z)\\right|.\n\\] For multivariate flows with \\(x = f(z)\\), we track the log-determinant at every layer: \\[\n\\log p_X(x) = \\log p_Z(z_0) + \\sum_{\\ell=1}^K \\log\\left|\\det J_{f_\\ell}(z_{\\ell-1})\\right|,\n\\] these terms is why we prefer architectures (like affine coupling, planar, or radial flows) with Jacobians that are either triangular or have closed-form determinants, so that we can avoid very expensive determinant calculations. When training a flow we maximise this exact log-likelihood, and during sampling we simply draw \\(z \\sim p_Z\\) and apply the inverses \\(f_\\ell^{-1}\\) in reverse order.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/normalizing_flows.html#constructing-a-flow-with-coupling-layers-using-realnvp",
    "href": "part2/gen_models/normalizing_flows.html#constructing-a-flow-with-coupling-layers-using-realnvp",
    "title": "13  Normalizing Flows",
    "section": "13.4 Constructing a Flow with Coupling Layers using RealNVP",
    "text": "13.4 Constructing a Flow with Coupling Layers using RealNVP\nOne of the first models for building tractable flows was the RealNVP (Real-valued Non-Volume Preserving) architecture. It splits the vector into two parts, using a coupling layer as we saw previously. This leaves one part unchanged, and uses it to condition an affine transformation on the other part. Because the transformation is triangular, the Jacobian determinant is the product of diagonal entries and is therefore easy to compute. Below implements a RealNVP flow using coupling layers where we swap in each layer which one of the two input dimensions is fixed. For a larger or more complex example with more inputs, these could be selected randomly using permutations, but our simple approach below is just to illustrate the concept.\n\nclass AffineCoupling(nn.Module):\n    \"\"\"RealNVP-style affine coupling layer for 2D inputs.\"\"\"\n    def __init__(self, dim: int, hidden_dim: int, mask: torch.Tensor):\n        super().__init__()\n        self.dim = dim\n        self.register_buffer('mask', mask)\n        self.scale_net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, dim)\n        )\n        self.translate_net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, dim)\n        )\n        nn.init.zeros_(self.scale_net[-1].weight)\n        nn.init.zeros_(self.scale_net[-1].bias)\n        nn.init.zeros_(self.translate_net[-1].weight)\n        nn.init.zeros_(self.translate_net[-1].bias)\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        x_masked = x * self.mask\n        s = self.scale_net(x_masked) * (1 - self.mask)\n        t = self.translate_net(x_masked) * (1 - self.mask)\n        s = torch.tanh(s)\n        y = x_masked + (1 - self.mask) * (x * torch.exp(s) + t)\n        log_det = ((1 - self.mask) * s).sum(dim=1)\n        return y, log_det\n\n    def inverse(self, y: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        y_masked = y * self.mask\n        s = self.scale_net(y_masked) * (1 - self.mask)\n        t = self.translate_net(y_masked) * (1 - self.mask)\n        s = torch.tanh(s)\n        x = y_masked + (1 - self.mask) * ((y - t) * torch.exp(-s))\n        log_det = -((1 - self.mask) * s).sum(dim=1)\n        return x, log_det\n\n\nclass RealNVP(nn.Module):\n    \"\"\"Stack of affine coupling layers implementing a normalizing flow.\"\"\"\n    def __init__(self, dim: int = 2, hidden_dim: int = 128, n_layers: int = 6):\n        super().__init__()\n        masks: List[torch.Tensor] = []\n        for i in range(n_layers):\n            if i % 2 == 0:\n                mask = torch.tensor([1.0, 0.0])\n            else:\n                mask = torch.tensor([0.0, 1.0])\n            masks.append(mask)\n        self.layers = nn.ModuleList([\n            AffineCoupling(dim=dim, hidden_dim=hidden_dim, mask=mask)\n            for mask in masks\n        ])\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        log_det_total = torch.zeros(x.shape[0], device=x.device)\n        z = x\n        for layer in self.layers:\n            z, log_det = layer(z)\n            # Note here how we can just accumulate log-determinants forward:\n            log_det_total = log_det_total + log_det\n        return z, log_det_total\n\n    def inverse(self, z: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        log_det_total = torch.zeros(z.shape[0], device=z.device)\n        x = z\n        for layer in reversed(self.layers):\n            x, log_det = layer.inverse(x)\n            # Likewise, here we can accumulate log-determinants backward:\n            log_det_total = log_det_total + log_det\n        return x, log_det_total\n\n    def inverse_with_intermediates(self, z: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        \"\"\"Inverse transformation that also returns intermediate layer outputs.\"\"\"\n        log_det_total = torch.zeros(z.shape[0], device=z.device)\n        x = z\n        intermediates = [z.clone()]\n        for layer in reversed(self.layers):\n            x, log_det = layer.inverse(x)\n            log_det_total = log_det_total + log_det\n            intermediates.append(x.clone())\n        return x, log_det_total, intermediates\n\n    def log_prob(self, x: torch.Tensor, base_dist: MultivariateNormal) -&gt; torch.Tensor:\n        z, log_det = self.forward(x)\n        log_prob_base = base_dist.log_prob(z)\n        # Now we can just combine base log-probability and log-determinant, which we accumulated earlier\n        return log_prob_base + log_det\n\n    def sample(self, num_samples: int, base_dist: MultivariateNormal) -&gt; torch.Tensor:\n        with torch.no_grad():\n            z = base_dist.sample((num_samples,))\n            x, _ = self.inverse(z)\n        return x\n\n\n13.4.1 Training a Flow on the Ring-of-Gaussians Dataset\nWe now train a small RealNVP model on the same ring dataset used in the VAE notebook. Unlike the VAE, the flow optimizes the exact log-likelihood of the data, so we do not need a separate encoder or a variational bound. The key ingredients are:\n\nA base density \\(p_0(z)\\) (we use a standard 2D Gaussian).\nA flow \\(f_{\\theta}\\) composed of coupling layers.\nAn optimizer to maximize \\(\\log p_X(x) = \\log p_0(f(x)) + \\log \\lvert \\det J_f(x) \\rvert\\).\n\n\n@dataclass\nclass FlowHistory:\n    loss: List[float]\n    log_prob: List[float]\n    diversity: List[float]\n\n\ndef train_flow_model(\n    data: np.ndarray,\n    flow: RealNVP,\n    base_dist: MultivariateNormal,\n    epochs: int = 300,\n    batch_size: int = 256,\n    lr: float = 1e-3,\n    print_every: int = 50\n) -&gt; Tuple[RealNVP, FlowHistory]:\n    flow.train()\n    loader = make_loader(data, batch_size)\n    optimizer = optim.Adam(flow.parameters(), lr=lr)\n    history = FlowHistory(loss=[], log_prob=[], diversity=[])\n\n    for epoch in range(epochs):\n        batch_losses = []\n        batch_log_probs = []\n        for (batch,) in loader:\n            batch = batch.float().to(device)\n            log_prob = flow.log_prob(batch, base_dist)\n            loss = -log_prob.mean()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            batch_losses.append(loss.item())\n            batch_log_probs.append(log_prob.mean().item())\n\n        mean_loss = float(np.mean(batch_losses))\n        mean_log_prob = float(np.mean(batch_log_probs))\n\n        with torch.no_grad():\n            samples = flow.sample(2048, base_dist)\n            diversity = compute_diversity_metric(samples)\n\n        history.loss.append(mean_loss)\n        history.log_prob.append(mean_log_prob)\n        history.diversity.append(diversity)\n\n        if (epoch + 1) % print_every == 0 or epoch == 1:\n            print(f'Epoch {epoch + 1:03d}/{epochs} | Loss: {mean_loss:.3f} | Log p(x): {mean_log_prob:.3f} | Div: {diversity:.3f}')\n\n    flow.eval()\n    return flow, history\n\nX_ring, y_ring = create_ring_gaussians(n_samples=2000)\n\nYou can modify the below RealNVP parameters (number of layers, hidden units) and retrain the flow to see how it affects expressiveness and training time, as we mention for the experiment at the end of the notebook.\n\n# Train the flow on the ring dataset\nflow = RealNVP(dim=2, hidden_dim=128, n_layers=6).to(device)\nbase_dist = MultivariateNormal(\n    loc=torch.zeros(2, device=device),\n    covariance_matrix=torch.eye(2, device=device)\n)\nflow, flow_history = train_flow_model(\n    data=X_ring,\n    flow=flow,\n    base_dist=base_dist,\n    epochs=300,\n    batch_size=256,\n    lr=1e-3,\n    print_every=50\n)\n\nEpoch 002/300 | Loss: 4.634 | Log p(x): -4.634 | Div: 15.833\nEpoch 050/300 | Loss: 2.800 | Log p(x): -2.800 | Div: 4.141\nEpoch 100/300 | Loss: 2.591 | Log p(x): -2.591 | Div: 4.106\nEpoch 150/300 | Loss: 2.430 | Log p(x): -2.430 | Div: 4.275\nEpoch 200/300 | Loss: 2.490 | Log p(x): -2.490 | Div: 4.298\nEpoch 250/300 | Loss: 2.383 | Log p(x): -2.383 | Div: 4.308\nEpoch 300/300 | Loss: 2.401 | Log p(x): -2.401 | Div: 4.274\n\n\n\n\n13.4.2 Monitoring Training Dynamics\nThe total loss is the negative log-likelihood (NLL). Because we optimize the exact density, improvements in the loss correspond directly to better calibrated probabilities. We also track a simple diversity statistic computed from generated samples to monitor coverage.\n\n\nShow Code\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\nepochs_axis = np.arange(1, len(flow_history.loss) + 1)\n\naxes[0].plot(epochs_axis, flow_history.loss, color='tab:blue', linewidth=2)\naxes[0].set_title('Negative log-likelihood')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_yscale('log')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(epochs_axis, flow_history.log_prob, color='tab:green', linewidth=2)\naxes[1].set_title('Mean log p(x)')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Log probability')\naxes[1].grid(True, alpha=0.3)\n\naxes[2].plot(epochs_axis, flow_history.diversity, color='tab:purple', linewidth=2)\naxes[2].set_title('Sample diversity metric')\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('Variance proxy')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n13.4.3 Inspecting the Learned Mapping\nWith a trained flow we can examine three complementary views: the data space, the latent space obtained by applying the forward transformation \\(f(x)\\), and samples generated by pushing base noise through the inverse transformation \\(f^{-1}(z)\\). A well-trained flow should map the multimodal ring distribution onto a near-Gaussian latent distribution while recovering the ring geometry when sampling.\n\n\nShow Code\nflow.eval()\nwith torch.no_grad():\n    X_tensor = torch.from_numpy(X_ring).float().to(device)\n    z_encoded, _ = flow.forward(X_tensor)\n    z_encoded = z_encoded.cpu().numpy()\n    generated = flow.sample(4000, base_dist).cpu().numpy()\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\nsc0 = axes[0].scatter(X_ring[:, 0], X_ring[:, 1], c=y_ring, cmap='tab10', s=15, alpha=0.65)\naxes[0].set_title('Original data space')\naxes[0].set_xlabel('$x_1$')\naxes[0].set_ylabel('$x_2$')\naxes[0].axis('equal')\naxes[0].grid(True, alpha=0.2)\nplt.colorbar(sc0, ax=axes[0], label='Mode index')\n\naxes[1].scatter(z_encoded[:, 0], z_encoded[:, 1], c=y_ring, cmap='tab10', s=15, alpha=0.65)\naxes[1].set_title('Latent space after flow')\naxes[1].set_xlabel('$z_1$')\naxes[1].set_ylabel('$z_2$')\naxes[1].axis('equal')\naxes[1].grid(True, alpha=0.2)\n\naxes[2].scatter(X_ring[:, 0], X_ring[:, 1], c='lightgray', s=10, alpha=0.3, label='Real')\naxes[2].scatter(generated[:, 0], generated[:, 1], c='tab:red', s=15, alpha=0.55, label='Generated')\naxes[2].set_title('Generated samples')\naxes[2].set_xlabel('$x_1$')\naxes[2].set_ylabel('$x_2$')\naxes[2].axis('equal')\naxes[2].grid(True, alpha=0.2)\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n13.4.4 Layer-by-Layer Transformation\nTo better understand how the normalizing flow works, let’s visualize how each coupling layer progressively transforms the base distribution into the target distribution. We’ll sample from the base Gaussian and then apply each layer’s inverse transformation one at a time, coloring the points by their eventual mode assignment (determined by which ring Gaussian they end up closest to).\nThis visualization shows the geometric action of each layer, illustrating how the flow gradually warps the simple base distribution into the complex ring structure. Note in particular how the RealNVP coupling layers show up here: in the first layer only one of the dimensions is changed, while the other remains fixed. In the next layer, the roles are swapped, and so on.\n\n\nShow Code\n# Visualize how each layer transforms the distribution\nflow.eval()\nwith torch.no_grad():\n    # Sample from base distribution\n    z_base = base_dist.sample((2000,))\n    \n    # Get intermediate transformations\n    x_final, _, intermediates = flow.inverse_with_intermediates(z_base)\n    \n    # Also encode the real data to get mode labels for coloring\n    X_tensor = torch.from_numpy(X_ring).float().to(device)\n    z_real, _ = flow.forward(X_tensor)\n    \n    # For each base sample, find the closest real data point to determine its \"mode\"\n    # This is a simple nearest-neighbor approach for visualization\n    from scipy.spatial.distance import cdist\n    distances = cdist(z_base.cpu().numpy(), z_real.cpu().numpy())\n    nearest_indices = distances.argmin(axis=1)\n    colors = y_ring[nearest_indices]\n\n# Create a grid of subplots\nn_layers = len(intermediates)\nn_cols = 4\nn_rows = (n_layers + n_cols - 1) // n_cols\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\naxes = axes.flatten() if n_rows &gt; 1 else [axes] if n_cols == 1 else axes\n\nfor i, intermediate in enumerate(intermediates):\n    ax = axes[i]\n    points = intermediate.cpu().numpy()\n    ax.scatter(points[:, 0], points[:, 1], c=colors, cmap='tab10', s=8, alpha=0.5)\n    \n    if i == 0:\n        ax.set_title('Base Distribution\\n(Latent Space)', fontsize=12, fontweight='bold')\n    elif i == len(intermediates) - 1:\n        ax.set_title(f'After Layer {i}\\n(Final Data Space)', fontsize=12, fontweight='bold')\n    else:\n        ax.set_title(f'After Layer {i}', fontsize=12)\n    \n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.axis('equal')\n    ax.grid(True, alpha=0.2)\n\n# Hide any unused subplots\nfor i in range(n_layers, len(axes)):\n    axes[i].axis('off')\n\nplt.suptitle('Progressive Transformation Through RealNVP Layers', fontsize=16, fontweight='bold', y=1.0)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExperiment: Effect of Flow Depth and Width\n\n\n\nGo back up to where we defined the RealNVP architecture and try it again with the following changes and note the behavior that you see:\n\nVary the number of coupling layers (e.g., 2, 4, 8, 16). What happens to the quality of the learned distribution and the training time?\nWhat does varying the number of hidden units in the scale and translate networks do to the expressiveness and stability of training?\nAs you make the number of coupling layers extremely large (50,100), what do you notice about the training dynamics as well as the effect of individual layers on the movement of the distribution?",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/normalizing_flows.html#advantages-and-limitations-of-normalizing-flows",
    "href": "part2/gen_models/normalizing_flows.html#advantages-and-limitations-of-normalizing-flows",
    "title": "13  Normalizing Flows",
    "section": "13.5 Advantages and Limitations of Normalizing Flows",
    "text": "13.5 Advantages and Limitations of Normalizing Flows\n\n\n\n\n\n\n\nStrengths\nLimitations\n\n\n\n\nExact log-likelihood evaluation enables principled model comparison\nComputing Jacobians can be expensive for high-dimensional data\n\n\nInvertible architecture provides bidirectional mapping without a separate encoder\nDesigning expressive yet tractable transforms is challenging\n\n\nAmenable to gradient-based training with standard optimizers\nCoupling layers may struggle with highly non-local dependencies\n\n\nSamples are sharp because no explicit reconstruction loss is used\nMemory footprint grows with the number of layers\n\n\n\nNormalizing flows shine when calibrated densities and invertible mappings are required, such as uncertainty-aware control or anomaly detection in sensor networks. However, they can become cumbersome in very high dimensions or when local affine transformations are insufficient.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/normalizing_flows.html#summary-and-looking-ahead",
    "href": "part2/gen_models/normalizing_flows.html#summary-and-looking-ahead",
    "title": "13  Normalizing Flows",
    "section": "13.6 Summary and Looking Ahead",
    "text": "13.6 Summary and Looking Ahead\n\nNormalizing flows learn an exact, differentiable transformation between a base distribution and the data, turning density modeling into a sequence of Jacobian adjustments.\nThe change-of-variables formula provides the theoretical backbone, allowing us to evaluate log-likelihoods exactly.\nCoupling layers (as in RealNVP) offer an efficient way to construct expressive flows with tractable inverses and determinants.\nOn the ring dataset, flows match the multimodal structure while keeping a simple latent Gaussian distribution.\n\nIf you ran the last experiment, you may have noticed that as we increase the number of layers, the individual transformations become smaller and more incremental. What would happen if we set the number of layers to infinity? In this case, each individual transformation would become an almost infinitesimal transformation from one probability distribution to another, and the integral over all of those many transformations would get us the flow from \\(z\\) to \\(x\\). This is exactly the idea behind continuous normalizing flows or neural ODE-based flows, where the model evolves samples through a learned vector field. Introducing stochastic dynamics yields stochastic flows that blend diffusion models with flow-based ideas. These extensions retain the change-of-variables logic while trading discrete layers for continuous-time dynamics, setting the stage for the next chapter.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/continuous_flows.html",
    "href": "part2/gen_models/continuous_flows.html",
    "title": "14  From Discrete Transformations to Continuous Flows",
    "section": "",
    "text": "14.1 Learning Objectives\nWe saw in the last chapter how normalizing flows transform simple base distributions into complex target distributions via a sequence of invertible mappings. Each mapping requires computing the determinant of the Jacobian to track how densities change.\nIn continuous normalizing flows, we ask: what happens if we let the number of layers go to infinity while shrinking each individual transformation to an infinitesimal step? That is, rather than a handful of large invertible maps, we stack infinitely many tiny, smooth updates. We will see that taking this limit produces an ordinary differential equation (ODE) whose solution transports probability mass.\nBy the end of this notebook you should be able to:",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>From Discrete Transformations to Continuous Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/continuous_flows.html#learning-objectives",
    "href": "part2/gen_models/continuous_flows.html#learning-objectives",
    "title": "14  From Discrete Transformations to Continuous Flows",
    "section": "",
    "text": "Recognize that a CNF is the infinitesimal limit of a discrete normalizing flow.\nExplain how the log-density evolves through the divergence of a velocity field.\nDerive the continuity equation from the change-of-variables formula.\nSimulate simple continuous flows and verify how densities change.\nConnect CNFs to probability transport ideas and the diffusion models we will meet later.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>From Discrete Transformations to Continuous Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/continuous_flows.html#from-finite-jacobians-to-differential-updates",
    "href": "part2/gen_models/continuous_flows.html#from-finite-jacobians-to-differential-updates",
    "title": "14  From Discrete Transformations to Continuous Flows",
    "section": "14.2 From Finite Jacobians to Differential Updates",
    "text": "14.2 From Finite Jacobians to Differential Updates\nAs we saw last chapter, in a discrete flow the change-of-variables formula is: \\[\n\\log p(x) = \\log p(z_K) = \\log p(z_0) - \\sum_{i=1}^{K} \\log\\left|\\det J_{f_i}(z_{i-1})\\right|.\n\\]\nfor a sequence of \\(K\\) invertible maps \\(f_i\\) transforming a base sample \\(z_0\\) to a target sample \\(x=z_K\\). Here, \\(J_{f_i}\\) is the Jacobian of the \\(i\\)-th transformation.\nIf we instead consider an infinite sequence of infinitesimal transformations, we can think of each layer as a tiny residual step: \\(z_{i+1} = z_i + \\varepsilon v(z_i, t_i)\\) where \\(\\varepsilon\\) is a small step size, \\(v\\) is a velocity field, and \\(t_i\\) is a time index. The change in probability density can then be thought of as an integral over these infinitesimal changes, like particles flowing through a fluid governed by the velocity field. The main learning challenge here is to learn the right velocity field \\(v(z, t)\\) that transports our simple base distribution to our desired target distribution over time, and also to solve this ODE inexpensively.\nConcretely, rather than consider a single step of the change of variables formula, we consider the limit as \\(\\varepsilon \\to 0\\) of many small steps, which gives rise to the instantaneous change of variables formula1: \\[\n\\frac{\\partial}{\\partial t} \\log p(z(t)) = -\\operatorname{tr}\\left( \\nabla_x v(z(t), t) \\right),\n\\]\n1 For the full derivation, see e.g. Chen et al., 2018 Appendix A.where \\(\\nabla_x v\\) is the Jacobian of the velocity field with respect to \\(z\\), and \\(\\operatorname{tr}\\) denotes the trace of a matrix.\nNow, given a vector field \\(v(z, t)\\), we can compute how a base density \\(p(z_0)\\) evolves into the final distribution \\(p(x)=p(z_K)\\) via: \\[\n\\log p(x) = \\log p(z_0) - \\int_{0}^{T} \\operatorname{tr}\\left( \\nabla_x v(z(t), t) \\right) \\, \\mathrm{d}t.\n\\]\nWe can think of this as starting a distribution at time \\(t=0\\) (i.e., at \\(p(z_0)\\)) and then integrating it through the velocity field \\(v(z, t)\\) until time \\(t=T\\), where we hope to recover the distribution of the training data (i.e., \\(p(x)\\)). To find the best velocity field, we can optimize the above expression for \\(\\log p(x)\\) with respect to the parameters of \\(v(z, t)\\) using maximum likelihood. For neural network-based parameterizations of \\(v(z, t)\\) we would parameterize the velocity field via neural network weights and denote this \\(v_\\theta(z,t)\\) where \\(\\theta\\) are the neural network weights. The main requirement we have for the velocity field \\(v(z, t)\\) is that it (and its first derivative) must be Lipschitz continuous so that the resulting ODE has a unique solution (and thus can be in principle inverted as well). We can enforce the Lipschitz continuity of \\(v_\\theta\\) by choosing activations that have bounded Lipschitz constants, and then further control the smoothnessness of the velocity field by regularizing the weights (e.g., via spectral normalization).\nPutting all of this together, and following the notation/construction of Grathwohl et al. 2019, we can summarize the continuous normalizing flow (CNF) model as the following Initial Value Problem (IVP), based on the above ODE: \\[\n\\underbrace{\\begin{bmatrix}\n\\mathbf{z}_0 \\\\\n\\log p(\\mathbf{x}) - \\log p_{\\mathbf{z}_0}(\\mathbf{z}_0)\n\\end{bmatrix}}_{\\text{solutions}}\n=\n\\int_{t_1}^{t_0}\n\\underbrace{\\begin{bmatrix}\nv_\\theta(\\mathbf{z}(t),t) \\\\\n-\\mathrm{tr}\\!\\left(\\frac{\\partial v_\\theta}{\\partial \\mathbf{z}(t)}\\right)\n\\end{bmatrix}}_{\\text{dynamics}}\n\\, dt,\n\\quad\n\\underbrace{\\begin{bmatrix}\n\\mathbf{z}(t_1) \\\\\n\\log p(\\mathbf{x}) - \\log p(\\mathbf{z}(t_1))\n\\end{bmatrix}}_{\\text{initial values}}\n=\n\\begin{bmatrix}\n\\mathbf{x} \\\\\n0\n\\end{bmatrix}.\n\\]\nWhich we can see involves solving the ODE backwards in time from \\(t_1 = T\\) to \\(t_0 = 0\\) and can recover both the latent variable \\(\\mathbf{z}_0\\) and the log-density \\(\\log p(\\mathbf{x})\\) given a training sample \\(\\mathbf{x}\\).\nSolving this IVP can be done in many ways, including using existing ODE solvers like Runge-Kutta methods or adaptive solvers, and the cost is largely determined by the complexity of the velocity field and the stiffness of the ODE, since more complex velocity fields may require more function evaluations to accurately solve the ODE. An important caveat to this is that the solver must be itself differentiable, since we need to be able to differentiate through it to see how to modify the vector field. This is not the case for many existing ODE solvers you may have used.2 Further techniques are used to reduce the computational burden, such through the use of Jacobian-vector products to compute the trace term efficiently, the use of the Hutchinson’s trace estimator, as well as regularization methods to ensure smoothness of the velocity field, which can help in reducing the number of function evaluations needed during ODE solving, but these details are beyond the scope of this book. For further details see the original paper by Chen et al., 2018 and the follow-up work by Grathwohl et al., 2019.\n2 Interestingly enough, we showed in our review of automatic differentiation how to make a Verlet Integrator differentiable. In practice, one could use an existing ODE solver that was designed to be differentiable, such as torchdiffeq in PyTorch, which we will use in the experiments below.To gain some intuition about continuous flows, we will do a couple of simple experiments using a simple flow integrator using Runge-Kutta 45 – in practice you could use an existing solver or customized code with some of the advanced tricks mentioned above, but this will allow us to get the main concept across with minimal code complexity.\n\n\nShow Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch import nn\nfrom typing import Callable, Tuple\nimport torch.optim as optim\n\nplt.style.use('seaborn-v0_8-muted')\nsns.set_context('talk')\n\ndevice = \"cpu\"\n\n\n\nTensor = torch.Tensor\n\n# Fourth-order Runge-Kutta (RK4) integrator step\ndef rk4_step(field: Callable[[Tensor, float], Tensor], x: Tensor, t: float, dt: float) -&gt; Tensor:\n    k1 = field(x, t)\n    k2 = field(x + 0.5 * dt * k1, t + 0.5 * dt)\n    k3 = field(x + 0.5 * dt * k2, t + 0.5 * dt)\n    k4 = field(x + dt * k3, t + dt)\n    return x + (dt / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\n# Integrate flow using RK4\ndef integrate_flow(\n    field: Callable[[Tensor, float], Tensor],\n    x0: Tensor,\n    t_span: Tuple[float, float] = (0.0, 1.0),\n    steps: int = 100,\n    record: bool = False,\n):\n    t0, t1 = t_span\n    dt = float(t1 - t0) / steps\n    x = x0.clone()\n    t = float(t0)\n    history = []\n    if record:\n        history.append(x.detach().cpu().numpy())\n    for _ in range(steps):\n        x = rk4_step(field, x, t, dt)\n        t += dt\n        if record:\n            history.append(x.detach().cpu().numpy())\n    if record:\n        times = np.linspace(t0, t1, steps + 1)\n        return x, np.stack(history), times\n    return x\n\n# Integrate flow and log-probability using RK4\ndef integrate_flow_with_logp(\n    field: Callable[[Tensor, float], Tensor],\n    divergence_fn: Callable[[Tensor, float], Tensor],\n    x0: Tensor,\n    logp0: Tensor,\n    t_span: Tuple[float, float] = (0.0, 1.0),\n    steps: int = 100,\n):\n    t0, t1 = t_span\n    dt = float(t1 - t0) / steps\n    x = x0.clone()\n    logp = logp0.clone()\n    t = float(t0)\n    for _ in range(steps):\n        div = divergence_fn(x, t)\n        logp = logp - dt * div\n        x = rk4_step(field, x, t, dt)\n        t += dt\n    return x, logp\n\n# Estimate divergence\ndef estimate_divergence_autograd(field: Callable[[Tensor, float], Tensor], x: Tensor, t: float) -&gt; Tensor:\n    x = x.clone().detach().requires_grad_(True)\n    v = field(x, t)\n    components = []\n    for idx in range(v.shape[1]):\n        grad = torch.autograd.grad(v[:, idx].sum(), x, retain_graph=idx &lt; v.shape[1] - 1)[0][:, idx]\n        components.append(grad)\n    return torch.stack(components, dim=1).sum(dim=1)",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>From Discrete Transformations to Continuous Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/continuous_flows.html#example-divergence-free-rotation",
    "href": "part2/gen_models/continuous_flows.html#example-divergence-free-rotation",
    "title": "14  From Discrete Transformations to Continuous Flows",
    "section": "14.3 Example: Divergence-Free Rotation",
    "text": "14.3 Example: Divergence-Free Rotation\nConsider the velocity field \\(v(x, y) = (-y, x)\\). Its divergence is zero, so the flow should rotate particles without changing their density. We integrate a few trajectories and compare a cloud of points before and after one full revolution.\n\ndef rotation_field(x: Tensor, t: float) -&gt; Tensor:\n    return torch.stack([-x[:, 1], x[:, 0]], dim=1)\n\ncloud0 = torch.randn(800, 2, device=device)\ncloudT = integrate_flow(rotation_field, cloud0, t_span=(0.0, 2 * np.pi), steps=200)\n\n\n\nShow Code\nfig, axes = plt.subplots(1, 3, figsize=(10, 4))\n\n# Plot the vector field corresponding to the rotation\nx1 = np.linspace(-3, 3, 20)\nx2 = np.linspace(-3, 3, 20)\nX1, X2 = np.meshgrid(x1, x2)\nU = -X2\nV = X1\n\naxes[0].quiver(X1, X2, U, V, color='tab:blue')\naxes[0].set_title('Vector field')\naxes[0].set_xlabel('$x_1$')\naxes[0].set_ylabel('$x_2$')\naxes[0].axis('equal')\naxes[0].grid(True, alpha=0.2)\n\naxes[1].scatter(cloud0[:, 0].cpu(), cloud0[:, 1].cpu(), s=10, alpha=0.4)\naxes[1].set_title('Initial distribution')\naxes[1].set_xlabel('$x_1$')\naxes[1].set_ylabel('$x_2$')\naxes[1].axis('equal')\naxes[1].grid(True, alpha=0.2)\n\naxes[2].scatter(cloudT[:, 0].cpu(), cloudT[:, 1].cpu(), s=10, alpha=0.4, color='tab:orange')\naxes[2].set_title('After one revolution')\naxes[2].set_xlabel('$x_1$')\naxes[2].set_ylabel('$x_2$')\naxes[2].axis('equal')\naxes[2].grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nBecause \\(\\nabla_x \\cdot v = 0\\), every particle completes a rotation on a circle of constant radius. The density of the cloud is unchanged, illustrating how divergence controls instantaneous volume change.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>From Discrete Transformations to Continuous Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/continuous_flows.html#example-contracting-divergence-controls-density-growth",
    "href": "part2/gen_models/continuous_flows.html#example-contracting-divergence-controls-density-growth",
    "title": "14  From Discrete Transformations to Continuous Flows",
    "section": "14.4 Example: Contracting Divergence Controls Density Growth",
    "text": "14.4 Example: Contracting Divergence Controls Density Growth\nNow take the compressible velocity field \\(v(x, y) = (-x, -y)\\). The divergence is \\(-2\\), so the log-density should increase as \\(\\frac{\\mathrm{d}}{\\mathrm{d}t} \\log p = 2\\). We’ll integrate particles, track log-density, and compare to the analytic solution \\(x(t) = e^{-t} x(0)\\).\n\ndef contracting_field(x: Tensor, t: float) -&gt; Tensor:\n    return -x\n\ndivergence_contracting = -2.0\ntorch.manual_seed(21)\nz0 = torch.randn(2000, 2, device=device)\nlogp0 = -0.5 * torch.sum(z0 ** 2, dim=1) - np.log(2 * np.pi)\nT_final = 1.5\nzT, logpT_est = integrate_flow_with_logp(\n    contracting_field,\n    lambda x, t: torch.full((x.shape[0],), divergence_contracting, device=x.device),\n    z0,\n    logp0,\n    t_span=(0.0, T_final),\n    steps=200,\n)\nscale = torch.exp(torch.tensor(-T_final))\nanalytic_zT = z0 * scale\nanalytic_logp = logp0 - divergence_contracting * T_final\n\n\n\nShow Code\nfig, axes = plt.subplots(1, 3, figsize=(10, 4))\n\n# Plot the vector field corresponding to the contracting flow\nx1 = np.linspace(-3, 3, 20)\nx2 = np.linspace(-3, 3, 20)\nX1, X2 = np.meshgrid(x1, x2)\nU = -X1\nV = -X2\n\naxes[0].quiver(X1, X2, U, V, color='tab:blue')\naxes[0].set_title('Vector field')\naxes[0].set_xlabel('$x_1$')\naxes[0].set_ylabel('$x_2$')\naxes[0].axis('equal')\naxes[0].grid(True, alpha=0.2)\n\n\naxes[1].scatter(z0[:, 0].cpu(), z0[:, 1].cpu(), s=8, alpha=0.3, label='t=0')\naxes[1].scatter(zT[:, 0].cpu(), zT[:, 1].cpu(), s=8, alpha=0.3, label=f't={T_final}')\naxes[1].set_title('Contracting flow trajectories')\naxes[1].set_xlabel('$x_1$')\naxes[1].set_ylabel('$x_2$')\naxes[1].axis('equal')\naxes[1].grid(True, alpha=0.3)\naxes[1].legend()\n\naxes[2].hist(logpT_est.cpu().numpy(), bins=60, alpha=0.6, label='Estimated log p')\naxes[2].axvline(analytic_logp.mean().item(), color='tab:red', linestyle='--', label='Analytic mean')\naxes[2].set_title('Log-density shift')\naxes[2].set_xlabel('log p')\naxes[2].set_ylabel('Frequency')\naxes[2].grid(True, alpha=0.3)\naxes[2].legend()\nplt.tight_layout()\nplt.show()\n\nrmse = torch.sqrt(torch.mean((zT - analytic_zT) ** 2)).item()\nlogp_error = torch.abs(logpT_est - analytic_logp).mean().item()\nprint(f'RMSE between simulated and analytic endpoints: {rmse:.3e}')\nprint(f'Mean absolute log-density error: {logp_error:.3e}')\n\n\n\n\n\n\n\n\n\nRMSE between simulated and analytic endpoints: 7.975e-08\nMean absolute log-density error: 4.175e-06\n\n\nThe numerical flow contracts every trajectory toward the origin while the histogram of \\(\\log p\\) shifts by approximately \\(2 T\\). The errors stay small because the velocity field is linear and the RK4 integrator tracks it well.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>From Discrete Transformations to Continuous Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/continuous_flows.html#example-1d-continuous-normalizing-flow-with-neural-odes",
    "href": "part2/gen_models/continuous_flows.html#example-1d-continuous-normalizing-flow-with-neural-odes",
    "title": "14  From Discrete Transformations to Continuous Flows",
    "section": "14.5 Example 1D Continuous Normalizing Flow with Neural ODEs",
    "text": "14.5 Example 1D Continuous Normalizing Flow with Neural ODEs\nFinally, we’ll implement a simple continuous normalizing flow using a neural ODE to learn a target distribution. We’ll define a neural network velocity field, integrate the ODE, and optimize the parameters to maximize likelihood on a simple 1D distribution.\nNote: this demo requires the torchdiffeq package. You can install it via pip if you haven’t already if you wish to run the demo.\n\n\nShow Code\n#%pip install torchdiffeq\n\n\n\nfrom torchdiffeq import odeint\n\ndef sample_base_gaussian(n: int) -&gt; Tensor:\n    return torch.normal(0.0, 1.0, size=(n, 1), device=device)\n\ndef log_prob_base_gaussian(x: Tensor) -&gt; Tensor:\n    const = -0.5 * torch.log(2 * torch.pi * torch.ones_like(x))\n    return -0.5 * x.pow(2) + const\n\n\n_UNIF_LOW, _UNIF_HIGH = -3.0, 3.0\ndef sample_base_uniform(n: int, low: float = _UNIF_LOW, high: float = _UNIF_HIGH) -&gt; Tensor:\n    return low + (high - low) * torch.rand(n, 1, device=device)\n\ndef log_prob_base_uniform(x: Tensor, low: float = _UNIF_LOW, high: float = _UNIF_HIGH) -&gt; Tensor:\n    width = high - low\n    logp = -torch.log(torch.tensor(width, device=x.device)) * torch.ones_like(x)\n    inside = (x &gt;= low) & (x &lt;= high)\n    # out-of-support points get -inf log-probability\n    return torch.where(inside, logp, torch.full_like(x, float(\"-inf\")))\n\n# We will use the uniform base distribution, but you can switch it to the Gaussian if desired\n# by changing these assignments below.\nsample_base = sample_base_uniform\nlog_prob_base = log_prob_base_uniform\n\ndef sample_target(n):\n    # mixture of two Gaussians, 0.5 each\n    mix = torch.randint(0, 2, (n, 1), device=device).float()\n    means = -2 * (1 - mix) + 2 * mix\n    return means + 0.4 * torch.randn(n, 1, device=device)\n\ndef log_prob_target(x):\n    # mixture density for visualization only\n    denom = 0.4 * torch.sqrt(2 * torch.pi * torch.ones_like(x))\n    p1 = torch.exp(-0.5 * ((x + 2) / 0.4)**2) / denom\n    p2 = torch.exp(-0.5 * ((x - 2) / 0.4)**2) / denom\n    return torch.log(0.5 * (p1 + p2) + 1e-9)\n\n# Define the neural network vector field f(x,t)\nclass CNF1D(nn.Module):\n    def __init__(self, hidden=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(1 + 1, hidden),\n            nn.Tanh(),\n            nn.Linear(hidden, hidden),\n            nn.Tanh(),\n            nn.Linear(hidden, 1)\n        )\n\n    def forward(self, t, x):\n        if isinstance(t, torch.Tensor):\n            t = t.to(x.device)\n        t_in = torch.ones_like(x) * t\n        return self.net(torch.cat([x, t_in], dim=1))\n\n\n# CNF dynamics: dx/dt = f, d(logp)/dt = -∂f/∂x\nclass CNFDynamics(nn.Module):\n    def __init__(self, fnet):\n        super().__init__()\n        self.fnet = fnet\n\n    def forward(self, t, states):\n        x, logp = states\n        x = x.requires_grad_(True)\n        f = self.fnet(t, x)\n        df_dx = torch.autograd.grad(f, x, torch.ones_like(f), create_graph=True)[0]\n        dlogp = -df_dx\n        return f, dlogp\n\nfnet = CNF1D(hidden=32).to(device)\ndynamics = CNFDynamics(fnet)\noptimizer = optim.AdamW(fnet.parameters(), lr=1e-3)\n\nfor step in range(600):\n    x0 = sample_base(256)\n    logp0 = log_prob_base(x0)\n\n    # ensure time tensor is on the same device as the model/state\n    t = torch.tensor([0.0, 1.0], device=device)\n    x1, logp1 = odeint(dynamics, (x0, logp0), t)\n    x1, logp1 = x1[-1], logp1[-1]\n\n    # Negative log-likelihood under target (maximum likelihood)\n    logpt = log_prob_target(x1)\n    loss = -(logpt - logp1).mean()\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if step % 50 == 0:\n        print(f\"Step {step}, loss {loss.item():.4f}\")\nprint(\"Training complete.\")\n\nStep 0, loss 2.3605\nStep 50, loss 1.5631\nStep 100, loss 1.0494\nStep 150, loss 0.4459\nStep 200, loss 0.1519\nStep 250, loss 0.1067\nStep 300, loss 0.1081\nStep 350, loss 0.0705\nStep 400, loss 0.0784\nStep 450, loss 0.1421\nStep 500, loss 0.0940\nStep 550, loss 0.0485\nTraining complete.\n\n\n\n\nShow Code\nwith torch.no_grad():\n    xs = torch.linspace(-6, 6, 300).unsqueeze(1)\n    t = torch.linspace(0, 1, 20)\n    xt = odeint(lambda t, x: fnet(t, x), xs, t)\n\n# Prepare a tensor that requires grad to compute ∂f/∂x\nxs_grad = xs.clone().detach().requires_grad_(True)\nf_vals = fnet(0.5, xs_grad)\n\n# Plot flow evolution\nfig, axes = plt.subplots(1, 3, figsize=(12, 3))\naxes[0].set_title(\"Base density\")\naxes[1].set_title(\"Flow evolution\")\naxes[2].set_title(\"Target mixture density\")\n\n# Plot the Base distribution\nxplot = xs.squeeze().cpu().numpy()\np_base = torch.exp(log_prob_base(xs_grad)).squeeze().detach().cpu().numpy()\np_target = torch.exp(log_prob_target(xs_grad)).squeeze().detach().cpu().numpy()\naxes[0].plot(xplot, p_base)\n\n# Evolution (trajectories)\nxt_np = xt.detach().cpu().numpy()\nfor i in range(0, len(xs), 10):\n    axes[1].plot(xt_np[:, i, 0], t.cpu().numpy(), color=\"C0\", alpha=0.3)\naxes[1].invert_yaxis()\naxes[1].set_xlabel(\"x\")\naxes[1].set_ylabel(\"t\")\n\n# Target vs learned\n# compute df/dx using autograd\ndf_dx = torch.autograd.grad(f_vals, xs_grad, torch.ones_like(f_vals), create_graph=False)[0]\np_learned = torch.exp(log_prob_base(xs_grad) - df_dx).squeeze().detach().cpu().numpy()\n\naxes[2].plot(xplot, p_target, label=\"target\", color=\"black\")\naxes[2].plot(xplot, p_learned, label=\"learned\", color=\"C1\")\naxes[2].legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from the center plot how the initial distribution (blue) is smoothly transformed into the target distribution over time via the learned velocity field, although not perfectly in this example given the small network and simulation time we gave it.\n\n\n\n\n\n\nTipExperiment: Effect of Base Distribution and Smoothness on the Flow Field\n\n\n\nChanging some of the below will help you see some possible failure modes of continuous normalizing flows:\n\nTry changing the base distribution from standard normal to a uniform distribution. How does this affect the learned flow? What happens if the base distribution has bounded support that is different from the target distribution? How does the flow handle this?\nExperiment with the smoothness of the velocity field by changing the architecture of the neural network (e.g., number of layers, activation functions, weight decay to promote regularization). How does this impact the quality of the learned distribution and the stability of training?\nChange sample_base to use more or fewer samples. How does the number of samples used to estimate the likelihood affect the training dynamics, total time taken, and the final learned distribution?",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>From Discrete Transformations to Continuous Flows</span>"
    ]
  },
  {
    "objectID": "part2/gen_models/continuous_flows.html#summary-and-next-steps",
    "href": "part2/gen_models/continuous_flows.html#summary-and-next-steps",
    "title": "14  From Discrete Transformations to Continuous Flows",
    "section": "14.6 Summary and Next Steps",
    "text": "14.6 Summary and Next Steps\n\nContinuous normalizing flows are the infinitesimal limit of discrete flows, replacing layerwise Jacobians with a divergence term integrated over time.\nThe continuity equation formalizes conservation of probability mass and links particle dynamics to density evolution.\nDivergence-free flows preserve density while compressive fields increase log-density.\nNeural ODEs show that we can learn velocity fields that sculpt data distributions without explicit discrete blocks. Inference now involves solving an ODE rather than evaluating a fixed sequence of transformations.\nHowever, ODE solving can be computationally intensive, especially in high dimensions or with complex velocity fields, since this will necessitate longer numerical integration times. Moreover, computing the trace of the Jacobian remains a bottleneck.\n\nWhat if we could get some of the benefits of continuous flows without the computational overhead of ODE solving? In the next chapter, we will explore score-based generative models, which learn to model data distributions by estimating something called a score function. These models can be seen as a special case of continuous flows, but they avoid the need for expensive ODE integration by leveraging denoising techniques and stochastic sampling methods that will ultimately lead us to diffusion models and flow matching approaches in the coming chapters.",
    "crumbs": [
      "Model-Specific Approaches",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>From Discrete Transformations to Continuous Flows</span>"
    ]
  },
  {
    "objectID": "problems/problems.html",
    "href": "problems/problems.html",
    "title": "Problems",
    "section": "",
    "text": "Here are the problem sets",
    "crumbs": [
      "Problems"
    ]
  },
  {
    "objectID": "problems/ps1.html",
    "href": "problems/ps1.html",
    "title": "15  Problem Set 1",
    "section": "",
    "text": "15.1 PS1 Part 1: Linear Models and Validation",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "problems/ps1.html#ps1-part-1-linear-models-and-validation",
    "href": "problems/ps1.html#ps1-part-1-linear-models-and-validation",
    "title": "15  Problem Set 1",
    "section": "",
    "text": "15.1.1 Preamble\nWe’ll be loading some CO2 concentration data that is a commonly used dataset for model building of time series prediction. You will build a few baseline linear models and assess them using some of the tools we discussed in class. Which model is best? Let’s find out.\nFirst let’s just load the data and take a look at it:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nsns.set_context('notebook')\n\n# Fetch the data\nmauna_lao = fetch_openml('mauna-loa-atmospheric-co2', as_frame = False)\nprint(mauna_lao.DESCR)\ndata = mauna_lao.data\n# Assemble the day/time from the data columns so we can plot it\nd1958 = datetime(year=1958,month=1,day=1)\ntime = [datetime(int(d[0]),int(d[1]),int(d[2])) for d in data]\nX = np.array([1958+(t-d1958)/timedelta(days=365.2425) for t in time]).T\nX = X.reshape(-1,1)  # Make it a column to make scikit happy\ny = np.array(mauna_lao.target)\n\n**Weekly carbon-dioxide concentration averages derived from continuous air samples for the Mauna Loa Observatory, Hawaii, U.S.A.**&lt;br&gt;&lt;br&gt;\nThese weekly averages are ultimately based on measurements of 4 air samples per hour taken atop intake lines on several towers during steady periods of CO2 concentration of not less than 6 hours per day; if no such periods are available on a given day, then no data are used for that day. The _Weight_ column gives the number of days used in each weekly average. _Flag_ codes are explained in the NDP writeup, available electronically from the [home page](http://cdiac.ess-dive.lbl.gov/ftp/trends/co2/sio-keel-flask/maunaloa_c.dat) of this data set. CO2 concentrations are in terms of the 1999 calibration scale (Keeling et al., 2002) available electronically from the references in the NDP writeup which can be accessed from the home page of this data set.\n&lt;br&gt;&lt;br&gt;\n### Feature Descriptions\n_co2_: average co2 concentration in ppvm &lt;br&gt;\n_year_: year of concentration measurement &lt;br&gt;\n_month_: month of concentration measurement &lt;br&gt;\n_day_: day of month of concentration measurement &lt;br&gt;\n_weight_: number of days used in each weekly average &lt;br&gt;\n_flag_: flag code &lt;br&gt;\n_station_: station code &lt;br&gt;\n&lt;br&gt;\n**Author**: Carbon Dioxide Research Group, Scripps Institution of Oceanography, University of California-San Diego, La Jolla, California, USA 92023-0444 &lt;br&gt;\n**Source**: [original](http://cdiac.ess-dive.lbl.gov/ftp/trends/co2/sio-keel-flask/maunaloa_c.dat) - September 2004\n\nDownloaded from openml.org.\n\n\n\n# Plot the data\nplt.figure(figsize=(10,5))    # Initialize empty figure\nplt.scatter(X, y, c='k',s=1) # Scatterplot of data\nplt.xlabel(\"Year\")\nplt.ylabel(r\"CO$_2$ in ppm\")\nplt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ny[:100]\n\narray([316.1, 317.3, 317.6, 317.5, 316.4, 316.9, 317.5, 317.9, 315.8,\n       315.8, 315.4, 315.5, 315.6, 315.1, 315. , 314.1, 313.5, 313. ,\n       313.2, 313.5, 314. , 314.5, 314.4, 314.7, 315.2, 315.2, 315.5,\n       315.6, 315.8, 315.4, 316.9, 316.6, 316.6, 316.8, 316.7, 316.7,\n       317.7, 317.1, 317.6, 318.3, 318.2, 318.7, 318. , 318.4, 318.5,\n       318.1, 317.8, 317.7, 316.8, 316.8, 316.4, 316.1, 315.6, 314.9,\n       315. , 314.1, 314.4, 313.9, 313.5, 313.5, 313. , 313.1, 313.4,\n       313.4, 314.1, 314.4, 314.8, 315.2, 315.1, 315. , 315.6, 315.8,\n       315.7, 315.7, 316.4, 316.7, 316.5, 316.6, 316.6, 316.9, 317.4,\n       317. , 316.9, 317.7, 318. , 317.7, 318.6, 319.3, 319. , 319. ,\n       319.7, 319.9, 319.8, 320. , 320. , 319.4, 320. , 319.4, 319. ,\n       318.1])\n\n\n\n\n15.1.2 Linear Models\nConstruct the following linear models: 1. Model 1: “Vanilla” Linear Regression, that is, where \\(CO_2 = a+b \\cdot time\\) 2. Model 2: Quadratic Regression, where \\(CO_2 = a+b \\cdot t + c\\cdot t^2\\) 3. Model 3: A more complex “linear” model with the following additive terms \\(CO_2=a+b\\cdot t+c\\cdot sin(\\omega\\cdot t)\\): * a linear (in time) term * a sinusoidal additive term with period such that the peak-to-peak of the sinsusoid is roughly ~1 year and phase shift of zero (set \\(\\omega\\) as appropriate to match the peaks) 4. Model 4: A “linear” model with the following additive terms (\\(CO_2=a+b\\cdot t+c\\cdot t^2+d\\cdot sin(\\omega\\cdot t)\\): * a quadratic (in time) polynomial * a sinusoidal additive term with period such that the peak-to-peak of the sinsusoid is roughly ~1 year and phase shift of zero (set \\(\\omega\\) as appropriate to match the peaks)\nEvauate these models using the appropriate kind of Cross Validation for each of the following amounts of Training data: 1. N=50 Training Data Points 2. N=100 3. N=200 4. N=500 5. N=1000 6. N=2000\nQuestion: Before you even construct the models or do any coding below, what is your initial guess or intuition behind how each of those four models will perform? Note: there is no right or wrong answer to this part of the assignment and this question will only be graded on completeness, not accuracy. It’s intent is to get you to think about and write down your preliminary intuition regarding what you think will happen before you actually implement anything, based on your approximate understanding of how functions of the above complexity should perform as N increases.\nStudent Response: [Insert your response here]\nQuestion: What is the appropriate kind of Cross Validation to perform in this case if we want a correct Out of Sample estimate of our Test MSE?\nStudent Response: [Insert your response here]\nNow, for each of the above models and training data sizes: * Plot the predicted CO2 as a function of time, including the actual data, for each of the N=X training data examples. This should correspond to six plots (one for each amount of training data) if you plot all models on the same plot, or 6x4 = 24 plots if you plot each model and training data plot separately. * Create a Learning Curve plot for the model which plots its Training and Test MSE as a function of training data. That is, plot how Training and Testing MSE change as you increase the training data for each model. This could be a single plot for all four models (8 lines on the plot) or four different plots corresponding to the learning curve of each model separately.\n\nimport numpy as np\n\nX_train_100 = X[:100]\ny_train_100 = y[:100]\nX_test = X[100:200]\nprint(\"Shape of X_train_100: %s\" % str(X_train_100.shape))\nprint(\"Beginning of X_train_100: %s\" % str(X_train_100[0:5]))\nprint(\"Shape of y_train_100: %s\" % str(y_train_100.shape))\nprint(\"Beginning of y_train_100: %s\" % str(y_train_100[0:5]))\n\nprint('Shape of X_test: %s' % str(X_test.shape))\nprint(\"Beginning of X_test: %s\" % str(X_test[0:5]))\n\n### Modify the below code. You can leave the code above as is. ###\n\n\nShape of X_train_100: (100, 1)\nBeginning of X_train_100: [[1958.23819791]\n [1958.25736326]\n [1958.27652861]\n [1958.29569396]\n [1958.31485931]]\nShape of y_train_100: (100,)\nBeginning of y_train_100: [316.1 317.3 317.6 317.5 316.4]\nShape of X_test: (100, 1)\nBeginning of X_test: [[1960.51887445]\n [1960.5380398 ]\n [1960.55720514]\n [1960.57637049]\n [1960.59553584]]\n\n\n\n# Insert Modeling Building or Plotting code here\n# Note, you may implement these however you see fit\n# Ex: using an existing library, solving the Normal Eqns\n#     implementing your own SGD solver for them. Your Choice.\nfrom sklearn.linear_model import SGDRegressor, LinearRegression\nsgd = SGDRegressor()\nlr = LinearRegression()\n\n\nsgd.fit(X,y)\nlr.fit(X,y)\n\nLinearRegression()\n\n\n\nsgd.predict(X)\n\narray([-1.78959753e+15, -1.78961505e+15, -1.78963256e+15, ...,\n       -1.82954889e+15, -1.82956640e+15, -1.82958392e+15])\n\n\n\nlr.predict(X)\n\narray([310.2080183 , 310.23375578, 310.25949326, ..., 368.9152125 ,\n       368.94094999, 368.96668747])\n\n\nQuestion: Which Model appears to perform best in the N=50 or N=100 Condition? Why is this?\nStudent Response: [Insert your response here]\nQuestion: Which Model appears to perform best as the N=200 to 500? Why is this?\nStudent Response: [Insert your response here]\nQuestion: Which Model appears to perform best as N = 2000? Why is this?\nStudent Response: [Insert your response here]",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "problems/ps1.html#ps1-part-2-unsupervised-linear-models",
    "href": "problems/ps1.html#ps1-part-2-unsupervised-linear-models",
    "title": "15  Problem Set 1",
    "section": "15.2 PS1 Part 2: Unsupervised Linear Models",
    "text": "15.2 PS1 Part 2: Unsupervised Linear Models\n\n15.2.1 Toy Dataset\nFor this problem, you will use the data file hb.csv. The input is 2,280 data points, each of which is 7 dimensional (i.e., input csv is 2280 rows by 7 columns). Use Principal Component Analysis (either an existing library, or through your own implementation by taking the SVD of the Covariance Matrix) for the follow tasks.\n\n%matplotlib inline\nimport pandas\nurl = \"https://raw.githubusercontent.com/IDEALLab/ML4ME_Textbook/main/problems/hb.csv\"\ndata = pandas.read_csv(url,header=None)\n#data.head()\n\n\n\n15.2.2 Task 1\nAssuming that the 7-dimensional space is excessive, you would like to reduce the dimension of the space. However, what dimensionality of space should we reduce it to? To determine this we need to compute its intrinsic dimensionality. Plot the relative value of the information content of each of the principal components and compare them.\nNote: this information content is called the “explained variance” of each component, but you can also get this from the magnitude of the singular values. This plot is sometimes called a “Scree Plot”.\n\n# Code Here\n\nQuestion: Approximately how many components dominate the space?, and what does this tell us about the intrinsic dimensionality of the space?\nResponse:\n\n15.2.2.1 Task 2\nNow use PCA to project the 7-dimensional points on the K-dimensional space (where K is your answer from above) and plot the points. (For K=1,2, or 3, use a 1, 2, or 3D plot, respectively. For 4+ dimensions, use a grid of pairwise 2D Plots, like the Scatter Matrix we used in class).\n\n# Code Here\n\nQuestion: What do you notice?\nResponse:\n\n\n\n15.2.3 Topology Optimization Dataset\nFor this problem, you will be using unsupervised linear models to help understand and interpret the results of a mechanical optimization problem. Specifically, to understand the solution space generated by a topology optimization code; that is, the results of finding the optimal geometries for minimizing the compliance of various bridge structures with different loading conditions. The input consists of 1,000 images of optimized material distribution for a beam as described in Figure 1. A symmetrical boundary condition, left side, is used to reduce the analysis to only half. Also, a rolling support is included at the lower right corner. Notice that the rolling support is the only support in the vertical direction.\n \n\n\n\n\nFigure 1: Left: Nx-by-Ny design domain for topology optimization problem. Right: Example loading configuration and resulting optimal topology. Two external forces, Fi, were applied to the beam at random nodes represented by (xi, yi) coordinates.1\n\n \nUse Principal Component Analysis (either an existing library, or through your own implementation by taking the SVD of the Covariance Matrix) for the follow tasks.\n1. This problems data is based on the problem setup seen in the following paper: Ulu, E., Zhang, R., & Kara, L. B. (2016). A data-driven investigation and estimation of optimal topologies under variable loading configurations. Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 4(2), 61-72.\n\n# To help you get started, the below code will load the images from the associated image folder:\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom PIL import Image\nimport requests, zipfile, io\n\n#im_dir = './topo_opt_runs/'\nurl = \"https://raw.githubusercontent.com/IDEALLab/ML4ME_Textbook/main/problems/topo_opt_runs.zip\"\nresp = requests.get(url)\nresp.raise_for_status()\nzf = zipfile.ZipFile(io.BytesIO(resp.content))\n\nimages = []\nfor name in sorted(zf.namelist()):\n    with zf.open(name) as f:\n        img = Image.open(f).convert('L')\n        images.append(np.asarray(img))\n\nheight,width = images[0].shape\nprint('The images are {:d} pixels high and {:d} pixels wide'.format(height,width))\n\n# Print matrix corresponding to the image:\nprint(images[-1])\n# And show example image, so you can see how matrix correponds:\nimg\n\nThe images are 217 pixels high and 434 pixels wide\n[[  0   0   0 ... 255 255 255]\n [  0   0   0 ... 255 255 255]\n [  0   0   0 ... 255 255 255]\n ...\n [  0   0   0 ...   0   0   0]\n [  0   0   0 ...   0   0   0]\n [  0   0   0 ...   0   0   0]]\n\n\n\n\n\n\n\n\n\n\n\n15.2.4 Task 1: Scree/Singular Value Plot\nAs with the toy example, assume that the 94,178-dimensional space is excessive. You would like to reduce the dimension of the image space. First compute its intrinsic dimensionality. For this application, “good enough” means capturing 95% of the variance in the dataset. How many dimensions are needed to capture at least 95% of the variance in the provided dataset? Store your answer in numDimsNeeded. (Hint: A Scree plot may be helpful, though visual inspection of a graph may not be precise enough.)\nQuestion: Approximately how many components dominate the space? What does this tell us about the intrinsic dimensionality of the space?\nResponse:\n\n\n15.2.5 Task 2: Principal Components\nNow plot the first 5 principal components. Hint: looking at each of these top 5 principal components; do they make sense physically, in terms of what it means for where material in the bridge is placed? Compare, for example, the differences between the 1st and 2nd principal component?",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "problems/ps1.html#ps1-part-3-bi-linear-models-and-sgd",
    "href": "problems/ps1.html#ps1-part-3-bi-linear-models-and-sgd",
    "title": "15  Problem Set 1",
    "section": "15.3 PS1 Part 3: Bi-Linear Models and SGD",
    "text": "15.3 PS1 Part 3: Bi-Linear Models and SGD\n\n15.3.1 Bilinear Models for Recommendation\nFor this problem, you will derive a very simple recommendation system that uses a combination of unsupervised and supervised approachs and demonstrates use of Stochastic Gradient Descent.\nSpecifically, in class we discussed recommender models of the form: \\[\nf(user,movie) = \\langle v_u,v_m \\rangle + b_u + b_m + \\mu\n\\]\nwhere \\(v\\) is a vector that represents a user’s or movie’s location in an N-Dimensional space, \\(b\\) is a vector that represents a specific “bias” term fo r each movie and user, and \\(\\mu\\) is a scalar that represents a kind of global anchor or base score (i.e., a sort of average movie rating). This means that each user has two vectors (e.g., \\(v_{\\mathrm{jack~smith}}\\) and \\(b_{\\mathrm{jack~smith}}\\)), and each movie has two vectors (e.g., \\(v_{\\mathrm{Avengers}}\\) and \\(b_{\\mathrm{Avengers}}\\)), with each of those vectors being N-Dimensional (in class we used two dimensions). For this, we constructed a loss function as follows: \\[\nCost = Loss + Penalty\n\\] where \\[\nLoss = \\Sigma_{(u,m)\\in \\mathrm{Ratings}} \\frac{1}{2}\\left( \\langle v_u,v_m \\rangle + b_u + b_m + \\mu - y_{u,m}\\right)^2\n\\] and \\[\nPenalty = \\frac{\\lambda}{2}\\left(\\Sigma_u \\left[\\| v_u\\|^2_2 + b_u^2\\right] + \\Sigma_m \\left[\\|v_m\\|^2_2 + b_m^2\\right]\\right)\n\\]\n\n\n15.3.2 Task 1: Analytical Gradients\nTo use stochastic gradient descent, we first need to write down the gradients. Using the above cost function (including both the loss and penalty), compute the following partial derivatives:\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial v_u } =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial v_m } =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial b_u} =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial b_m} =\n\\]\n\\[\n\\frac{\\partial \\textrm{Cost}}{\\partial \\mu} =\n\\]\nYou can either do this directly in the notebook using LaTeX notation, or via a scanned image. Please remember to show your work in how you computed the derivatives, not just the final result. Note: Recall that the partial derivative of e.g. Nicholas’s rating on Titanic with respect to the user Mark would be zero. When computing your SGD updates, consider how this might impact individual terms for users and movies in the loss function.\n\n\n15.3.3 Task 2: Stochastic Gradient Descent\nNow you are actually going to implement SGD on this type of model and optimize it until convergence on a toy dataset. To simplify the implementation, we’ll actually make the model a little simpler than the one you derived updates for in task 1. Specifically, we’ll just use:\n\\[\nCost = \\Sigma_{(u,m)\\in \\mathrm{Ratings}} \\frac{1}{2}\\left( \\langle v_u,v_m \\rangle + \\mu - y_{u,m}\\right)^2 + \\frac{\\lambda}{2}\\left(\\| v_u\\|^2_2 + \\|v_m\\|^2_2\\right)\n\\]\nThis way all you have to estimate is two vectors — \\(v_u\\) for each user and \\(v_m\\) for each movie — and \\(\\mu\\) — a scalar value similar to an average rating. For simplicity, we’ll assume here that the size of the latent space (K) is 2 (i.e., the length of each \\(v_u\\) & \\(v_m\\)).\nUsing your above gradients, write down the update equations for each vector using stochastic gradient descent. Once you have done this, implement those update equations in code like we did in the in-class notebook. For simplicity, you can just use a constant step size \\(\\alpha\\) if you wish, though you may change this if you want. Note: depending on exactly how you implement your model and what batch size you use, i.e., one point at a time, or some subset of data points, values of \\(\\alpha\\) anywhere between around 0.7 and 0.01 should be sufficient to converge the model in under 1000 epochs, i.e., passes through the dataset. If you implement more advanced tricks covered in some optional readings this can converge much faster, but that is not necessary for this assignment, and it does not matter to me how quickly your model coverges, so long as it does so.\nUse the below small sample dataset of movie ratings for five users and six movies to perform stochastic gradient descent to update those vectors until your model converges. To initialize your SGD, you can use the initial weights/terms we provide below, or you can initialize the model any other way you wish – the exact initialization should not make a big difference here.\n\n# Your Code below!\n\n\nimport numpy as np\nimport pandas as pd\nmissing_ratings = pd.read_csv('missing.csv')\nratings = pd.read_csv('ratings.csv')\nratings\n\n\n\n\n\n\n\n\nmovie\nuser\nratings\n\n\n\n\n0\nThe Avengers\nAlex\n3.0\n\n\n1\nThe Avengers\nPriya\n3.5\n\n\n2\nThe Avengers\nYichen\n3.5\n\n\n3\nWhen Harry Met Sally\nAlex\n3.0\n\n\n4\nWhen Harry Met Sally\nSally\n4.5\n\n\n5\nWhen Harry Met Sally\nPriya\n3.0\n\n\n6\nWhen Harry Met Sally\nYichen\n3.0\n\n\n7\nSilence of the Lambs\nAlex\n3.0\n\n\n8\nSilence of the Lambs\nSally\n4.0\n\n\n9\nSilence of the Lambs\nJuan\n3.5\n\n\n10\nSilence of the Lambs\nPriya\n3.0\n\n\n11\nSilence of the Lambs\nYichen\n2.5\n\n\n12\nShawshank Redemption\nJuan\n2.5\n\n\n13\nShawshank Redemption\nPriya\n4.0\n\n\n14\nShawshank Redemption\nYichen\n4.0\n\n\n15\nThe Hangover\nAlex\n3.0\n\n\n16\nThe Hangover\nSally\n3.5\n\n\n17\nThe Hangover\nPriya\n3.0\n\n\n18\nThe Hangover\nYichen\n2.5\n\n\n19\nThe Godfather\nAlex\n3.0\n\n\n20\nThe Godfather\nPriya\n3.5\n\n\n\n\n\n\n\n\n# Alternatively, if you prefer, you can convert it into numpy first:\nratings_numpy = ratings.to_numpy()\nratings_numpy\n\narray([['The Avengers', 'Alex', 3.0],\n       ['The Avengers', 'Priya', 3.5],\n       ['The Avengers', 'Yichen', 3.5],\n       ['When Harry Met Sally', 'Alex', 3.0],\n       ['When Harry Met Sally', 'Sally', 4.5],\n       ['When Harry Met Sally', 'Priya', 3.0],\n       ['When Harry Met Sally', 'Yichen', 3.0],\n       ['Silence of the Lambs', 'Alex', 3.0],\n       ['Silence of the Lambs', 'Sally', 4.0],\n       ['Silence of the Lambs', 'Juan', 3.5],\n       ['Silence of the Lambs', 'Priya', 3.0],\n       ['Silence of the Lambs', 'Yichen', 2.5],\n       ['Shawshank Redemption', 'Juan', 2.5],\n       ['Shawshank Redemption', 'Priya', 4.0],\n       ['Shawshank Redemption', 'Yichen', 4.0],\n       ['The Hangover', 'Alex', 3.0],\n       ['The Hangover', 'Sally', 3.5],\n       ['The Hangover', 'Priya', 3.0],\n       ['The Hangover', 'Yichen', 2.5],\n       ['The Godfather', 'Alex', 3.0],\n       ['The Godfather', 'Priya', 3.5]], dtype=object)\n\n\nLet’s initialize the vectors to some random numbers, and \\(\\mu\\) to 2.5\n\nK=2\nuser_names = ratings['user'].unique()\nmovie_names = ratings['movie'].unique()\nmu= 2.5\n# Setting the seed of the random generator to a value so that everyone sees the same initialization\n# should should be able to comment out the below with no ill-effects on whatever model you implement\n# this may just help us in office hours if folks have difficulty implementing things\nnp.random.seed(0)\nV = pd.DataFrame(np.random.random((len(user_names)+len(movie_names),K)),index=np.hstack([user_names,movie_names]))\nprint(V)\n\n                             0         1\nAlex                  0.548814  0.715189\nPriya                 0.602763  0.544883\nYichen                0.423655  0.645894\nSally                 0.437587  0.891773\nJuan                  0.963663  0.383442\nThe Avengers          0.791725  0.528895\nWhen Harry Met Sally  0.568045  0.925597\nSilence of the Lambs  0.071036  0.087129\nShawshank Redemption  0.020218  0.832620\nThe Hangover          0.778157  0.870012\nThe Godfather         0.978618  0.799159\n\n\n\n# Here is one example of how to go through rows of a ratings matrix\nfor index, rating in ratings.iterrows():\n    user  = rating['user']\n    movie = rating['movie']\n    score = rating['ratings']\n    print(f\"{user} gave {movie} a score of {score}\")\n\nAlex gave The Avengers a score of 3.0\nPriya gave The Avengers a score of 3.5\nYichen gave The Avengers a score of 3.5\nAlex gave When Harry Met Sally a score of 3.0\nSally gave When Harry Met Sally a score of 4.5\nPriya gave When Harry Met Sally a score of 3.0\nYichen gave When Harry Met Sally a score of 3.0\nAlex gave Silence of the Lambs a score of 3.0\nSally gave Silence of the Lambs a score of 4.0\nJuan gave Silence of the Lambs a score of 3.5\nPriya gave Silence of the Lambs a score of 3.0\nYichen gave Silence of the Lambs a score of 2.5\nJuan gave Shawshank Redemption a score of 2.5\nPriya gave Shawshank Redemption a score of 4.0\nYichen gave Shawshank Redemption a score of 4.0\nAlex gave The Hangover a score of 3.0\nSally gave The Hangover a score of 3.5\nPriya gave The Hangover a score of 3.0\nYichen gave The Hangover a score of 2.5\nAlex gave The Godfather a score of 3.0\nPriya gave The Godfather a score of 3.5\n\n\n\n# Here is an example of one way to access rows of V\nfor index, rating in ratings.iterrows():\n    user  = rating['user']\n    movie = rating['movie']\n    print(f\"{user}'s location in V is {V.loc[user].to_numpy()}.\")\n    print(f\"{movie}'s location in V is {V.loc[movie].to_numpy()}.\")\n    print()\n\n# You could also do it in Numpy directly, which will likely lead to much faster SGD updates,\n# but that shouldn't be necessary for problems of this size. Up to you!\n\nAlex's location in V is [0.5488135  0.71518937].\nThe Avengers's location in V is [0.79172504 0.52889492].\n\nPriya's location in V is [0.60276338 0.54488318].\nThe Avengers's location in V is [0.79172504 0.52889492].\n\nYichen's location in V is [0.4236548  0.64589411].\nThe Avengers's location in V is [0.79172504 0.52889492].\n\nAlex's location in V is [0.5488135  0.71518937].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nSally's location in V is [0.43758721 0.891773  ].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nPriya's location in V is [0.60276338 0.54488318].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nYichen's location in V is [0.4236548  0.64589411].\nWhen Harry Met Sally's location in V is [0.56804456 0.92559664].\n\nAlex's location in V is [0.5488135  0.71518937].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nSally's location in V is [0.43758721 0.891773  ].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nJuan's location in V is [0.96366276 0.38344152].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nPriya's location in V is [0.60276338 0.54488318].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nYichen's location in V is [0.4236548  0.64589411].\nSilence of the Lambs's location in V is [0.07103606 0.0871293 ].\n\nJuan's location in V is [0.96366276 0.38344152].\nShawshank Redemption's location in V is [0.0202184  0.83261985].\n\nPriya's location in V is [0.60276338 0.54488318].\nShawshank Redemption's location in V is [0.0202184  0.83261985].\n\nYichen's location in V is [0.4236548  0.64589411].\nShawshank Redemption's location in V is [0.0202184  0.83261985].\n\nAlex's location in V is [0.5488135  0.71518937].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nSally's location in V is [0.43758721 0.891773  ].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nPriya's location in V is [0.60276338 0.54488318].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nYichen's location in V is [0.4236548  0.64589411].\nThe Hangover's location in V is [0.77815675 0.87001215].\n\nAlex's location in V is [0.5488135  0.71518937].\nThe Godfather's location in V is [0.97861834 0.79915856].\n\nPriya's location in V is [0.60276338 0.54488318].\nThe Godfather's location in V is [0.97861834 0.79915856].\n\n\n\n\n\n15.3.4 Train your Bilinear Model using SGD\n\n# Your Model building and training code here!\n\n\n\n15.3.5 Assessing your accuracy\nLet’s predict the ratings for the missing entries using our (randomly initialized) model.\n\nfor index, rating in missing_ratings.iterrows():\n    user  = rating['user']\n    movie = rating['movie']\n    prediction = np.dot(V.loc[user],V.loc[movie])+mu\n    print(f\"Prediction: {user} will rate {movie}: {prediction:.2f}\")\n\nPrediction: Sally will rate The Avengers: 3.32\nPrediction: Juan will rate The Avengers: 3.47\nPrediction: Juan will rate When Harry Met Sally: 3.40\nPrediction: Alex will rate Shawshank Redemption: 3.11\nPrediction: Sally will rate Shawshank Redemption: 3.25\nPrediction: Juan will rate The Hangover: 3.58\nPrediction: Sally will rate The Godfather: 3.64\nPrediction: Juan will rate The Godfather: 3.75\nPrediction: Yichen will rate The Godfather: 3.43",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Problem Set 1</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#problem-set-2-part-1-automatic-differentiation",
    "href": "problems/ps2.html#problem-set-2-part-1-automatic-differentiation",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "16.1 Problem Set 2 Part 1: Automatic Differentiation",
    "text": "16.1 Problem Set 2 Part 1: Automatic Differentiation\n\n16.1.1 Overview\nThis problem set is designed for independent exploration of automatic differentiation (AD) and its applications.\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import optimize\nimport time\n\nsns.set_context('notebook')\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x27affd53c30&gt;",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#problem-1-manual-forward-and-backward-mode-ad",
    "href": "problems/ps2.html#problem-1-manual-forward-and-backward-mode-ad",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "16.2 Problem 1: Manual Forward and Backward Mode AD",
    "text": "16.2 Problem 1: Manual Forward and Backward Mode AD\n\n16.2.1 Background\nRecall, the main idea behind automatic differentiation is to apply the chain rule systematically to computational operations.\nThere are two main ‘modes’:\nForward Mode AD: Propagates derivatives forward through the computation graph alongside the function values. If you have a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), forward mode computes one column of the Jacobian matrix per pass.\nBackward Mode AD: Propagates derivatives backward through the computation graph (this is the same as backpropagation used in neural networks). Backward mode computes one row of the Jacobian matrix per pass.\n\n\n16.2.2 The Problem\nConsider the following function with 2 inputs and 2 outputs:\n\\[\n\\begin{aligned}\nf_1(x_1, x_2) &= x_1^2 + \\sin(x_2) \\\\\nf_2(x_1, x_2) &= x_1 \\cdot x_2 + \\exp(x_1)\n\\end{aligned}\n\\]\nWe want to compute the Jacobian matrix:\n\\[\nJ = \\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\\n\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2}\n\\end{bmatrix}\n\\]\nevaluated at the point \\((x_1, x_2) = (2, \\pi)\\).\n\n\n16.2.3 Part A: Forward Mode AD\nIn forward mode, you augment each variable with its derivative. For each variable \\(v\\), you track both: - \\(v\\) (the value) - \\(\\dot{v}\\) (the derivative)\nExample: For \\(v = x_1^2\\), we have: - Value: \\(v = x_1^2 = 2^2 = 4\\) - Derivative: \\(\\dot{v} = 2x_1 \\cdot \\dot{x}_1 = 2(2)(1) = 4\\) (if we seeded with \\(\\dot{x}_1 = 1\\))\nYour Task: Compute the Jacobian using forward mode automatic differentiation. You should:\n\nDraw the computational graph for both \\(f_1\\) and \\(f_2\\), showing all intermediate variables\nPerform the forward pass to compute function values at \\((2, \\pi)\\)\nPerform forward mode AD with seed vector \\(\\dot{x} = [1, 0]^T\\) (What does this compute?)\nPerform forward mode AD with seed vector \\(\\dot{x} = [0, 1]^T\\) (What does this compute?)\nAssemble the complete Jacobian matrix\n\nShow all intermediate computations.\nStudent Response:\n[Write out your computational graph here (or on paper, possibly)]\n[Insert your forward mode AD calculations here]\nJacobian (Forward Mode): \\[\nJ = \\begin{bmatrix}\n? & ? \\\\\n? & ?\n\\end{bmatrix}\n\\]\n\n\n16.2.4 Part B: Backward Mode AD\nIn backward mode (backpropagation), you first compute all the forward values, then propagate derivatives backward. For each variable \\(v\\), you compute: - \\(\\bar{v} = \\frac{\\partial f}{\\partial v}\\) (the adjoint or “sensitivity” of \\(f\\) with respect to \\(v\\))\nExample: For \\(f_1 = v_3\\) where \\(v_3 = v_1 + v_2\\): - If \\(\\bar{v}_3 = 1\\) (seed value for output) - Then \\(\\bar{v}_1 = \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_1} = 1 \\cdot 1 = 1\\) - And \\(\\bar{v}_2 = \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_2} = 1 \\cdot 1 = 1\\)\nYour Task: Compute the same Jacobian using backward mode automatic differentiation. You should:\n\nUse the same computational graph from Part A\nPerform backward mode AD starting from \\(f_1\\) with seed vector \\(\\bar{f} = [1, 0]^T\\) (What does this compute?)\nPerform backward mode AD starting from \\(f_2\\) with seed vector \\(\\bar{f} = [0, 1]^T\\) (What does this compute?)\nAssemble the complete Jacobian matrix\n\nShow all intermediate computations.\nStudent Response:\n[Insert your backward mode AD calculations here]\nJacobian (Backward Mode, should match Forward Mode if done correctly): \\[\nJ = \\begin{bmatrix}\n? & ? \\\\\n? & ?\n\\end{bmatrix}\n\\]\n\n\n16.2.5 Part C: Method Comparison and Understanding\nNow that you’ve computed the Jacobian both ways, let’s reflect on the computational cost.\nQuestion 1: How many forward passes did you need to compute the complete Jacobian using forward mode AD?\nStudent Response: [Insert your answer here]\nQuestion 2: How many backward passes did you need to compute the complete Jacobian using backward mode AD?\nStudent Response: [Insert your answer here]\nQuestion 3: For this problem, is there a preferred method (forward vs backward AD) in terms of computational efficiency?\nStudent Response: [Insert your answer here]",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#problem-2-aircraft-external-fuel-tank-design-optimization",
    "href": "problems/ps2.html#problem-2-aircraft-external-fuel-tank-design-optimization",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "16.3 Problem 2: Aircraft External Fuel Tank Design Optimization",
    "text": "16.3 Problem 2: Aircraft External Fuel Tank Design Optimization\n\n16.3.1 Background\nThis problem is adapted, with modification, from Martins, J. R. R. A., & Ning, A. (2021). Engineering Design Optimization, Cambridge University Press (page 218, problem 5.11).\nIn aerospace engineering, external fuel tanks are often used to extend aircraft range. These streamlined containers must carry a specific volume of fuel while minimizing aerodynamic drag.\nThe Design Problem: A jet aircraft needs to carry a streamlined external fuel tank with a required volume. We want to minimize the drag of an ellipsoid-shaped tank by controlling: - \\(l\\): length (semi-axis along the flow direction) - \\(d\\): diameter (semi-axis perpendicular to flow)\nGeometry Equations:\nFor an ellipsoid, the surface area is: \\[\nS = \\frac{\\pi}{2}d^{2}\\left(1 + \\frac{l}{d\\sqrt{1-\\frac{d^{2}}{l^{2}}}}\\arcsin\\left(\\sqrt{1-\\frac{d^{2}}{l^{2}}}\\right)\\right)\n\\]\nAnd the volume is: \\[\nV = \\frac{\\pi}{6}d^{2}l\n\\]\nAerodynamics:\nThe drag force on the tank is given by: \\[\nD = \\frac{1}{2}\\rho U^{2} C_{D} S\n\\]\nwhere: - \\(\\rho\\) = air density = 0.312 kg/m³ (at 12,000m altitude) - \\(U\\) = velocity = 200 m/s - \\(C_D\\) = drag coefficient - \\(S\\) = surface area\nThe drag coefficient for an ellipsoid can be estimated (from Martins & Ning) as: \\[\nC_D = C_f \\cdot \\left[1 + 1.5\\left(\\frac{d}{l}\\right)^{3/2} + 7\\left(\\frac{d}{l}\\right)^{3}\\right]\n\\]\nwhere \\(C_f\\) = 0.0035 (skin friction coefficient).\nEngineering Insight: - Long, slender tanks (small \\(d/l\\)) have less drag but require more material - Short, fat tanks (large \\(d/l\\)) have more drag but are more compact - We need to maintain a minimum volume for fuel storage\nLet’s first visualize what we’re designing!\n\n# Interactive visualization of ellipsoid fuel tank\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_ellipsoid(l, d, title=\"Ellipsoid Fuel Tank\"):\n    \"\"\"Visualize the ellipsoid fuel tank design\"\"\"\n    fig = plt.figure(figsize=(12, 5))\n    \n    # 3D view\n    ax1 = fig.add_subplot(121, projection='3d')\n    \n    # Generate ellipsoid surface\n    u = np.linspace(0, 2 * np.pi, 50)\n    v = np.linspace(0, np.pi, 50)\n    x = l * np.outer(np.cos(u), np.sin(v))\n    y = d/2 * np.outer(np.sin(u), np.sin(v))\n    z = d/2 * np.outer(np.ones(np.size(u)), np.cos(v))\n    \n    # Plot surface\n    ax1.plot_surface(x, y, z, alpha=0.7, cmap='viridis', edgecolor='none')\n    ax1.set_xlabel('Length (l)', fontsize=8)\n    ax1.set_ylabel('Diameter (d)', fontsize=8)\n    ax1.set_zlabel('Diameter (d)', fontsize=8)\n    ax1.set_title(title, fontsize=12)\n    ax1.set_box_aspect([l, d/2, d/2])\n    \n    # Side view showing dimensions\n    ax2 = fig.add_subplot(122)\n    theta = np.linspace(0, 2*np.pi, 100)\n    x_side = l * np.cos(theta)\n    y_side = d/2 * np.sin(theta)\n    ax2.plot(x_side, y_side, 'b-', linewidth=2.5)\n    ax2.fill(x_side, y_side, alpha=0.3)\n    ax2.arrow(0, 0, l, 0, head_width=0.1, head_length=0.1, fc='red', ec='red', linewidth=2)\n    ax2.text(l/2, -0.3, f'l = {l:.2f} m', fontsize=12, ha='center', color='red', fontweight='bold')\n    ax2.arrow(0, 0, 0, d/2, head_width=0.1, head_length=0.05, fc='blue', ec='blue', linewidth=2)\n    ax2.arrow(0, 0, 0, -d/2, head_width=0.1, head_length=0.05, fc='blue', ec='blue', linewidth=2)\n    ax2.text(0.3, d/4, f'd = {d:.2f} m', fontsize=12, color='blue', fontweight='bold')\n    ax2.set_xlabel('x (flow direction)', fontsize=11)\n    ax2.set_ylabel('y', fontsize=11)\n    ax2.set_title('Side View with Dimensions', fontsize=12)\n    ax2.axis('equal')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Example: visualize a candidate design\nl_example = 2.0  # meters\nd_example = 0.8  # meters\nplot_ellipsoid(l_example, d_example, \"Example Fuel Tank Design\")\n\n# Compute properties for this design\nV_example = (np.pi/6) * d_example**2 * l_example\nprint(f\"\\nExample Design Properties:\")\nprint(f\"  Length (l) = {l_example:.2f} m\")\nprint(f\"  Diameter (d) = {d_example:.2f} m\")\nprint(f\"  Volume = {V_example:.4f} m³\")\nprint(f\"  Aspect ratio (l/d) = {l_example/d_example:.2f}\")\n\n\n\n\n\n\n\n\n\nExample Design Properties:\n  Length (l) = 2.00 m\n  Diameter (d) = 0.80 m\n  Volume = 0.6702 m³\n  Aspect ratio (l/d) = 2.50\n\n\n\n\n16.3.2 Part A: Computing Derivatives\nAs engineers, we need to compute the derivatives of the output objectives (drag \\(D\\) and volume \\(V\\)) with respect to the design inputs (length \\(l\\) and diameter \\(d\\)). Although we could do this analytically using calculus, we want to test the best method for computing derivatives so we can adapt when problems become more complex.\nDesign Point: Let’s analyze a baseline design with \\(l = 2.5\\) m and \\(d = 0.8\\) m.\nFirst, let’s implement the objective functions:\n\n# Constants\nRHO = 0.312  # Air density at 12,000m (kg/m³)\nU = 200.0    # Velocity (m/s)\nC_F = 0.0035 # Skin friction coefficient\n\n# NumPy implementations\ndef surface_area(l, d):\n    \"\"\"Ellipsoid surface area (numpy version)\"\"\"\n    # Ensure l &gt; d to avoid domain errors in arcsin\n    ratio_sq = (d / l) ** 2\n    if ratio_sq &gt;= 1.0:\n        # Degenerate case - return sphere surface area as approximation\n        return 4 * np.pi * (d/2)**2\n    sqrt_term = np.sqrt(1 - ratio_sq)\n    S = (np.pi / 2) * d**2 * (1 + (l / (d * sqrt_term)) * np.arcsin(sqrt_term))\n    return S\n\ndef volume(l, d):\n    \"\"\"Ellipsoid volume (numpy version)\"\"\"\n    return (np.pi / 6) * d**2 * l\n\ndef drag_coefficient(l, d):\n    \"\"\"Drag coefficient for ellipsoid (numpy version)\"\"\"\n    ratio = d / l\n    C_D = C_F * (1 + 1.5 * ratio**(3/2) + 7 * ratio**3)\n    return C_D\n\ndef drag(l, d):\n    \"\"\"Total drag force (numpy version)\"\"\"\n    S = surface_area(l, d)\n    C_D = drag_coefficient(l, d)\n    D = 0.5 * RHO * U**2 * C_D * S\n    return D\n\n# Test at baseline design - use a design where l &gt; d\nl_baseline = 2.5\nd_baseline = 0.8\n\nS_baseline = surface_area(l_baseline, d_baseline)\nV_baseline = volume(l_baseline, d_baseline)\nCD_baseline = drag_coefficient(l_baseline, d_baseline)\nD_baseline = drag(l_baseline, d_baseline)\n\nprint(f\"Baseline Design (l={l_baseline}m, d={d_baseline}m):\")\nprint(f\"  Surface area: {S_baseline:.4f} m²\")\nprint(f\"  Volume: {V_baseline:.4f} m³\")\nprint(f\"  Drag coefficient: {CD_baseline:.6f}\")\nprint(f\"  Drag force: {D_baseline:.2f} N\")\nprint(f\"  Aspect ratio l/d: {l_baseline/d_baseline:.2f}\")\n\nBaseline Design (l=2.5m, d=0.8m):\n  Surface area: 5.1339 m²\n  Volume: 0.8378 m³\n  Drag coefficient: 0.005253\n  Drag force: 168.29 N\n  Aspect ratio l/d: 3.12\n\n\n\n\n16.3.3 Understanding Backward Mode AD\nBackward mode AD works by: 1. Forward pass: Compute all intermediate values from inputs to output 2. Backward pass: Starting from the output, propagate gradients backward using the chain rule\nIn our case we have a function with 2 inputs (\\(l\\), \\(d\\)) and 1 output (\\(D\\), drag force). Let’s focus on computing the gradient of drag:\n\\[\n\\nabla D = \\begin{bmatrix}\n\\frac{\\partial D}{\\partial l} \\\\\n\\frac{\\partial D}{\\partial d}\n\\end{bmatrix}\n\\]\nFor each intermediate variable \\(v\\), we compute its adjoint (sensitivity): \\[\n\\bar{v} = \\frac{\\partial D}{\\partial v}\n\\]\nThe adjoint tells us “how much does the output \\(D\\) change if we perturb \\(v\\)?”\nYour Task: Compute this gradient using backward mode automatic differentiation (also called reverse-mode AD or backpropagation).\n\n\n16.3.4 Step 1: Draw the Computational Graph\nFirst, break down the drag computation into elementary operations. Here’s a template to get you started:\nInputs: l, d\n\nIntermediate calculations example (fill in the formulas):\nv₁ = d/l                     (diameter-to-length ratio)\nv₂ = v₁²                     \nv₃ = 1 - v₂                  \nv₄ = √v₃                     \nv₅ = arcsin(v₄)              \nv₆ = d²                      \nv₇ = d·v₄                   \nv₈ = l/v₇                   \nv₉ = v₈·v₅                  \nv₁₀ = 1 + v₉                \n...\n[Continue adding intermediate variables until you reach D]\nTask 1a: Complete the computational graph by identifying ALL intermediate variables from the inputs (\\(l\\), \\(d\\)) to the final output (\\(D\\)). Number them sequentially (v₁, v₂, v₃, …, v_N).\nStudent Response: [Draw or write out your complete computational graph here]\n\n\n16.3.5 Step 2: Forward Pass\nEvaluate all intermediate variables at the baseline design point: \\(l = 2.5\\) m, \\(d = 0.8\\) m.\nTask 1b: Compute the numerical values of all intermediate variables in your graph.\nStudent Response:\nl = 2.5\nd = 0.8\nv₁ = ?\nv₂ = ?\nv₃ = ?\n...\nD = ?\n\n\n16.3.6 Step 3: Backward Pass\nNow propagate gradients backward! Start with \\(\\bar{D} = 1\\) (the gradient of \\(D\\) with respect to itself), then work backward through the graph.\nTask 1c: Compute all adjoints working backward from \\(\\bar{D} = 1\\) to \\(\\bar{l}\\) and \\(\\bar{d}\\).\nStudent Response:\nBackward pass starting from D:\nD̄ = 1  (seed value)\n\n[Work backward through your computational graph]\nv̄_N = ?\nv̄_{N-1} = ?\n...\nv̄₃ = ?\nv̄₂ = ?\nv̄₁ = ?\n\nFinal gradients (these are what we want!):\n∂D/∂l = l̄ = ?\n∂D/∂d = d̄ = ?\n\n\n16.3.7 Step 4: Interpretation\nTask 1d: Once you’ve computed the gradients, interpret them physically:\nQuestion 1: What does the value of \\(\\frac{\\partial D}{\\partial l}\\) imply physically? (If we increase length, does drag increase or decrease?)\nStudent Response: [Insert your answer here]\nQuestion 2: What does the value of \\(\\frac{\\partial D}{\\partial d}\\) imply physically?\nStudent Response: [Insert your answer here]\nQuestion 3: Which variable has a larger effect on drag? Does this make engineering sense?\nStudent Response: [Insert your answer here]\n\n\n16.3.8 Part B: Implementing AD with PyTorch\nNow let’s implement automatic differentiation using PyTorch and time both the forward and backward passes.\nYour Task: 1. Reimplement your functions using PyTorch operations (torch tensors) 2. Compute the Jacobian matrix using PyTorch’s autograd 3. Time the forward pass (function evaluation) 4. Time the backward pass (gradient computation)\n\n# Student Task: Implement PyTorch versions of the functions\n\ndef surface_area_torch(l, d):\n    \"\"\"PyTorch version of surface_area\"\"\"\n    # TODO: Implement using torch operations (torch.arcsin, torch.sqrt, etc.)\n    pass\n\ndef volume_torch(l, d):\n    \"\"\"PyTorch version of volume\"\"\"\n    # TODO: Implement\n    pass\n\ndef drag_coefficient_torch(l, d):\n    \"\"\"PyTorch version of drag_coefficient\"\"\"\n    # TODO: Implement\n    pass\n\ndef drag_torch(l, d):\n    \"\"\"PyTorch version of drag\"\"\"\n    # TODO: Implement\n    pass\n\n# Compute Jacobian using PyTorch\n# Hints:\n# 1. Create torch tensors for l and d with requires_grad=True\n# 2. Compute both outputs (D and V)\n# 3. Use torch.autograd.grad or .backward() to get gradients\n# 4. For a Jacobian with multiple outputs, you need to call backward() for each output\n\nx = torch.tensor([l_baseline, d_baseline], requires_grad=True, dtype=torch.float64)\nl_t, d_t = x[0], x[1]\n\n# First row: gradients of D\nD = drag_torch(l_t, d_t)\ngrad_D = torch.autograd.grad(D, x, retain_graph=True)[0]\n\n# Second row: gradients of V  \nV = volume_torch(l_t, d_t)\ngrad_V = torch.autograd.grad(V, x)[0]\n\njacobian = torch.stack([grad_D, grad_V]).numpy()\n\nprint(\"\\nJacobian Matrix (from PyTorch AD):\")\nprint(f\"  ∂D/∂l = {jacobian[0,0]:.6f}\")\nprint(f\"  ∂D/∂d = {jacobian[0,1]:.6f}\")\nprint(f\"  ∂V/∂l = {jacobian[1,0]:.6f}\")\nprint(f\"  ∂V/∂d = {jacobian[1,1]:.6f}\")\n\n# Time the forward and backward passes\nn_reps = 10000\n\n# Time forward pass\nstart = time.time()\nfor _ in range(n_reps):\n    x = torch.tensor([l_baseline, d_baseline], requires_grad=True, dtype=torch.float64)\n    D = drag_torch(x[0], x[1])\n    V = volume_torch(x[0], x[1])\ntime_forward = (time.time() - start) / n_reps\n\n# Time backward pass\nstart = time.time()\nfor _ in range(n_reps):\n    x = torch.tensor([l_baseline, d_baseline], requires_grad=True, dtype=torch.float64)\n    D = drag_torch(x[0], x[1])\n    V = volume_torch(x[0], x[1])\n    grad_D = torch.autograd.grad(D, x, retain_graph=True)[0]\n    grad_V = torch.autograd.grad(V, x)[0]\ntime_backward = (time.time() - start) / n_reps\n\nprint(f\"\\nTiming Results:\")\nprint(f\"  Forward pass:  {time_forward*1e6:.2f} µs\")\nprint(f\"  Backward pass: {time_backward*1e6:.2f} µs\")\nprint(f\"  Overhead of AD: {(time_backward/time_forward - 1)*100:.1f}%\")\n\nReflection: Did you observe the backward pass taking more time than the forward pass? If you did, does this make sense?\n\n\n16.3.9 Part C: Constrained Optimization\nNow for the real engineering problem! The aircraft requires the fuel tank to have a volume of at least 0.5 m³ to carry enough fuel. This gives us a constraint:\n\\[\nV \\geq V_{\\text{req}} = 0.5 \\text{ m}^3\n\\]\nOr equivalently (for standard optimization form): \\[\ng(l, d) = V_{\\text{req}} - V(l, d) \\leq 0\n\\]\nOptimization Problem: \\[\n\\begin{aligned}\n\\text{minimize} \\quad & D(l, d) \\\\\n\\text{subject to} \\quad & V(l, d) \\geq 0.5 \\\\\n& l &gt; 0, \\quad d &gt; 0\n\\end{aligned}\n\\]\nYour Task: Solve this optimization problem using three different methods for computing gradients: 1. Finite differences (from Problem 3, Part B) 2. PyTorch automatic differentiation 3. Third Method: Could be the SciPy default gradient computation (SciPy defaults to a “2-point finite difference estimation with an absolute step size”), or another method, such as complex-step method (which can be done manually but is also built in to SciPy), another FD method, or an analytical gradient method\nWe’ll use scipy’s SLSQP optimizer, which is designed for constrained optimization problems. The code is provided below - your job is to implement the gradient functions and compare the results. When you compare the results, it may be useful to run multiple trials\n\nV_REQUIRED = 0.5  # m³\n\n# Wrapper functions for scipy (which expects 1D arrays)\ndef drag_wrapper(x):\n    \"\"\"Objective function: drag\"\"\"\n    l, d = x[0], x[1]\n    return drag(l, d)\n\ndef volume_constraint(x):\n    \"\"\"Constraint function: V - V_req &gt;= 0\"\"\"\n    l, d = x[0], x[1]\n    return volume(l, d) - V_REQUIRED\n\n# Student Task: Implement gradient functions\n\ndef drag_gradient_fd(x, eps=1e-7):\n    \"\"\"\n    Compute gradient of drag using finite differences.\n    Use the finite_difference_gradient function from Problem 3!\n    \"\"\"\n    # TODO: Implement\n    pass\n\ndef drag_gradient_torch(x):\n    \"\"\"\n    Compute gradient of drag using PyTorch AD.\n    \n    Hint: Convert x to torch tensor, compute drag, call backward(), extract gradient\n    \"\"\"\n    # TODO: Implement\n    pass\n\ndef constraint_gradient_fd(x, eps=1e-7):\n    \"\"\"Gradient of volume constraint using finite differences\"\"\"\n    # TODO: Implement\n    pass\n\ndef constraint_gradient_torch(x):\n    \"\"\"Gradient of volume constraint using PyTorch AD\"\"\"\n    # TODO: Implement\n    pass\n\n# Depending on what you do, you can also manually define the third method gradients here. \n# If you are using SciPy's built in gradient estimation methods, you do not need to define these functions.\n\n# Initial guess (start from baseline design)\nx0 = np.array([l_baseline, d_baseline])\n\nprint(\"=\"*70)\nprint(\"SOLVING CONSTRAINED OPTIMIZATION PROBLEM\")\nprint(\"=\"*70)\nprint(f\"Objective: Minimize drag\")\nprint(f\"Constraint: Volume &gt;= {V_REQUIRED} m³\")\nprint(f\"Initial design: l={x0[0]:.2f}m, d={x0[1]:.2f}m\")\nprint(f\"Initial drag: {drag_wrapper(x0):.2f} N\")\nprint(f\"Initial volume: {volume(x0[0], x0[1]):.4f} m³\")\nprint()\n\n======================================================================\nSOLVING CONSTRAINED OPTIMIZATION PROBLEM\n======================================================================\nObjective: Minimize drag\nConstraint: Volume &gt;= 0.5 m³\nInitial design: l=2.50m, d=0.80m\nInitial drag: 168.29 N\nInitial volume: 0.8378 m³\n\n\n\nImplementation Note: The optimization problem below may converge to different local minima depending on the initial guess. For a more robust analysis, consider running multiple trials with random initial guesses and computing both the average and best results.\n\n# Method 1: Finite Differences\nprint(\"Method 1: Optimization with Finite Difference Gradients\")\nprint(\"-\"*70)\nconstraint_fd = {'type': 'ineq', 'fun': volume_constraint, 'jac': constraint_gradient_fd}\nbounds = [(0.1, 10.0), (0.1, 10.0)]  # l and d must be positive\n\nresult_fd = optimize.minimize(\n    drag_wrapper,\n    x0=x0,\n    method='SLSQP',\n    jac=drag_gradient_fd,\n    constraints=constraint_fd,\n    bounds=bounds,\n    options={'disp': True, 'maxiter': 100} # If you are averaging over many trials, consider turning disp off (this prints convergence information)\n)\n\nl_opt_fd, d_opt_fd = result_fd.x\nprint(f\"\\nOptimal design (FD):\")\nprint(f\"  l = {l_opt_fd:.4f} m\")\nprint(f\"  d = {d_opt_fd:.4f} m\")\nprint(f\"  Drag = {drag_wrapper(result_fd.x):.2f} N\")\nprint(f\"  Volume = {volume(l_opt_fd, d_opt_fd):.4f} m³\")\nprint(f\"  Function evaluations: {result_fd.nfev}\")\nprint(f\"  Gradient evaluations: {result_fd.njev}\")\nprint()\n\n\n# Method 2: PyTorch AD\nprint(\"Method 2: Optimization with PyTorch AD Gradients\")\nprint(\"-\"*70)\nconstraint_torch = {'type': 'ineq', 'fun': volume_constraint, 'jac': constraint_gradient_torch}\n\nresult_torch = optimize.minimize(\n    drag_wrapper,\n    x0=x0,\n    method='SLSQP',\n    jac=drag_gradient_torch,\n    constraints=constraint_torch,\n    bounds=bounds,\n    options={'disp': True, 'maxiter': 100} \n)\n\nl_opt_torch, d_opt_torch = result_torch.x\nprint(f\"\\nOptimal design (PyTorch AD):\")\nprint(f\"  l = {l_opt_torch:.4f} m\")\nprint(f\"  d = {d_opt_torch:.4f} m\")\nprint(f\"  Drag = {drag_wrapper(result_torch.x):.2f} N\")\nprint(f\"  Volume = {volume(l_opt_torch, d_opt_torch):.4f} m³\")\nprint(f\"  Function evaluations: {result_torch.nfev}\")\nprint(f\"  Gradient evaluations: {result_torch.njev}\")\nprint()\n\n\n# Method 3: Your Choice\nprint(\"Method 3: Optimization with [Third Method]\")\nprint(\"-\"*70)\n# bogus placeholder for third method constraint\nconstraint_gradient_third_method = None  # Implement OR use a built in method from the SciPy documentation\nconstraint_third_method = {'type': 'ineq', 'fun': volume_constraint, 'jac': constraint_gradient_third_method}\n\nresult_third_method = optimize.minimize(\n    drag_wrapper,\n    x0=x0,\n    method='SLSQP',\n    jac=constraint_gradient_third_method,  # Actually implement this with your third method gradient function if applicable\n    constraints=constraint_third_method,\n    bounds=bounds,\n    options={'disp': True, 'maxiter': 100}\n)\n\nl_opt_third_method, d_opt_third_method = result_third_method.x\nprint(f\"\\nOptimal design ([Third Method]):\")\nprint(f\"  l = {l_opt_third_method:.4f} m\")\nprint(f\"  d = {d_opt_third_method:.4f} m\")\nprint(f\"  Drag = {drag_wrapper(result_third_method.x):.2f} N\")\nprint(f\"  Volume = {volume(l_opt_third_method, d_opt_third_method):.4f} m³\")\nprint(f\"  Function evaluations: {result_third_method.nfev}\")\nprint(f\"  Gradient evaluations: {result_third_method.njev}\")\nprint()\n\nAverage and find the best results\n\nprint(\"=\"*70)\nprint(\"OPTIMIZATION RESULTS SUMMARY (BEST TRIALS)\")\nprint(\"=\"*70)\nprint(f\"{'Method':&lt;20} {'l (m)':&lt;10} {'d (m)':&lt;10} {'Drag (N)':&lt;12} {'Func Evals':&lt;12} {'Grad Evals':&lt;12}\")\nprint(\"-\"*70)\n\n# bogus values:\nl_opt_fd_best = -999\nd_opt_fd_best = -999\nnfev_fd_best = -999\nnjev_fd_best = -999\nl_opt_torch_best = -999\nd_opt_torch_best = -999\nnfev_torch_best = -999\nnjev_torch_best = -999\nl_opt_third_method_best = -999\nd_opt_third_method_best = -999\nnfev_third_method_best = -999\nnjev_third_method_best = -999\n\nl_opt_fd_avg = -999\nd_opt_fd_avg = -999\nnfev_fd_avg = -999\nnjev_fd_avg = -999\nl_opt_torch_avg = -999\nd_opt_torch_avg = -999\nnfev_torch_avg = -999\nnjev_torch_avg = -999\nl_opt_third_method_avg = -999\nd_opt_third_method_avg = -999\nnfev_third_method_avg = -999\nnjev_third_method_avg = -999\n\nprint(f\"{'(best) Finite Diff':&lt;20} {l_opt_fd_best:&lt;10.4f} {d_opt_fd_best:&lt;10.4f} {drag_wrapper([l_opt_fd_best, d_opt_fd_best]):&lt;12.2f} {nfev_fd_best:&lt;12} {njev_fd_best:&lt;12}\")\nprint(f\"{'(best) PyTorch AD':&lt;20} {l_opt_torch_best:&lt;10.4f} {d_opt_torch_best:&lt;10.4f} {drag_wrapper([l_opt_torch_best, d_opt_torch_best]):&lt;12.2f} {nfev_torch_best:&lt;12} {njev_torch_best:&lt;12}\")\nprint(f\"{'(best) Third Method':&lt;20} {l_opt_third_method_best:&lt;10.4f} {d_opt_third_method_best:&lt;10.4f} {drag_wrapper([l_opt_third_method_best, d_opt_third_method_best]):&lt;12.2f} {nfev_third_method_best:&lt;12} {njev_third_method_best:&lt;12}\")\nprint(f\"{'Average Finite Diff':&lt;20} {l_opt_fd_avg:&lt;10.4f} {d_opt_fd_avg:&lt;10.4f} {drag_wrapper([l_opt_fd_avg, d_opt_fd_avg]):&lt;12.2f} {nfev_fd_avg:&lt;12.1f} {njev_fd_avg:&lt;12.1f}\")\nprint(f\"{'Average PyTorch AD':&lt;20} {l_opt_torch_avg:&lt;10.4f} {d_opt_torch_avg:&lt;10.4f} {drag_wrapper([l_opt_torch_avg, d_opt_torch_avg]):&lt;12.2f} {nfev_torch_avg:&lt;12.1f} {njev_torch_avg:&lt;12.1f}\")\nprint(f\"{'Average [Third Method]':&lt;20} {l_opt_third_method_avg:&lt;10.4f} {d_opt_third_method_avg:&lt;10.4f} {drag_wrapper([l_opt_third_method_avg, d_opt_third_method_avg]):&lt;12.2f} {nfev_third_method_avg:&lt;12.1f} {njev_third_method_avg:&lt;12.1f}\")\n\n# Accuracies could be done too if you get the analytical solution:\n# This can be useful, since the methods may converge to very similar results\n# You can also try to exclude convergence failures from the averages above based on tolerances\n\n# bogus values:\nl_analytical = -999\nd_analytical = -999\n\nprint(\"\\nAnalytical Values (if derived - see the 'Bonus Side Note' if you are interested):\")\nprint(f\"  l = {l_analytical:.4f} m\")\nprint(f\"  d = {d_analytical:.4f} m\")\nprint(f\"  Drag = {drag_wrapper([l_analytical, d_analytical]):.2f} N\")\nprint(f\"  Volume = {volume(l_analytical, d_analytical):.4f} m³\")\nprint(\"\\nAnalytical Error Comparison (best):\")\nprint(f\"{'Method':&lt;20} {'l Error (m)':&lt;15} {'d Error (m)':&lt;15} {'Drag Error (N)':&lt;15}\")\nprint(\"-\"*70)\nprint(f\"{'(best) Finite Diff':&lt;20} {abs(l_opt_fd_best - l_analytical):&lt;15.6f} {abs(d_opt_fd_best - d_analytical):&lt;15.6f} {abs(drag_wrapper([l_opt_fd_best, d_opt_fd_best]) - drag_wrapper([l_analytical, d_analytical])):&lt;15.6f}\")\nprint(f\"{'(best) PyTorch AD':&lt;20} {abs(l_opt_torch_best - l_analytical):&lt;15.6f} {abs(d_opt_torch_best - d_analytical):&lt;15.6f} {abs(drag_wrapper([l_opt_torch_best, d_opt_torch_best]) - drag_wrapper([l_analytical, d_analytical])):&lt;15.6f}\")\nprint(f\"{'(best) Third Method':&lt;20} {abs(l_opt_third_method_best - l_analytical):&lt;15.6f} {abs(d_opt_third_method_best - d_analytical):&lt;15.6f} {abs(drag_wrapper([l_opt_third_method_best, d_opt_third_method_best]) - drag_wrapper([l_analytical, d_analytical])):&lt;15.6f}\")\nprint(f\"{'Average Finite Diff':&lt;20} {abs(l_opt_fd_avg - l_analytical):&lt;15.6f} {abs(d_opt_fd_avg - d_analytical):&lt;15.6f} {abs(drag_wrapper([l_opt_fd_avg, d_opt_fd_avg]) - drag_wrapper([l_analytical, d_analytical])):&lt;15.6f}\")\nprint(f\"{'Average PyTorch AD':&lt;20} {abs(l_opt_torch_avg - l_analytical):&lt;15.6f} {abs(d_opt_torch_avg - d_analytical):&lt;15.6f} {abs(drag_wrapper([l_opt_torch_avg, d_opt_torch_avg]) - drag_wrapper([l_analytical, d_analytical])):&lt;15.6f}\")\nprint(f\"{'Average [Third Method]':&lt;20} {abs(l_opt_third_method_avg - l_analytical):&lt;15.6f} {abs(d_opt_third_method_avg - d_analytical):&lt;15.6f} {abs(drag_wrapper([l_opt_third_method_avg, d_opt_third_method_avg]) - drag_wrapper([l_analytical, d_analytical])):&lt;15.6f}\")\n\n======================================================================\nOPTIMIZATION RESULTS SUMMARY (BEST TRIALS)\n======================================================================\nMethod               l (m)      d (m)      Drag (N)     Func Evals   Grad Evals  \n----------------------------------------------------------------------\n(best) Finite Diff   -999.0000  -999.0000  650514660.30 -999         -999        \n(best) PyTorch AD    -999.0000  -999.0000  650514660.30 -999         -999        \n(best) Third Method  -999.0000  -999.0000  650514660.30 -999         -999        \nAverage Finite Diff  -999.0000  -999.0000  650514660.30 -999.0       -999.0      \nAverage PyTorch AD   -999.0000  -999.0000  650514660.30 -999.0       -999.0      \nAverage Third Method -999.0000  -999.0000  650514660.30 -999.0       -999.0      \n\nAnalytical Values (if derived - see the 'Bonus Side Note' if you are interested):\n  l = -999.0000 m\n  d = -999.0000 m\n  Drag = 650514660.30 N\n  Volume = -522029549.5442 m³\n\nAnalytical Error Comparison (best):\nMethod               l Error (m)     d Error (m)     Drag Error (N) \n----------------------------------------------------------------------\n(best) Finite Diff   0.000000        0.000000        0.000000       \n(best) PyTorch AD    0.000000        0.000000        0.000000       \n(best) Third Method  0.000000        0.000000        0.000000       \nAverage Finite Diff  0.000000        0.000000        0.000000       \nAverage PyTorch AD   0.000000        0.000000        0.000000       \nAverage Third Method 0.000000        0.000000        0.000000       \n\n\n\n# Visualize initial and optimal designs\nfig, axes = plt.subplots(1, 4, figsize=(18, 5))\n\n\ndesigns = [\n    (l_baseline, d_baseline, \"Initial Design\"),\n    (l_opt_fd_best, d_opt_fd_best, \"Optimal (Finite Diff)\"),\n    (l_opt_torch_best, d_opt_torch_best, \"Optimal (PyTorch AD)\"),\n    (l_opt_third_method_best, d_opt_third_method_best, \"Optimal ([Third Method])\")\n]\n\nprint(l_baseline, d_baseline, l_opt_fd_best, d_opt_fd_best, l_opt_torch_best, d_opt_torch_best)\n\nfor idx, (l, d, title) in enumerate(designs):\n    ax = axes[idx]\n    theta = np.linspace(0, 2*np.pi, 100)\n    x_side = l * np.cos(theta)\n    y_side = d/2 * np.sin(theta)\n    ax.plot(x_side, y_side, 'b-', linewidth=2.5)\n    ax.fill(x_side, y_side, alpha=0.3)\n    ax.set_xlabel('x (flow direction) [m]', fontsize=10)\n    ax.set_ylabel('y [m]', fontsize=10)\n    drag_val = drag(l, d)\n    vol_val = volume(l, d)\n    ax.set_title(f'{title}\\nDrag: {drag_val:.1f}N, Vol: {vol_val:.3f}m³, l/d: {l/d:.1f}', fontsize=11)\n    ax.axis('equal')\n    ax.grid(True, alpha=0.3)\n    # Add aspect markers\n    ax.axhline(0, color='k', linewidth=0.5, alpha=0.5)\n    ax.axvline(0, color='k', linewidth=0.5, alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Initial aspect ratio: {x0[0]/x0[1]:.1f}:1\")\nprint(f\"Optimal aspect ratio: {l_opt_torch_best/d_opt_torch_best:.1f}:1\")\nprint(f\"Drag reduction: {(1 - drag(l_opt_torch_best, d_opt_torch_best)/drag(x0[0], x0[1]))*100:.1f}%\")\n\n2.5 0.8 -999 -999 -999 -999\n\n\n\n\n\n\n\n\n\nInitial aspect ratio: 3.1:1\nOptimal aspect ratio: 1.0:1\nDrag reduction: -386548751.7%\n\n\n\n\n16.3.10 Bonus Side Note:\nAs a smart engineer, you may have realized that the volume constraint is probably active at the optimum (i.e., the tank will just meet the volume requirement). With this information, you can reduce the problem to a single-variable optimization by expressing one variable in terms of the other using the volume constraint, allowing you to solve this analytically without the need for constrained optimization methods.\nIf you want, you can easily derive this, and compare your solution against the analytical one - this could help you compare the numerical accuracy of your optimization results in a more rigorous way.",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#problem-3-bouncing-ball-sensitivity-analysis",
    "href": "problems/ps2.html#problem-3-bouncing-ball-sensitivity-analysis",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "16.4 Problem 3: Bouncing Ball Sensitivity Analysis",
    "text": "16.4 Problem 3: Bouncing Ball Sensitivity Analysis\n\n16.4.1 Background\nOne powerful application of automatic differentiation is sensitivity analysis: understanding how outputs depend on inputs.\nAD makes sensitivity analysis trivial—we just compute gradients! Without AD, you’d need finite differences (slow and inaccurate) or hand-derived formulas (error-prone and not maintainable).\n\n\n16.4.2 The Problem\nConsider a simple physics simulation: a bouncing ball. The simulation has parameters: - g: gravitational acceleration (m/s²) - e: coefficient of restitution - controls energy loss (0 &lt; e &lt; 1) - v0: initial velocity (m/s) - h0: initial height (m)\nWe will use the following parameters: g = 10.0 m/s² e = 0.9 v0 = 0.0 m/s h0 = 10.0 m\nAnd we will use a dt of 0.01 seconds for the simulation time step, for a total of 1000 time steps.\nThe simulation uses a simple Euler integrator with a conditional (bounce detection), making it non-trivial to differentiate by hand!\n\n\ndef bounce_simulation(g, e, v0, h0, num_timesteps=1000):\n    \"\"\"\n    Simulate a bouncing ball using Euler integration.\n    \n    Args:\n        g: gravity\n        e: coefficient of restitution\n        v0: initial velocity\n        h0: initial height\n        num_timesteps: number of timesteps to simulate\n    Returns:\n        v_final: final velocity after simulation\n    \"\"\"\n    v = 0\n    h = 0\n    hprev = h0\n    vprev = v0\n    dt = 0.01 # seconds\n    h_tape = [hprev]\n    t_tape = [0]\n    v_tape = [vprev]\n    \n    for i in range(1, num_timesteps):\n        # # Update position and velocity\n        h = hprev + vprev * dt\n        v = vprev - g * dt\n        \n        # Check for bounce (conditional!)\n        if h &lt;= 0:  \n            h = -h  # Assume ball regains height lost\n            v = -e * v  # Reverse velocity with energy loss\n        \n        hprev = h\n        vprev = v\n        t_tape.append(t_tape[-1] + dt)\n        h_tape.append(h)\n        v_tape.append(v)\n\n    return v, h, h_tape, t_tape, v_tape\n\n# Side note: Since we are using Euler/verlet integration - \n# if you change h0 or another parameter to get a lot of bounces, the error may accumulate significantly.\n# This could result in an unphysical result (e.g., the ball bouncing higher than its initial height).\n# For our purposes here, dt and timesteps have been tuned for the parameters you will try to keep this error small.\ng = 10.0\ne = 0.9\nv0 = 0.0\nh0 = 10.0\n# Test the simulation\nv_final, h_final, h_tape, t_tape, v_tape = bounce_simulation(g, e, v0, h0, 1000)\nprint(f\"Final velocity: {v_final:.2f} m/s\")\nprint(f\"Final height: {h_final:.2f} m\")\nprint(f\"\\nParameters: g={g}, e={e}, v0={v0}, h0={h0}\")\nplt.figure(figsize=(10, 5))\nplt.plot(t_tape, h_tape, label='Height (m)')\nplt.xlabel('Time (s)')\nplt.ylabel('Height (m)')\nv_tape_h0_1 = v_tape\nh_tape_h0_1 = h_tape\n\nFinal velocity: -5.48 m/s\nFinal height: 3.41 m\n\nParameters: g=10.0, e=0.9, v0=0.0, h0=10.0\n\n\n\n\n\n\n\n\n\n\n\n16.4.3 Part A: Sensitivity Analysis\nQuestion: Which parameter has the biggest influence on the final velocity?\nWith AD, we want to compute the sensitivities: \\[\n\\nabla v_{\\text{final}} = \\left[\\frac{\\partial v_{\\text{final}}}{\\partial g}, \\frac{\\partial v_{\\text{final}}}{\\partial e}, \\frac{\\partial v_{\\text{final}}}{\\partial v_0}, \\frac{\\partial v_{\\text{final}}}{\\partial h_0}\\right]\n\\]\nYour Task: Use PyTorch to compute these sensitivities at the final timestep. Also, plot your sensitivities as a function of time. You may want to change the function such that the input is a vector of parameters rather than separate scalars.\n\n# Compute gradient using PyTorch\n# Hint: Call backward() on the final velocity with respect to inputs within a loop over the timesteps\n\npass  # TODO: Implement your PyTorch AD sensitivity analysis here\n\nQuestion 1: Which parameter has the largest influence on the final velocity (largest absolute sensitivity)?\nStudent Response: [Insert your answer here]\nQuestion 2: How do the sensitivities behave as a function of time? What happens to each parameter at and between bounces?\nStudent Response: [Insert your answer here]\nQuestion 3: What happens when we change \\(h_0\\) to 1.0 m? How do the sensitivities change?\nStudent Response: [Insert your answer here]\nQuestion 4: What parameters have the least influence on the final velocity? Is there a flaw in the simulation or AD approach that could explain this - or does everything make sense physically?\nStudent Response: [Insert your answer here]\n\n\n16.4.4 Part B: Diagnosing the Results\nObservation: You may have noticed that the sensitivity to one of the parameters is approximately zero. Let’s investigate this further.\nWe can confirm our suspicions by double-checking our AD result against an approximate sensitivity calculation.\nTask: For the parameter(s) you found to have near-zero sensitivity, compute a sensitivity approximation by perturbing the parameter in question. Plot the sensitvity as a function of time and compare it to the AD result.\n\n# Plot the sensitivity results\n\npass  # TODO: Implement your sensitivity plotting code here\n\nQuestion 1: Is there a discrepancy between the AD and approximated results? If so, why could this be happening? (Hint: are there discontinuities in the simulation or not?)\nStudent Response: [Insert your answer here]\nQuestion 2: If there is a discrepancy, how would you fix it? If not, why does everything make sense physically? (You do not actually need to implement the fix, just describe it. Alternatively, you can try and prove that no discrepancy exists (i.e by writing out an example AD computation through a bounce))\nStudent Response: [Insert your answer here]",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#learning-objectives",
    "href": "problems/ps2.html#learning-objectives",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "17.1 Learning Objectives",
    "text": "17.1 Learning Objectives\nIn this problem set, you will: 1. Train a baseline GAN on 1D airfoil curves 2. Diagnose training issues using metrics and visualizations 3. Select and implement GAN improvements from a “word bank” of modern techniques 4. Hypothesize the impact of each improvement before testing 5. Compare results and understand which techniques help in which scenarios",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#overview-1",
    "href": "problems/ps2.html#overview-1",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "17.2 Overview",
    "text": "17.2 Overview\nGenerative Adversarial Networks are notoriously difficult to train. Over the years, researchers have developed various techniques to improve training stability, output quality, and diversity. In this exercise, you’ll work with a 1D GAN that generates airfoil shapes.\nYou’ll start with a baseline implementation that has known issues, then systematically apply improvements from the literature to see their effects.",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#problem-setup-airfoil-generation",
    "href": "problems/ps2.html#problem-setup-airfoil-generation",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "17.3 Problem Setup: Airfoil Generation",
    "text": "17.3 Problem Setup: Airfoil Generation\nDataset: 1,528 airfoil curves, each represented as 192 (x,y) coordinate pairs\nTask: Generate realistic airfoil shapes\nChallenge: Airfoils must be smooth, physically valid, and diverse",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#setup-and-imports",
    "href": "problems/ps2.html#setup-and-imports",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "17.4 Setup and Imports",
    "text": "17.4 Setup and Imports\n\nimport os\nimport io\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nfrom typing import NamedTuple, Optional\nimport requests\nfrom sklearn.decomposition import PCA\nimport warnings\nimport random\nwarnings.filterwarnings('ignore')\n\n# Set style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.dpi'] = 100\n\n# Set seeds for reproducibility\nSEED = 0\nrandom.seed(SEED)\nnp.random.seed(SEED)\nth.manual_seed(SEED)\nth.cuda.manual_seed(SEED)\nth.cuda.manual_seed_all(SEED)\nth.backends.cudnn.deterministic = True\nth.backends.cudnn.benchmark = False\n\n# Device selection\nif th.backends.mps.is_available():\n    device = th.device(\"mps\")\n    # Set MPS seed\n    th.mps.manual_seed(SEED)\nelif th.cuda.is_available():\n    device = th.device(\"cuda\")\nelse:\n    device = th.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\nprint(f\"Random seed: {SEED}\")\n\nUsing device: mps",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#load-airfoil-dataset",
    "href": "problems/ps2.html#load-airfoil-dataset",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "17.5 Load Airfoil Dataset",
    "text": "17.5 Load Airfoil Dataset\n\n# Load airfoil data\nurl = \"https://github.com/IDEALLab/ML4ME_Textbook/raw/main/part1/airfoil_interp_uniform.npy\"\nresponse = requests.get(url)\nX_airfoils = np.load(io.BytesIO(response.content))  # Shape: (1528, 192, 2)\n\nprint(f\"Dataset shape: {X_airfoils.shape}\")\nprint(f\"Number of airfoils: {X_airfoils.shape[0]}\")\nprint(f\"Points per airfoil: {X_airfoils.shape[1]}\")\n\n# Flatten to (N, 384) for easier handling\nX_flat = X_airfoils.reshape(X_airfoils.shape[0], -1)\n\n# Normalize to [-1, 1] range (important for tanh output)\nX_mean = X_flat.mean(axis=0, keepdims=True)\nX_std = X_flat.std(axis=0, keepdims=True) + 1e-8\nX_normalized = (X_flat - X_mean) / X_std\nX_normalized = np.clip(X_normalized, -3, 3) / 3  # Soft clip to [-1, 1]\n\nprint(f\"\\nNormalized data range: [{X_normalized.min():.3f}, {X_normalized.max():.3f}]\")\n\n# Visualize some airfoils\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\naxes = axes.flatten()\nfor i in range(8):\n    idx = np.random.randint(0, len(X_airfoils))\n    airfoil = X_airfoils[idx]\n    axes[i].plot(airfoil[:, 0], airfoil[:, 1], 'b-', linewidth=2)\n    axes[i].set_aspect('equal')\n    axes[i].set_title(f'Airfoil {idx}', fontsize=11)\n    axes[i].grid(True, alpha=0.3)\n    axes[i].set_xlim(-0.1, 1.1)\n    axes[i].set_ylim(-0.3, 0.3)\nplt.suptitle('Sample Airfoils from Dataset', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nDataset shape: (1528, 192, 2)\nNumber of airfoils: 1528\nPoints per airfoil: 192\n\nNormalized data range: [-1.000, 1.000]",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#convert-to-pytorch-tensor",
    "href": "problems/ps2.html#convert-to-pytorch-tensor",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "17.6 Convert to PyTorch Tensor",
    "text": "17.6 Convert to PyTorch Tensor\n\n# Convert to PyTorch tensor\nX_tensor = th.FloatTensor(X_normalized).to(device)\n\nprint(f\"Tensor shape: {X_tensor.shape}\")\n\nTensor shape: torch.Size([1528, 384])\n\n\n\nclass TrainingHistory(NamedTuple):\n    \"\"\"Store training metrics.\"\"\"\n    d_loss: list\n    g_loss: list\n    d_real_score: list\n    d_fake_score: list\n    diversity: list\n    gradient_penalty: list  # For WGAN-GP\n    \n\ndef compute_diversity(samples: th.Tensor) -&gt; float:\n    \"\"\"Compute diversity using average pairwise L2 distance.\"\"\"\n    samples_flat = samples.reshape(samples.size(0), -1)\n    dists = th.cdist(samples_flat, samples_flat, p=2)\n    n = samples.size(0)\n    if n &lt;= 1:\n        return 0.0\n    # Average of upper triangular (exclude diagonal)\n    return dists.sum().item() / (n * (n - 1))\n\n\ndef plot_pca_coverage(generator, latent_dim: int, title_suffix: str = \"\", n_samples: int = 500):\n    \"\"\"Plot 2D PCA visualization comparing real vs generated distribution.\"\"\"\n    generator.eval()\n    \n    # Generate samples\n    with th.no_grad():\n        z = th.randn(n_samples, latent_dim, device=device)\n        gen_designs_flat = generator(z).cpu().numpy()\n    \n    # Denormalize generated samples\n    gen_designs_flat = gen_designs_flat * 3 * X_std + X_mean\n    \n    # Get real samples (denormalized)\n    real_designs_flat = X_flat\n    \n    # Fit PCA on real data\n    pca = PCA(n_components=2)\n    real_pca = pca.fit_transform(real_designs_flat)\n    gen_pca = pca.transform(gen_designs_flat)\n    \n    # Plot\n    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n    \n    # Plot real data\n    ax.scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.3, s=20, c='blue', label='Real', edgecolors='none')\n    \n    # Plot generated data\n    ax.scatter(gen_pca[:, 0], gen_pca[:, 1], alpha=0.5, s=20, c='red', label='Generated', edgecolors='none')\n    \n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n    ax.set_title(f'2D PCA: Real vs Generated Distribution{title_suffix}', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=11)\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_training_diagnostics(history: TrainingHistory, title_suffix: str = \"\"):\n    \"\"\"Plot comprehensive training diagnostics.\"\"\"\n    fig = plt.figure(figsize=(16, 10))\n    gs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)\n    \n    epochs = np.arange(1, len(history.d_loss) + 1)\n    \n    # Row 1: Losses\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax1.plot(epochs, history.d_loss, label='Discriminator', linewidth=2, alpha=0.8, color='C0')\n    ax1.plot(epochs, history.g_loss, label='Generator', linewidth=2, alpha=0.8, color='C1')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training Losses', fontweight='bold')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    ax2 = fig.add_subplot(gs[0, 1])\n    ax2.plot(epochs, history.d_real_score, label='D(real)', linewidth=2, alpha=0.8, color='C2')\n    ax2.plot(epochs, history.d_fake_score, label='D(fake)', linewidth=2, alpha=0.8, color='C3')\n    ax2.axhline(y=0.5, color='k', linestyle='--', alpha=0.4, linewidth=1.5, label='Random')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Score')\n    ax2.set_title('Discriminator Predictions', fontweight='bold')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    ax2.set_ylim(-0.05, 1.05)\n    \n    ax3 = fig.add_subplot(gs[0, 2])\n    loss_ratio = np.array(history.d_loss) / (np.array(history.g_loss) + 1e-8)\n    ax3.plot(epochs, loss_ratio, linewidth=2, alpha=0.8, color='purple')\n    ax3.axhline(y=1.0, color='k', linestyle='--', alpha=0.4, linewidth=1.5)\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('D_loss / G_loss')\n    ax3.set_title('Loss Ratio (Balance Check)', fontweight='bold')\n    ax3.grid(True, alpha=0.3)\n    \n    # Row 2: Diversity and gradient penalty\n    ax4 = fig.add_subplot(gs[1, 0])\n    ax4.plot(epochs, history.diversity, linewidth=2, alpha=0.8, color='teal')\n    ax4.set_xlabel('Epoch')\n    ax4.set_ylabel('Avg Pairwise Distance')\n    ax4.set_title('Sample Diversity', fontweight='bold')\n    ax4.grid(True, alpha=0.3)\n    \n    if any(gp &gt; 0 for gp in history.gradient_penalty):\n        ax5 = fig.add_subplot(gs[1, 1])\n        ax5.plot(epochs, history.gradient_penalty, linewidth=2, alpha=0.8, color='orange')\n        ax5.set_xlabel('Epoch')\n        ax5.set_ylabel('Gradient Penalty')\n        ax5.set_title('WGAN Gradient Penalty', fontweight='bold')\n        ax5.grid(True, alpha=0.3)\n    \n    # Row 3: Score gap and loss smoothness\n    ax6 = fig.add_subplot(gs[1, 2])\n    score_gap = np.array(history.d_real_score) - np.array(history.d_fake_score)\n    ax6.plot(epochs, score_gap, linewidth=2, alpha=0.8, color='darkgreen')\n    ax6.axhline(y=0, color='k', linestyle='--', alpha=0.4, linewidth=1.5)\n    ax6.set_xlabel('Epoch')\n    ax6.set_ylabel('D(real) - D(fake)')\n    ax6.set_title('Discriminator Score Gap', fontweight='bold')\n    ax6.grid(True, alpha=0.3)\n    \n    plt.suptitle(f'Training Diagnostics{title_suffix}', fontsize=16, fontweight='bold')\n    plt.show()\n\n\ndef plot_generated_airfoils(generator, latent_dim: int, \n                           title_suffix: str = \"\", n_samples: int = 8):\n    \"\"\"Visualize generated airfoils.\"\"\"\n    generator.eval()\n    \n    # Sample random airfoils from dataset for comparison\n    indices = np.random.choice(len(X_airfoils), n_samples, replace=False)\n    real_designs = X_airfoils[indices]\n    \n    # Generate airfoils\n    with th.no_grad():\n        z = th.randn(n_samples, latent_dim, device=device)\n        gen_designs_flat = generator(z).cpu().numpy()\n    \n    # Denormalize\n    gen_designs_flat = gen_designs_flat * 3 * X_std + X_mean\n    gen_designs = gen_designs_flat.reshape(n_samples, 192, 2)\n    \n    # Plot\n    fig, axes = plt.subplots(2, n_samples, figsize=(20, 6))\n    \n    for i in range(n_samples):\n        # Real\n        axes[0, i].plot(real_designs[i, :, 0], real_designs[i, :, 1], 'b-', linewidth=2)\n        axes[0, i].set_aspect('equal')\n        axes[0, i].set_xlim(-0.1, 1.1)\n        axes[0, i].set_ylim(-0.3, 0.3)\n        axes[0, i].grid(True, alpha=0.3)\n        if i == 0:\n            axes[0, i].set_ylabel('Real', fontsize=12, fontweight='bold')\n        axes[0, i].set_xticks([])\n        axes[0, i].set_yticks([])\n        \n        # Generated\n        axes[1, i].plot(gen_designs[i, :, 0], gen_designs[i, :, 1], 'r-', linewidth=2)\n        axes[1, i].set_aspect('equal')\n        axes[1, i].set_xlim(-0.1, 1.1)\n        axes[1, i].set_ylim(-0.3, 0.3)\n        axes[1, i].grid(True, alpha=0.3)\n        if i == 0:\n            axes[1, i].set_ylabel('Generated', fontsize=12, fontweight='bold')\n        axes[1, i].set_xticks([])\n        axes[1, i].set_yticks([])\n    \n    plt.suptitle(f'Real vs Generated Airfoils{title_suffix}', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\nprint(\"Training utilities defined.\")\n\nTraining utilities defined.\n\n\n—# Part 1: Baseline GANWe start with a vanilla GAN using:- Binary Cross-Entropy (BCE) loss- Standard fully-connected architectures- Basic Adam optimizer- No batch normalization- No dropout- Moderate learning rates (1e-4)Note: The baseline is intentionally simplified to create a clean starting point. This helps you better appreciate the benefits of the improvement techniques you’ll implement later.",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#baseline-architecture",
    "href": "problems/ps2.html#baseline-architecture",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "17.7 Baseline Architecture",
    "text": "17.7 Baseline Architecture\n\nclass BaselineGenerator(nn.Module):    \"\"\"Baseline unconditional generator for 1D airfoil curves.\"\"\"        def __init__(self, latent_dim: int, output_dim: int):        super().__init__()                def block(in_feat: int, out_feat: int):            layers = [nn.Linear(in_feat, out_feat)]            layers.append(nn.LeakyReLU(0.2, inplace=True))            return layers                # Main generation path        self.model = nn.Sequential(            *block(latent_dim, 256),            *block(256, 512),            *block(512, 1024),            nn.Linear(1024, output_dim),            nn.Tanh(),  # Output in [-1, 1]        )        def forward(self, z: th.Tensor) -&gt; th.Tensor:        return self.model(z)class BaselineDiscriminator(nn.Module):    \"\"\"Baseline unconditional discriminator for 1D airfoil curves.\"\"\"        def __init__(self, input_dim: int):        super().__init__()                # Main discrimination path        model_layers = [            nn.Linear(input_dim, 512),            nn.LeakyReLU(0.2, inplace=True),            nn.Linear(512, 512),            nn.LeakyReLU(0.2, inplace=True),            nn.Linear(512, 256),            nn.LeakyReLU(0.2, inplace=True),            nn.Linear(256, 1),            nn.Sigmoid(),  # Probability output        ]                self.model = nn.Sequential(*model_layers)        def forward(self, design: th.Tensor) -&gt; th.Tensor:        return self.model(design)print(\"Baseline models defined.\")",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#baseline-training-function",
    "href": "problems/ps2.html#baseline-training-function",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "17.8 Baseline Training Function",
    "text": "17.8 Baseline Training Function\n\ndef train_baseline_gan(X_data: th.Tensor,                       n_epochs: int = 100, batch_size: int = 32,                       latent_dim: int = 64, lr_gen: float = 5e-4, lr_disc: float = 5e-4,                       seed: int = 42, print_every: int = 20):    \"\"\"    Train baseline unconditional GAN.        Args:        X_data: Normalized design data (N, output_dim)        n_epochs: Number of training epochs        batch_size: Batch size        latent_dim: Latent noise dimension        lr_gen: Generator learning rate        lr_disc: Discriminator learning rate        seed: Random seed        print_every: Print frequency        Returns:        generator, discriminator, history    \"\"\"    # Setup    th.manual_seed(seed)    np.random.seed(seed)        output_dim = X_data.shape[1]        # Create DataLoader    dataset = TensorDataset(X_data)    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)        # Initialize models    generator = BaselineGenerator(latent_dim, output_dim).to(device)    discriminator = BaselineDiscriminator(output_dim).to(device)        # Loss and optimizers    criterion = nn.BCELoss()    opt_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.999))    opt_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.999))        # Training history    history = TrainingHistory([], [], [], [], [], [])        # Training loop    for epoch in range(n_epochs):        d_losses, g_losses = [], []        d_real_scores, d_fake_scores = [], []                for (designs,) in dataloader:            batch_size_actual = designs.size(0)            valid = th.ones(batch_size_actual, device=device)            fake = th.zeros(batch_size_actual, device=device)                        # -----------------            # Train Discriminator            # -----------------            opt_disc.zero_grad()                        # Real samples            real_pred = discriminator(designs).squeeze()            real_loss = criterion(real_pred, valid)                        # Fake samples            z = th.randn(batch_size_actual, latent_dim, device=device)            gen_designs = generator(z)            fake_pred = discriminator(gen_designs.detach()).squeeze()            fake_loss = criterion(fake_pred, fake)                        # Total discriminator loss            d_loss = (real_loss + fake_loss) / 2            d_loss.backward()            opt_disc.step()                        # -----------------            # Train Generator            # -----------------            opt_gen.zero_grad()                        z = th.randn(batch_size_actual, latent_dim, device=device)            gen_designs = generator(z)            gen_pred = discriminator(gen_designs).squeeze()                        g_loss = criterion(gen_pred, valid)  # Want discriminator to predict \"real\"            g_loss.backward()            opt_gen.step()                        # Record metrics            d_losses.append(d_loss.item())            g_losses.append(g_loss.item())            d_real_scores.append(real_pred.mean().item())            d_fake_scores.append(fake_pred.mean().item())                # Compute diversity        with th.no_grad():            z_eval = th.randn(100, latent_dim, device=device)            samples_eval = generator(z_eval)            diversity = compute_diversity(samples_eval)                # Store epoch metrics        history.d_loss.append(np.mean(d_losses))        history.g_loss.append(np.mean(g_losses))        history.d_real_score.append(np.mean(d_real_scores))        history.d_fake_score.append(np.mean(d_fake_scores))        history.diversity.append(diversity)        history.gradient_penalty.append(0.0)  # Not used in baseline                if (epoch + 1) % print_every == 0 or epoch == 0:            print(f\"Epoch {epoch+1:03d}/{n_epochs} | \"                  f\"D loss: {history.d_loss[-1]:.4f} | \"                  f\"G loss: {history.g_loss[-1]:.4f} | \"                  f\"D(real): {history.d_real_score[-1]:.3f} | \"                  f\"D(fake): {history.d_fake_score[-1]:.3f} | \"                  f\"Diversity: {history.diversity[-1]:.2f}\")        return generator, discriminator, historyprint(\"Baseline training function defined.\")",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#exercise-1-train-baseline-and-diagnose-issues",
    "href": "problems/ps2.html#exercise-1-train-baseline-and-diagnose-issues",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "17.9 Exercise 1: Train Baseline and Diagnose Issues",
    "text": "17.9 Exercise 1: Train Baseline and Diagnose Issues\nTask: Train the baseline GAN and analyze the results.\nQuestions to answer: 1. Does the training appear stable? Look at the loss curves. 2. What is happening with D(real) and D(fake) scores? Are they diverging or collapsing? 3. How does sample diversity evolve over training? 4. Do the generated airfoils look realistic? 5. What problems do you observe?\n\n# Train baselineprint(\"=\"*80)print(\"TRAINING BASELINE GAN\")print(\"=\"*80)latent_dim = 64gen_baseline, disc_baseline, hist_baseline = train_baseline_gan(    X_tensor,    n_epochs=150,    batch_size=64,    latent_dim=latent_dim,    lr_gen=1e-4,      lr_disc=1e-4,    seed=42,    print_every=30)# Visualize resultsplot_training_diagnostics(hist_baseline, \" - Baseline\")plot_generated_airfoils(gen_baseline, latent_dim, \" - Baseline\")plot_pca_coverage(gen_baseline, latent_dim, \" - Baseline\")\n\n\n17.9.1 Your Observations:\nWrite your observations here:\n\n17.10 Training stability:\n17.11 Discriminator scores:\n17.12 Sample diversity:\n17.13 Visual quality:\n17.14 Main problems identified:",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#available-techniques",
    "href": "problems/ps2.html#available-techniques",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "18.1 Available Techniques:",
    "text": "18.1 Available Techniques:\n\n18.1.1 1. Wasserstein Loss with Gradient Penalty (WGAN-GP)\nWhat it does: Replaces BCE loss with Wasserstein distance. Adds gradient penalty to enforce Lipschitz constraint.\nPaper: “Improved Training of Wasserstein GANs” (Gulrajani et al., 2017)\nWhen to use: Helps with training stability and vanishing gradients\nConceptual difficulty: Medium (requires understanding Wasserstein distance)\n\n\n18.1.2 2. Spectral Normalization\nWhat it does: Normalizes weight matrices to have spectral norm ≤ 1 (Lipschitz-1 constraint).\nPaper: “Spectral Normalization for GANs” (Miyato et al., 2018)\nWhen to use: Discriminator becomes too powerful, generator gradients vanish\nConceptual difficulty: Easy (just add to layers)\n\n\n18.1.3 3. Minibatch Discrimination / Diversity Penalty\nWhat it does: Encourages generator to produce diverse samples by penalizing similarity.\nPaper: “Improved Techniques for Training GANs” (Salimans et al., 2016)\nWhen to use: Mode collapse (generator produces limited variety)\nConceptual difficulty: Easy (add diversity term to loss)\n\n\n18.1.4 4. Two-Timescale Update Rule (TTUR)\nWhat it does: Uses separate learning rates for G and D, typically lr_D &gt; lr_G.\nPaper: “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium” (Heusel et al., 2017)\nWhen to use: Loss oscillations, unstable training dynamics\nConceptual difficulty: Very easy (just adjust learning rates)\n\n\n18.1.5 5. Label Smoothing\nWhat it does: Use soft labels (e.g., 0.9 instead of 1.0) for real samples to prevent overconfidence.\nPaper: “Improved Techniques for Training GANs” (Salimans et al., 2016)\nWhen to use: Discriminator becomes overconfident, gradients saturate\nConceptual difficulty: Very easy (change target labels)\n\n\n18.1.6 6. Feature Matching\nWhat it does: Train generator to match statistics of intermediate discriminator features, not just final output.\nPaper: “Improved Techniques for Training GANs” (Salimans et al., 2016)\nWhen to use: Training instability, helps generator focus on meaningful features\nConceptual difficulty: Medium (requires extracting intermediate features)",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#advanced-topics-dont-recommend-implementing-for-time-unless-interested",
    "href": "problems/ps2.html#advanced-topics-dont-recommend-implementing-for-time-unless-interested",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "18.2 Advanced Topics (Don’t Recommend Implementing for Time Unless Interested)",
    "text": "18.2 Advanced Topics (Don’t Recommend Implementing for Time Unless Interested)\n\n18.2.1 a. Progressive Growing / Curriculum Learning\nWhat it does: Start training with low-resolution/simple outputs, gradually increase complexity.\nPaper: “Progressive Growing of GANs” (Karras et al., 2018)\nWhen to use: Complex output space, difficulty learning fine details\nConceptual difficulty: Hard (requires architectural changes)\n\n\n18.2.2 b. Self-Attention Mechanism\nWhat it does: Allows network to model long-range dependencies in the data.\nPaper: “Self-Attention GANs” (Zhang et al., 2019)\nWhen to use: Data has long-range structure (like airfoil smoothness)\nConceptual difficulty: Medium (requires attention layer implementation)",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#exercise-2-select-and-hypothesize",
    "href": "problems/ps2.html#exercise-2-select-and-hypothesize",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "19.1 Exercise 2: Select and Hypothesize",
    "text": "19.1 Exercise 2: Select and Hypothesize\nBefore implementing anything, answer these questions:\n\nWhich TWO techniques do you think will help most with the baseline issues you identified?\n\nTechnique 1: _______________\nTechnique 2: _______________\n\nFor each technique, hypothesize what specific improvement you expect:\n\nTechnique 1 hypothesis:\n\nWhat metric will improve? (loss stability / diversity / visual quality / etc.)\nWhy do you think this will help?\n\nTechnique 2 hypothesis:\n\nWhat metric will improve?\nWhy do you think this will help?\n\n\nDo you expect these techniques to work well together or could they conflict?",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#implementation-templates",
    "href": "problems/ps2.html#implementation-templates",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "19.2 Implementation Templates",
    "text": "19.2 Implementation Templates\nBelow are code templates for implementing each technique. Choose 2-3 to implement and test.\n\n19.2.1 Example Template: Wasserstein Loss with Gradient Penalty (WGAN-GP)\n\ndef compute_gradient_penalty(discriminator, real_samples, fake_samples):\n    \"\"\"\n    Compute gradient penalty for WGAN-GP.\n    \n    The gradient penalty enforces the Lipschitz constraint by penalizing\n    gradients that deviate from norm 1.\n    \"\"\"\n    batch_size = real_samples.size(0)\n    \n    # Random interpolation coefficient\n    alpha = th.rand(batch_size, 1, device=real_samples.device)\n    \n    # Interpolate between real and fake samples\n    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n    \n    # Get discriminator output for interpolated samples\n    d_interpolates = discriminator(interpolates)\n    \n    # Compute gradients\n    gradients = th.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=th.ones_like(d_interpolates),\n        create_graph=True,\n        retain_graph=True,\n    )[0]\n    \n    gradients = gradients.view(batch_size, -1)\n    gradient_norm = gradients.norm(2, dim=1)\n    \n    # Penalty: (||gradient|| - 1)^2\n    penalty = ((gradient_norm - 1) ** 2).mean()\n    return penalty\n\n\n# Modified Discriminator for WGAN (no sigmoid!)\nclass WGANDiscriminator(nn.Module):\n    \"\"\"WGAN-GP discriminator (critic) - no sigmoid output!\"\"\"\n    \n    def __init__(self, input_dim: int):\n        super().__init__()\n        \n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            # NO SIGMOID! - WGAN uses raw scores\n        )\n    \n    def forward(self, design: th.Tensor) -&gt; th.Tensor:\n        return self.model(design)\n\n\n# TODO: Implement WGAN-GP training function\n# IMPLEMENTATION HINTS:\n# 1. Start by copying the train_baseline_gan function as a template\n# 2. Replace BaselineDiscriminator with WGANDiscriminator (already defined above)\n# 3. Remove the BCE criterion - WGAN doesn't use it!\n# 4. In the discriminator training loop:\n#    - Add a for loop to train the critic n_critic times per batch\n#    - Wasserstein loss: critic_loss = -real_pred.mean() + fake_pred.mean() + lambda_gp * gp\n#    - Use compute_gradient_penalty(critic, designs, gen_designs) to get gp\n#    - Store gp values in a list for tracking: gp_values.append(gp.item())\n# 5. In generator training (happens once per n_critic discriminator updates):\n#    - Generator loss: gen_loss = -gen_pred.mean()\n# 6. IMPORTANT: For visualization, normalize critic scores to [0,1] range using running min/max\n#    - Before the epoch loop, create: real_score_history = [], fake_score_history = []\n#    - In the batch loop, extend these lists: real_score_history.extend(d_real_scores)\n#    - After each epoch, compute: score_min = min(real_score_history + fake_score_history)\n#    - score_max = max(real_score_history + fake_score_history)\n#    - Normalize for storage: normalized_real = (np.mean(d_real_scores) - score_min) / (score_max - score_min)\n# 7. Use Adam optimizer with betas=(0.5, 0.9) for both G and C (different from baseline's (0.5, 0.999))\n# 8. Store gradient penalty in history: history.gradient_penalty.append(np.mean(gp_values))\n# 9. Typical hyperparameters: lambda_gp=10, n_critic=5, lr_gen=1e-4, lr_disc=1e-4\n\ndef train_wgan_gp(X_data, n_epochs=100, batch_size=32, latent_dim=64,\n                  lr_gen=1e-4, lr_disc=1e-4, lambda_gp=10, n_critic=5,\n                  seed=42, print_every=20):\n    \"\"\"\n    Train WGAN with gradient penalty.\n    \n    Args:\n        lambda_gp: Gradient penalty weight (typically 10)\n        n_critic: Train discriminator n_critic times per generator update\n    \"\"\"\n    # TODO: Implement this!\n    # Start by copying train_baseline_gan and modifying the loss computation\n    pass\n\nprint(\"WGAN-GP template defined.\")\n\nWGAN-GP template defined.\n\n\n\n\n19.2.2 Example Template: Spectral Normalization\n\nclass SpectralNormDiscriminator(nn.Module):    \"\"\"Discriminator with spectral normalization on all linear layers.\"\"\"        def __init__(self, input_dim: int):        super().__init__()                # Apply spectral norm to linear layers        model_layers = [            nn.utils.spectral_norm(nn.Linear(input_dim, 512)),            nn.LeakyReLU(0.2, inplace=True),            nn.utils.spectral_norm(nn.Linear(512, 512)),            nn.LeakyReLU(0.2, inplace=True),            nn.utils.spectral_norm(nn.Linear(512, 256)),            nn.LeakyReLU(0.2, inplace=True),            nn.utils.spectral_norm(nn.Linear(256, 1)),            nn.Sigmoid(),        ]                self.model = nn.Sequential(*model_layers)        def forward(self, design: th.Tensor) -&gt; th.Tensor:        return self.model(design)# TODO: Implement training with spectral norm discriminator# IMPLEMENTATION HINTS:# 1. Copy the train_baseline_gan function# 2. Replace: discriminator = BaselineDiscriminator(output_dim).to(device)#    With: discriminator = SpectralNormDiscriminator(output_dim).to(device)# 3. Everything else stays exactly the same! That's the beauty of spectral normalization.# 4. Use the same BCE loss and training loop as baseline# 5. Recommended hyperparameters: lr_gen=1e-4, lr_disc=1e-4 (can try lr_disc slightly higher like 4e-4)print(\"Spectral normalization template defined.\")\n\n\n\n19.2.3 Example Template: Diversity Penalty\n\ndef diversity_penalty(generated_samples, lambda_div=1.0):\n    \"\"\"\n    Compute diversity penalty to encourage varied outputs.\n    \n    Penalizes the generator if generated samples are too similar.\n    We want HIGH diversity, so we add NEGATIVE diversity to the loss.\n    \n    Args:\n        generated_samples: Batch of generated samples (B, D)\n        lambda_div: Weight for diversity penalty\n    \n    Returns:\n        Penalty term (lower diversity = higher penalty)\n    \"\"\"\n    batch_size = generated_samples.size(0)\n    if batch_size &lt;= 1:\n        return th.tensor(0.0, device=generated_samples.device)\n    \n    # Compute pairwise distances\n    samples_flat = generated_samples.view(batch_size, -1)\n    dists = th.cdist(samples_flat, samples_flat, p=2)\n    \n    # Average pairwise distance (higher = more diverse)\n    avg_dist = dists.sum() / (batch_size * (batch_size - 1))\n    \n    # We want to MAXIMIZE diversity, so MINIMIZE negative diversity\n    penalty = -lambda_div * avg_dist\n    return penalty\n\n# TODO: Modify generator training to include diversity penalty\n# IMPLEMENTATION HINTS:\n# 1. Copy the train_baseline_gan function\n# 2. In the generator training section, modify the loss calculation:\n#    OLD: g_loss = criterion(gen_pred, valid)\n#    NEW: adversarial_loss = criterion(gen_pred, valid)\n#         div_penalty = diversity_penalty(gen_designs, lambda_div)\n#         g_loss = adversarial_loss + div_penalty\n# 3. Add lambda_div as a parameter to your training function (typical value: 0.1)\n# 4. Note: diversity_penalty returns NEGATIVE diversity, so adding it encourages diversity\n# 5. Experiment with lambda_div values: 0.01 (subtle), 0.1 (moderate), 0.5 (strong)\n# 6. Recommended: lr_gen=1e-4, lr_disc=1e-4, lambda_div=0.1\n\nprint(\"Diversity penalty template defined.\")\n\nDiversity penalty template defined.\n\n\n\n\n19.2.4 Example Template: Label Smoothing\n\n# TODO: Implement label smoothing\n# IMPLEMENTATION HINTS:\n# 1. Copy the train_baseline_gan function\n# 2. In the discriminator training section, replace the label creation:\n#    OLD: valid = th.ones(batch_size_actual, device=device)\n#         fake = th.zeros(batch_size_actual, device=device)\n#    NEW: valid = th.ones(batch_size_actual, device=device) * smooth_real  # e.g., 0.9\n#         fake = th.ones(batch_size_actual, device=device) * smooth_fake   # e.g., 0.1\n# 3. In the generator training section, keep using hard labels:\n#    valid_gen = th.ones(batch_size_actual, device=device)  # Still 1.0 for generator\n# 4. Add smooth_real and smooth_fake as parameters (typical: smooth_real=0.9, smooth_fake=0.1)\n# 5. Alternative (one-sided smoothing): smooth_real=0.9, smooth_fake=0.0\n# 6. This prevents the discriminator from becoming overconfident\n# 7. Recommended: lr_gen=1e-4, lr_disc=1e-4, smooth_real=0.9, smooth_fake=0.1\n\nprint(\"Label smoothing hint provided.\")\n\nLabel smoothing hint provided.\n\n\n\n\n19.2.5 Example Template: Feature Matching\n\nclass FeatureMatchingDiscriminator(nn.Module):    \"\"\"Discriminator that exposes intermediate features for feature matching.\"\"\"        def __init__(self, input_dim: int):        super().__init__()                # Separate intermediate layers for feature extraction        self.hidden1 = nn.Sequential(            nn.Linear(input_dim, 512),            nn.LeakyReLU(0.2, inplace=True),            nn.Linear(512, 512),            nn.LeakyReLU(0.2, inplace=True),        )                self.hidden2 = nn.Sequential(            nn.Linear(512, 256),            nn.LeakyReLU(0.2, inplace=True),        )                self.output = nn.Sequential(            nn.Linear(256, 1),            nn.Sigmoid(),        )        def forward(self, design: th.Tensor, return_features=False):        # Extract intermediate features        h1 = self.hidden1(design)        h2 = self.hidden2(h1)        out = self.output(h2)                if return_features:            return out, h2  # Return final layer features        return out# TODO: Implement feature matching loss# IMPLEMENTATION HINTS:# 1. Copy the train_baseline_gan function# 2. Replace: discriminator = BaselineDiscriminator(output_dim).to(device)#    With: discriminator = FeatureMatchingDiscriminator(output_dim).to(device)# 3. In the discriminator training section (no changes needed here):#    - Keep using: real_pred = discriminator(designs).squeeze()#    - Keep using: fake_pred = discriminator(gen_designs.detach()).squeeze()# 4. In the generator training section, modify to use feature matching:#    # Get features from both real and fake samples#    gen_pred, fake_features = discriminator(gen_designs, return_features=True)#    with th.no_grad():  # Don't backprop through real features#        _, real_features = discriminator(designs, return_features=True)#    #    # Feature matching loss: match mean statistics#    fm_loss = th.mean((real_features.mean(0) - fake_features.mean(0)) ** 2)#    #    # Total generator loss#    adversarial_loss = criterion(gen_pred.squeeze(), valid)#    g_loss = adversarial_loss + lambda_fm * fm_loss# 5. Add lambda_fm as a parameter (typical value: 1.0 to 10.0)# 6. Recommended: lr_gen=1e-4, lr_disc=1e-4, lambda_fm=10.0print(\"Feature matching template defined.\")",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#exercise-3-implement-and-test-your-chosen-techniques",
    "href": "problems/ps2.html#exercise-3-implement-and-test-your-chosen-techniques",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "19.3 Exercise 3: Implement and Test Your Chosen Techniques",
    "text": "19.3 Exercise 3: Implement and Test Your Chosen Techniques\nInstructions: 1. Choose 2-3 techniques from the word bank above 2. Complete the TODO sections in the templates 3. Train models with your improvements 4. Compare results to baseline\nUse the cells below to implement your experiments.\n\n# YOUR IMPLEMENTATION HERE - Technique 1\n# Copy relevant template and complete the TODOs\n\n\n# Train and evaluate Technique 1\n\n\n# YOUR IMPLEMENTATION HERE - Technique 2\n\n\n# Train and evaluate Technique 2\n\n\n# (Optional) Combine techniques",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#exercise-4-compare-results",
    "href": "problems/ps2.html#exercise-4-compare-results",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "20.1 Exercise 4: Compare Results",
    "text": "20.1 Exercise 4: Compare Results\nCreate comparison plots and fill in the analysis table below.\n\n# Example: Plot loss curves for all experiments\n# TODO: Create comparison plots\n\n# fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n# \n# # Compare losses\n# axes[0].plot(hist_baseline.d_loss, label='Baseline', alpha=0.7)\n# axes[0].plot(hist_technique1.d_loss, label='Technique 1', alpha=0.7)\n# axes[0].plot(hist_technique2.d_loss, label='Technique 2', alpha=0.7)\n# axes[0].set_title('Discriminator Loss Comparison')\n# axes[0].legend()\n# \n# # Compare diversity\n# axes[1].plot(hist_baseline.diversity, label='Baseline', alpha=0.7)\n# axes[1].plot(hist_technique1.diversity, label='Technique 1', alpha=0.7)\n# axes[1].plot(hist_technique2.diversity, label='Technique 2', alpha=0.7)\n# axes[1].set_title('Diversity Comparison')\n# axes[1].legend()\n# \n# # Compare score gap\n# # ...\n\n\n20.1.1 Results Table\nFill in this table with your results:\n\n\n\nMetric\nBaseline\nTechnique 1\nTechnique 2\nCombined\n\n\n\n\nFinal D Loss\n\n\n\n\n\n\nFinal G Loss\n\n\n\n\n\n\nFinal Diversity\n\n\n\n\n\n\nTraining Stability (1-5)\n\n\n\n\n\n\nVisual Quality (1-5)\n\n\n\n\n\n\n\n\n\n20.1.2 Analysis Questions\n\n20.2 Were your hypotheses correct? Did the techniques improve the metrics you expected?\n20.3 Which technique was most effective? Why do you think this is?\n20.4 Did any techniques make things worse? What do you think went wrong?\n20.5 How did the techniques interact when combined? Were they complementary or did they conflict?\n20.6 What other techniques from the word bank might help with remaining issues?",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#exercise-5-critical-thinking",
    "href": "problems/ps2.html#exercise-5-critical-thinking",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "21.1 Exercise 5: Critical Thinking",
    "text": "21.1 Exercise 5: Critical Thinking\nAnswer the following questions:\n\nComputational Cost: Which techniques added significant computational overhead? Was the improvement worth the cost?\nHyperparameter Sensitivity: Which techniques required careful hyperparameter tuning? Which were more robust?\nTheoretical Understanding: For each technique you implemented, explain in 2-3 sentences WHY it helps with GAN training from a theoretical perspective.\nEngineering Constraints: If you were deploying this GAN in production, which technique would you choose and why?\nFuture Improvements: What other techniques (from the word bank or elsewhere) would you like to try? Why?",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "problems/ps2.html#summary",
    "href": "problems/ps2.html#summary",
    "title": "Problem Set: Improving GANs for Airfoil Generation",
    "section": "21.2 Summary",
    "text": "21.2 Summary\nIn this problem set, you: - Trained a baseline GAN and diagnosed its issues - Learned about modern GAN training techniques from the literature - Formed hypotheses about which techniques would help - Implemented and tested your chosen techniques - Analyzed results and understood trade-offs\nKey Takeaway: GAN training is challenging, but systematic application of well-understood techniques can significantly improve stability and output quality. Different problems may benefit from different techniques - there’s no one-size-fits-all solution!\n\n\n21.2.1 References\n\nGoodfellow et al. (2014) - “Generative Adversarial Networks”\nMirza & Osindero (2014) - “Conditional Generative Adversarial Nets”\nSalimans et al. (2016) - “Improved Techniques for Training GANs”\nGulrajani et al. (2017) - “Improved Training of Wasserstein GANs”\nHeusel et al. (2017) - “GANs Trained by a Two Time-Scale Update Rule”\nMiyato et al. (2018) - “Spectral Normalization for GANs”\nKarras et al. (2018) - “Progressive Growing of GANs”\nZhang et al. (2019) - “Self-Attention GANs”",
    "crumbs": [
      "Problems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Problem Set 2</span>"
    ]
  },
  {
    "objectID": "notebooks/notebooks.html",
    "href": "notebooks/notebooks.html",
    "title": "In-Class Notebooks",
    "section": "",
    "text": "This is a list of some of the in-class notebooks I have created for my course, elements of which are already part of the book. I am listing the individual notebooks here to have a direct link to download individual notebooks, since we often use these in class and it is helpful to have them in a centralized place with download links:\n\nReviewing Data Visualization: California Housing Dataset\nIntroduction to Linear Regression and Cross-Validation\nIntroduction to Gradient Descent\nLoss Functions for Linear Models",
    "crumbs": [
      "In-Class Notebooks"
    ]
  },
  {
    "objectID": "notebooks/california_housing_visualization.html",
    "href": "notebooks/california_housing_visualization.html",
    "title": "17  Housing Price Data Visualization In-Class Exercise",
    "section": "",
    "text": "17.1 Exercise 1: Visualize the relationship between features and price\nIn this review notebook, we will review the basics of data visualization and the importance of this for being able to build performant ML models. It is also a good starting point for people to get used to Python and the use of Jupyter Notebooks. It is designed as an in-class (or on your own) exercise to get your feet wet in working with data. Specifically, in this notebook we will:\nThe below code is a useful starting point that should get you started with exploring the California Housing data.\nSome people might find it useful to use a Pandas dataframe to manipulate the data, but that is not really required for basic plotting and visualization:\nFor the below tasks, you might want to try out some basic Python plotting libraries. I recommend:\nIf you aren’t familiar with any of the above libraries, I would suggest starting with Seaborn, since it hides many of the complex features you might not need right away (check out their tutorial). Bokeh also has a nice Quick Start guide if you like having the ability to pan/zoom the data.\nVisualize the 2D relationship between housing prices and the provided features of the data. You can choose how you want to do this.\n[Enter your code into the empty cell below to create the necessary visualizations. You can create multiple cells for code or Markdown code if that helps you.]\nfeatures = ['MedInc','HouseAge','AveRooms','AveBedrms','Population','AveOccup']\nfor i,feature in enumerate(features):\n    plt.figure()\n    sns.jointplot(x=df[feature],y=df['price'],alpha=0.1)\n    plt.show()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\nQuestion: Do any of the features appear linearly correlated with price?",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Housing Price Data Visualization In-Class Exercise</span>"
    ]
  },
  {
    "objectID": "notebooks/california_housing_visualization.html#exercise-2-visualize-the-relationships-between-features",
    "href": "notebooks/california_housing_visualization.html#exercise-2-visualize-the-relationships-between-features",
    "title": "17  Housing Price Data Visualization In-Class Exercise",
    "section": "17.2 Exercise 2: Visualize the relationships between features",
    "text": "17.2 Exercise 2: Visualize the relationships between features\nVisualize the 1D and 2D relationships between the features in the dataset. For example, how are house ages distributed? What is the relationship between house age and the number of bedrooms? Feel free to explore different 1D and 2D options.\n\n\nfor feature in features:\n    plt.figure()\n    sns.displot(df[feature])\n    plt.show()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# Plot the pairwise distributions between each of the features:\nfor i,feature in enumerate(features):\n    for j,feature2 in enumerate(features):\n        if j&gt;i:\n            plt.figure()\n            sns.jointplot(x=df[feature],y=df[feature2],alpha=1.0)\n            plt.show()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nQuestion: Are there any anomalies that look strange in the data, and which visualization helped you identify them (hint: there should be several)?",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Housing Price Data Visualization In-Class Exercise</span>"
    ]
  },
  {
    "objectID": "notebooks/california_housing_visualization.html#exercise-3-visualize-relationships-with-the-anomalies-removed",
    "href": "notebooks/california_housing_visualization.html#exercise-3-visualize-relationships-with-the-anomalies-removed",
    "title": "17  Housing Price Data Visualization In-Class Exercise",
    "section": "17.3 Exercise 3: Visualize relationships with the Anomalies Removed",
    "text": "17.3 Exercise 3: Visualize relationships with the Anomalies Removed\nUsing your knowledge of the anomalies you found above, remove those anomalies using appropriate code below (either by removing the entire data record, or just the specific values that were anomalous, if you prefer to be more surgical) and replot the Task 1 and Task 2 plots you produced above.\n\n# Code goes here for dealing with outliers\n# You can copy/paste some of your plotting code from above, if that is helpful.\n\nQuestion: Does this change your answer to the original Task 1 or Task 2 questions?",
    "crumbs": [
      "In-Class Notebooks",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Housing Price Data Visualization In-Class Exercise</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html",
    "href": "appendices/helpful_tooling.html",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "",
    "text": "A.1 TL;DR Checklist\nMachine learning projects are notoriously brittle: minor implementation details can cause major differences in outcomes. Good practices and tools make your implementation reproducible (and thus debuggable), and portable across different hardware environments, such as a teammate’s laptop or a high-performance computing (HPC) system.\nUnlike traditional programming, debugging ML models by simply “running and fixing in a loop” is rarely effective. Instead, a structured set of practices and tools is needed to understand model behavior and reproduce results reliably.\nHere is what we encourage doing, sorted by impact over effort.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#tldr-checklist",
    "href": "appendices/helpful_tooling.html#tldr-checklist",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "",
    "text": "Environment & Dependencies: Use a proper development environment, virtual environments and pin package versions. Document GPU/CUDA.\n\nLinting: Use tools like ruff to help you write clean code from the start.\nRandom Seeds: Seed all libraries (torch, numpy, random) and enable deterministic operations.\n\nLogging & Checkpoints: Log hyperparameters, save models, track experiments (e.g., wandb).\n\nVisualization: Plot data and learning curves.\nStart Small Then Scale: Debug on small datasets first.\n\nVersion Control: Track code with Git; use branches for experiments.\n\nModular Code: Split code into functions/classes and separate files.\n\nTesting & Type Hints: Write pytest tests and use mypy for type checking.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#reproducible-runs",
    "href": "appendices/helpful_tooling.html#reproducible-runs",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.2 Reproducible Runs",
    "text": "A.2 Reproducible Runs\nThe first requirement for a robust ML project is to make the codebase deterministic. Because ML is inherently probabilistic, running the same code twice without precautions may yield different results. Determinism is therefore a prerequisite for debugging.\n\nA.2.1 Python Project Setup\nA good practice is to use a virtual environment (e.g., venv) to isolate dependencies. This prevents conflicts between projects (e.g., my linear regression project uses sklearn 1.0.2, and my generative design project uses sklearn 2.5.3) and avoids interfering with the base operating system.\nDependencies should be pinned to exact versions in a file such as pyproject.toml or requirements.txt. This enables others (including future you) to reproduce the same environment.\nNote that not all dependencies are automatically captured in Python configuration files—for instance, the CUDA version used for GPU processing must be documented separately (typically in a README.md).\n\n\nA.2.2 Seeded Runs\nMost ML techniques involve randomness (e.g., parameter initialization, sampling, data shuffling). To ensure reproducibility, it is necessary to set a random seed so that random number generators produce a deterministic sequence.\nIn practice, several libraries must be seeded and it will look similar to:\nimport torch as th\nimport numpy as np\nimport random\n\nmy_seed = 42\n\nth.manual_seed(my_seed)  # PyTorch\nth.backends.cudnn.deterministic = True\ntorch.cuda.benchmark = False\nrng = np.random.default_rng(my_seed)  # NumPy\nrandom.seed(my_seed)  # Python's built-in random\n\n\nA.2.3 Hyperparameters\nHyperparameter values can influence results as strongly as changing the algorithm itself. It is essential to record which hyperparameters were used for each experiment.\nExperiment-tracking platforms automate this process. For example, Weights and Biases (wandb), or Trackio can log hyperparameters, Python version, and hardware details, as well as visualize results such as learning curves. See, for example:\n\nRun overview with hyperparameter “Config.”\nLearning curves and sampled designs\n\n\n\n\n\n\n\nTipCheckpoint\n\n\n\nOnce versions, seeds and hyperparameters are fixed, running the model multiple times should yield identical results across runs (look at the wandb curves). Inconsistent results usually indicate a missing seed or an unpinned dependency. Without determinism, debugging will be much more time-consuming. We strongly advise to pay attention to this.\n\n\n\n\n\n\n\n\nNoteOther Sources of Non-Determinism\n\n\n\n\n\nThis is unlikely to happen within the course but it is still worth mentioning.\nEven if you set seeds and pin library versions, some sources of non-determinism may persist due to the environment:\n\nMultithreading or parallelism: Operations may be executed in different orders on CPU threads.\n\nGPU operations: Certain GPU kernels are non-deterministic by design, even with fixed seeds.\n\nLibrary versions or BLAS/CUDA backends: Different versions of underlying math libraries may produce slightly different results.\n\nTo mitigate these issues:\n\nEnable deterministic operations where possible (e.g., torch.backends.cudnn.deterministic = True for PyTorch).\n\nBe aware that some operations may never be fully deterministic on GPU—document this for reproducibility.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#code-management",
    "href": "appendices/helpful_tooling.html#code-management",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.3 Code Management",
    "text": "A.3 Code Management\nML projects should be approached as software engineering projects. Code quality and management are especially critical: poorly organized or fragile code increases the likelihood of errors and makes debugging more difficult. In addition, Python’s permissiveness can hide subtle mistakes. For example, automatic broadcasting of scalars to vectors may not raise an exception when performing operations on vectors or matrices of mismatched sizes, yet it can still produce incorrect results. Such silent errors are often harder to detect than explicit crashes.\n\nA.3.1 Code Organization\nNotebooks are valuable for exploration and prototyping, but they are less suited for building robust and reproducible experiments. Relying on a single notebook or script often leads to unmanageable code as the project grows. By contrast, a modular codebase is easier to test, extend, and maintain. Organizing code into smaller, modular components simplifies both debugging and collaboration.\n\nDivide the project into functions, each with a single, well-defined purpose. A useful rule of thumb is: if you cannot clearly explain what a function does in one sentence, it should probably be split.\n\nUse classes when it is natural to group related data and behavior together.\n\nSplit large projects across multiple files to make navigation easier. Avoid single files with 1000+ lines, as they are hard to read, debug, and extend.\n\n\n\nA.3.2 Version Control\nVersion control ensures that specific states of a project can be identified and restored. We strongly recommend using Git (with GitHub or similar platforms) for ML projects. When working in teams, branches help manage changes and prevent conflicts.\n\n\nA.3.3 Formatting and Linting\nCode formatting conventions (e.g., number of spaces per indentation, placement of comments, naming conventions) do not affect program behavior but improve readability. There are formatters that can automatically fix the visual style of the code—things like indentation, line breaks, spacing around operators, and alignment.\nLinting, goes beyond formatting: linters analyze your code for potential errors or risky patterns, such as unused variables, variables that may be undefined, or suspicious comparisons.\nruff integrates formatting, linting, and error detection in a single tool. It improves code quality, reduces stylistic disagreements, and allows developers to focus on the intent rather than the syntax.\n\n\nA.3.4 Type Hints\nIn addition to formatting and linting, static type checking helps catch errors before running your code. Python is dynamically typed, which means you can easily pass the wrong type of object to a function without immediate errors. Tools like mypy analyze your code using type hints and report mismatches.\nFor example, in:\ndef add_numbers(a: int, b: int) -&gt; int:\n    return a + b\n\nadd_numbers(2, \"3\") # note the String here\nRunning mypy will output:\nmain.py:4: error: Argument 2 to \"add_numbers\" has incompatible type \"str\"; expected \"int\"  [arg-type]\nFound 1 error in 1 file (checked 1 source file)\nUsing type hints (a: int, -&gt; int) together with mypy lets you detect bugs early, improves code readability, and helps IDEs provide better autocompletion and refactoring support.\n\n\nA.3.5 Testing\nTesting individual components—functions, classes, or modules—is an important way to ensure reliability. Well-written tests allow developers to:\n\nIsolate potential error sources: When a bug occurs, thoroughly tested components can be excluded from investigation, saving time.\n\nDetect unintended side effects: Tests help ensure that changes in one part of the codebase do not break other parts.\n\nTesting and good code organization go hand in hand: modular code is naturally easier to test, and writing tests often encourages cleaner, more maintainable designs.\nThe most common tool for this is pytest. For example:\nIf you define a function in your project:\n# my_project/utils.py\ndef add_numbers(a, b):\n    return a + b\nYou can define tests with:\n# tests/test_math.py -- this is your test file\nfrom my_project.utils import add_numbers\n\ndef test_add_numbers():\n    assert add_numbers(2, 3) == 5\n    assert add_numbers(-1, 1) == 0\nRunning pytest will automatically discover these tests and report any failures.\n\n\nA.3.6 Integrated Development Environments (IDEs)\nWhile you can write Python code in any text editor, using an IDE significantly improves productivity. Visual Studio Code (VS Code) is the most popular choice for Python and ML development. It supports:\n\nExtensions: Add functionality and friendly interface for linting ruff, type checking mypy, testing pytest, and git integration.\n\nHandling of virtual environments: VSCode can create and handle virtual environments for you.\nDebugger: Set breakpoints and inspect the current state of variables, run instruction by instruction. This is much easier than putting prints everywhere.\nNotebooks inside VS Code: You can run Jupyter notebooks directly within your IDE.\nLLMs integration: Students have access to GitHub education (and Copilot), VSCode has a direct LLM integration for code completion and agent.\n\n\n\nA.3.7 Large Language Models (LLMs) for Coding\nTools like ChatGPT or GitHub Copilot can generate code quickly. While this can accelerate boilerplate writing, it does not replace understanding.\nMachine learning code is particularly sensitive to details: a small mistake in data preprocessing, tensor dimensions, or random seeding can completely change results. Using LLMs without knowing what the code does may:\n\nHide important assumptions.\n\nLead to silent bugs that are hard to detect.\n\nPrevent you from learning how ML algorithms really work.\n\nGuideline: LLMs are great for generating snippets (e.g., “write a function to convert my CSV data to JSON”), but always read, run, and understand the code before using it in experiments. For ML, correctness and reproducibility are more important than speed.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#debugging",
    "href": "appendices/helpful_tooling.html#debugging",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.4 Debugging",
    "text": "A.4 Debugging\nIf the codebase is well-structured and reproducible but issues persist, the problem is likely related to the maths, hyperparameters, or data.\n\nA.4.1 Visualizing\nVisualization is one of the most effective debugging tools in ML, particularly in engineering contexts where results can often be represented graphically.\n\nA.4.1.1 Algorithm-level Visualizations\n\nLoss curves: Simple plots can reveal overfitting, underfitting, or learning failures.\nPredictions: Comparing model outputs with reference data at various training stages provides direct insight into progress.\n\nFor these, we often report metrics and outputs in wandb, see for instance these lines.\n\n\nA.4.1.2 Data-level Visualizations\n\nInspect dataset distributions: Check whether features are on compatible scales, whether rescaling or normalization is needed, and whether outliers are present. Tools like matplotlib or seaborn can help.\n\nAssess assumptions: Determine whether the data distribution aligns with the model’s underlying assumptions, e.g., can the data distribution be captured by a Gaussian distribution.\n\n\n\n\nA.4.2 Split Your Pipeline\nIt is good practice to split your training pipeline into distinct stages:\n\nData analysis: Visualize your data. Look at the distributions, detect outliers, and gain insights into what preprocessing might be needed and which models may perform well.\nData pre-processing: Massage your data before feeding it to the model. Visualize to ensure transformations are correct and consistent.\nTraining: Train your model on the preprocessed data. Save trained models to disk after each run (torch.save, pickle, or similar). This allows you to avoid retraining from scratch every time you tweak evaluation code.\n\nEvaluation: Load the saved model and run your evaluation routines on validation or test datasets.\n\nBy separating these stages, you can debug each part independently, and validate progress.\n\n\nA.4.3 Start Small, Then Scale\nWhen debugging, it is inefficient to run large-scale experiments immediately. Instead:\n\nBegin with small, fast experiments (e.g., a reduced dataset or a lightweight simulator).\n\nValidate that the model can learn on trivial cases.\n\nAttempt to reproduce established results or baseline performance.\n\nScaling to larger, more complex runs should only occur once smaller experiments confirm that the model behaves as expected.\n\n\nA.4.4 Performance Profiling with timeit\nSometimes the bug is actually that the code is too slow. When this happens, the first step is often to measure where the time goes. You can do that with Python’s built-in timeit.\ntimeit runs a snippet of code multiple times and reports the average execution time, helping you compare different implementations or detect bottlenecks.\nHere is an example for normalizing data:\nimport numpy as np\nimport timeit\n\nsetup = \"\"\"\nimport numpy as np\ndata = np.random.rand(10000, 100)  # 10k samples, 100 features\n\"\"\"\n\n# Option 1: Pure Python loops\nstmt1 = \"\"\"\nnormalized = []\nfor row in data:\n    mean = np.mean(row)\n    std = np.std(row)\n    normalized.append((row - mean) / std)\nnormalized = np.array(normalized)\n\"\"\"\n\n# Option 2: NumPy vectorization\nstmt2 = \"\"\"\nmeans = np.mean(data, axis=1, keepdims=True)\nstds = np.std(data, axis=1, keepdims=True)\nnormalized = (data - means) / stds\n\"\"\"\n\nprint(\"Python loops:\", timeit.timeit(stmt1, setup=setup, number=10))\nprint(\"NumPy vectorization:\", timeit.timeit(stmt2, setup=setup, number=10))\nResults:\nPython loops: 0.8857301659882069\nNumPy vectorization: 0.04489224997814745\nUsing NumPy vectorization is ~20x faster than Python loops.\nIn notebooks, you don’t even need imports:\n%timeit sum(range(1000))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/helpful_tooling.html#a-practical-example",
    "href": "appendices/helpful_tooling.html#a-practical-example",
    "title": "Appendix A — Helpful Tooling for Working with and Debugging Machine Learning Models",
    "section": "A.5 A Practical Example",
    "text": "A.5 A Practical Example\nThis section shows a practical example using the techniques explained above on an actual code.\n\nA.5.1 Step 0: The Ugly Script\nWe start with some messy code that Ruff would flag:\n# train.py\nimport numpy as np, torch, torch.nn as nn, torch.optim as optim, matplotlib.pyplot as plt, random\nfrom sklearn.model_selection import train_test_split\n\nX_all =np.linspace(0, 100, 500).reshape(-1,1)\ny_all = 5* np.sin(0.1 * X_all)+np.random.randn(500,1)\nX_train,X_test,y_train,y_test =train_test_split(X_all,y_all,test_size=0.2,random_state=42)\n\nmodel =nn.Linear(1,1)\noptimizer =optim.SGD(model.parameters(),lr=0.01)\nloss_fn= nn.MSELoss()\n\nfor epoch in range(500):\n    pred =model(torch.tensor(X_train))\n    loss= loss_fn(pred, torch.tensor(y_train))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f\"Epoch {epoch} - Loss: {loss.item()}\")\nAt first glance, it looks okay but it won’t run. Try executing python train.py to see the errors.\n\n\nA.5.2 Step 1: From Ugly to Bad\nRun ruff to format and check. Fix the errors (or call ruff check --fix train.py).\nNow the code is already cleaner and easier to debug. Still, running the file throws errors.\n\n\nA.5.3 Step 2: Debugging\nRunning python train.py gives a cryptic type error at the loss computation: RuntimeError: mat1 and mat2 must have the same dtype, but got Double and Float.\nThe problem is that the model outputs float32 predictions while y is float64. This causes a mismatch in the loss computation. The fix is to change the lines to:\n- pred = model(torch.tensor(X_train))\n- loss = loss_fn(pred, torch.tensor(y_train))\n+ pred = model(torch.tensor(X_train, dtype=torch.float32))\n+ loss = loss_fn(pred, torch.tensor(y_train, dtype=torch.float32))\n\n\nA.5.4 Step 3: Make your Script as Deterministic as possible\nIt is important to remove sources of non determinism when debugging ML models, see seeding.\n# right after the imports\nrng = np.random.default_rng(42)  # seed NumPy random\ntorch.manual_seed(42)  # seed PyTorch\ntorch.cuda.manual_seed(42)  # see PyTorch CUDA (for NVIDIA GPUs)\ntorch.backends.cudnn.deterministic = True  # tell PyTorch to use deterministic kernels\ntorch.backends.cudnn.benchmark = False # removes internal optimizations that can cause non-determinism \nAnd when defining your outputs:\n# replace np.random by the seeded RNG\n- y = 5 * np.sin(0.1 * X_all) + np.random.randn(100, 1)\n+ y = 5 * np.sin(0.1 * X_all) + rng.standard_normal(size=(100, 1))\n\n\nA.5.5 Step 4: Visualizing\nIt is extremely important to visualize your data. For this, we can add this to the script:\n\nA.5.5.1 Visualizing data\nimport matplotlib.pyplot as plt\nand before the training loop:\nplt.scatter(X_train, y_train, label=\"Train\")\nplt.scatter(X_test, y_test, color=\"red\", label=\"Test\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"Data\")\nplt.legend()\nplt.show()\n\n\nA.5.5.2 Visualizing loss\n# before training loop\nlosses = []\n\n# in your training loop\nlosses.append(loss.item())\n\n# after training loop\nplt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over time\")\nplt.show()\nObservations:\n\nThe target is sinusoidal, a simple linear model cannot capture this (we’ve made bad assumptions for the model).\nThe loss is producing nans and going to inf.\nThe features are not normalized, making learning difficult.\n\n\n\n\nA.5.6 Step 5: Normalizing Features\nX_mean = X_train.mean(axis=0, keepdims=True)\nX_std = X_train.std(axis=0, keepdims=True)\ny_mean = y_train.mean(axis=0, keepdims=True)\ny_std = y_train.std(axis=0, keepdims=True)\nX_train_norm = (X_train - X_mean) / X_std\ny_train_norm = (y_train - y_mean) / y_std\nX_test_norm = (X_test - X_mean) / X_std\ny_test_norm = (y_test - y_mean) / y_std\nor using scikit-learn utils:\nx_scaler = StandardScaler()\ny_scaler = StandardScaler()\nX_train_norm = x_scaler.fit_transform(X_train)\ny_train_norm = y_scaler.fit_transform(y_train)\nX_test_norm = x_scaler.transform(X_test)\ny_test_norm = y_scaler.transform(y_test)\nUse these normalized tensors in the training loop instead of the raw values.\n\n\nA.5.7 Step 6: Visualizing Predictions\nNow the loss seems to go down, the code runs. Let’s look at the predictions. This code will help you visualize the predictions vs. the true values:\n# after the training loop\nwith torch.no_grad():\n    predictions = model(X_tensor)\n\nplt.figure(figsize=(8, 5))\nplt.scatter(X_tensor.numpy(), y_tensor.numpy(), label=\"True data\", color=\"blue\", alpha=0.5)\nplt.scatter(\n    X_tensor.numpy(), predictions.numpy(), label=\"Predictions\", color=\"red\", alpha=0.5\n)\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"True vs Predicted\")\nplt.legend()\nplt.show()\nObservation: It is pretty obvious that our model has not enough capacity to capture the data.\n\n\nA.5.8 Step 7: Adjusting Hyperparameters\nLet’s try to increase the model size.\nmodel = nn.Sequential(\n    nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 16), nn.ReLU(), nn.Linear(16, 1)\n)\nAnd re-run. Now we see the loss is not optimal.\n\nCan you adjust the learning rate and model size?\nWhat about the optimizer?\nAnd how are you going to keep track of what combination of hyperparameter values you have tried?\nAlso, you have several plots (prediction, loss) for each run which are helpful.\n\nFor this, we recommend using experiment trackers, such as Weights and Biases or Trackio.\nFirst, you start by defining your hyperparameters on top the file:\nhyperparameters = {\n    \"learning_rate\": 0.01,\n    \"model_layers\": [16, 16],\n    \"activation\": \"ReLU\",\n}\nand use them in your training script. For instance, your model definition becomes\nmodel_layers: list[int] = hyperparameters[\"model_layers\"]\nlayers = []\n\n# Build all layers including input and hidden layers -- this allows to just change the model_layers in your hyperparameters dictionary.\ncurrent_size = 1\nfor layer_size in model_layers:\n    layers.append(nn.Linear(current_size, layer_size))\n    if hyperparameters[\"activation\"] == \"ReLU\":\n        layers.append(nn.ReLU())\n    elif hyperparameters[\"activation\"] == \"Sigmoid\":\n        layers.append(nn.Sigmoid())\n    current_size = layer_size\n\n# Add output layer\nlayers.append(nn.Linear(current_size, 1))\n\nmodel = nn.Sequential(*layers)\noptimizer = optim.SGD(model.parameters(), lr=hyperparameters[\"learning_rate\"]) # see hyperparameter here\nThen, you log these hyperparameters for each experiment:\n wandb.init(project=\"example\", config=hyperparameters)\nIn your training loop:\nwandb.log({\"loss\": loss.item()})\nYou can even log an image of prediction vs. true data at each training step. See below.\n\n\nA.5.9 Final Code\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport trackio as wandb\nfrom sklearn.model_selection import train_test_split\n\nrng = np.random.default_rng(42)  # seed NumPy random\ntorch.manual_seed(42)  # seed PyTorch\ntorch.cuda.manual_seed(42)  # see PyTorch CUDA (for NVIDIA GPUs)\ntorch.backends.cudnn.deterministic = True  # tell PyTorch to use deterministic kernels\ntorch.backends.cudnn.benchmark = (\n    False  # removes internal optimizations that can cause non-determinism\n)\n\nhyperparameters = {\n    \"learning_rate\": 0.01,\n    \"model_layers\": [16, 16],\n    \"activation\": \"ReLU\",\n}\n\n\ndef visualize_data(\n    model: nn.Module,\n    X_test: np.ndarray,\n    y_test: np.ndarray,\n    epoch: int,\n    open_window: bool = False,\n    log: bool = False,\n):\n    \"\"\"Visualize the data and the predictions.\n\n    Args:\n        model: The model to visualize the predictions of.\n        X_test: The test data.\n        y_test: The test labels.\n        epoch: The epoch number.\n        open_window: Whether to open a window to display the plot.\n        log: Whether to log the plot to wandb.\n    \"\"\"\n    with torch.no_grad():\n        predictions = model(torch.tensor(X_test, dtype=torch.float32))\n    plt.scatter(X_test, y_test, color=\"red\", alpha=0.5, label=\"Data\")\n    plt.scatter(\n        X_test, predictions.numpy(), color=\"blue\", alpha=0.5, label=\"Predictions\"\n    )\n    plt.xlabel(\"X\")\n    plt.ylabel(\"y\")\n    plt.title(f\"Data - Epoch {epoch}\")\n    plt.legend()\n    if log:\n        plt.savefig(\"predictions.png\")\n        wandb.log({\"predictions\": wandb.Image(\"predictions.png\")})\n    if open_window:\n        plt.show()\n    plt.close()\n\n\nif __name__ == \"__main__\":\n    wandb.init(project=\"example\", config=hyperparameters)\n\n    X_all = np.linspace(0, 100, 500).reshape(-1, 1)\n    y_all = 5 * np.sin(0.1 * X_all) + rng.standard_normal(size=(500, 1))\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_all, y_all, test_size=0.2, random_state=42\n    )\n\n    plt.scatter(X_train, y_train, label=\"Train\")\n    plt.scatter(X_test, y_test, color=\"red\", label=\"Test\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"y\")\n    plt.title(\"Data\")\n    plt.legend()\n    plt.show()\n\n    # Normalize the data\n    X_mean = X_train.mean(axis=0, keepdims=True)\n    X_std = X_train.std(axis=0, keepdims=True)\n    y_mean = y_train.mean(axis=0, keepdims=True)\n    y_std = y_train.std(axis=0, keepdims=True)\n    X_train_norm = (X_train - X_mean) / X_std\n    y_train_norm = (y_train - y_mean) / y_std\n    X_test_norm = (X_test - X_mean) / X_std\n    y_test_norm = (y_test - y_mean) / y_std\n\n    model_layers: list[int] = hyperparameters[\"model_layers\"]\n    layers = []\n\n    # Build all layers including input and hidden layers\n    current_size = 1\n    for layer_size in model_layers:\n        layers.append(nn.Linear(current_size, layer_size))\n        if hyperparameters[\"activation\"] == \"ReLU\":\n            layers.append(nn.ReLU())\n        elif hyperparameters[\"activation\"] == \"Sigmoid\":\n            layers.append(nn.Sigmoid())\n        current_size = layer_size\n\n    # Add output layer\n    layers.append(nn.Linear(current_size, 1))\n\n    model = nn.Sequential(*layers)\n    optimizer = optim.Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n    loss_fn = nn.MSELoss()\n\n    losses = []\n    for epoch in range(500):\n        pred = model(torch.tensor(X_train_norm, dtype=torch.float32))\n        loss = loss_fn(pred, torch.tensor(y_train_norm, dtype=torch.float32))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(f\"Epoch {epoch} - Loss: {loss.item()}\")\n        wandb.log({\"loss\": loss.item()})\n        losses.append(loss.item())\n        if epoch % 20 == 0:\n            visualize_data(\n                model,\n                X_test_norm,\n                y_test_norm,\n                epoch=epoch,\n                open_window=False,\n                log=True,\n            )\n\n    plt.plot(losses)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss over time\")\n    plt.show()\n\n    # Show predictions plot at the end of training\n    visualize_data(\n        model, X_test_norm, y_test_norm, epoch=500, open_window=True, log=True\n    )\n    wandb.finish()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Helpful Tooling for Working with and Debugging Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html",
    "href": "appendices/course_progression.html",
    "title": "Appendix B — Course Lecture Progression",
    "section": "",
    "text": "B.1 Part 0: Review and Foundations\nThis section describes an example progression through the course material based on my “Machine Learning for Mechanical Engineering” course at ETHZ. It assumes two lecture+exercise sessions twice a week, for a duration of 1 hour and 45 minutes each with a 5-10 minutes break in the middle of each session. This provides a sample of how you might work through the content in this book, and also acts as a reference to the students. My course is structured such that lectures and exercises occur in the same session, often alternating between lecture and in-class demonstrations or exercises, and this is frequently reflected in each of the book chapters. In class, for time reasons, I may skip some of the longer derivations in the book, since these can be effectively studied on their own.\nThese lectures introduce the course and also cover some background information that will be foundational and critical later in the course. For illustrative and notational purposes, we will use Linear Regression as a simple model to understand these foundations first, and this will allow us to build up to more complex models as we go through the course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-0-review-and-foundations",
    "href": "appendices/course_progression.html#part-0-review-and-foundations",
    "title": "Appendix B — Course Lecture Progression",
    "section": "",
    "text": "B.1.1 Lecture 1: Course Introduction and Review of ML Basics\n\nOverview the course structure and syllabus\nReview several basics that should have been covered in the prior Stochastics and Machine Learning course\n\nVisualizing Data Review using the California Housing Dataset Notebook\nReview of Cross Validation in Evaluating ML Models\nReview of Linear models\n\n(Re-)Introduction to Coding and Tooling basics to help with the rest of the course, such as Editors, Colab, Version Control, Basic Debugging. Read Helpful Tooling.\n\n\n\nB.1.2 Lecture 2: Review of (Stochastic) Gradient Descent\n\nRead Stochastic Gradient Descent\nRead Why Momentum Really Works\nReview of Regularization for Linear Regresson models, including weight decay (L2) and L1/sparsity control, and effects on loss functions.\nReview of Linear Unsupervised Learning (e.g., PCA, Sparse PCA, etc.)\n\n\n\nB.1.3 Lecture 3: Automatic Differentiation and Taking Derivatives\n\nRead Taking Derivatives, except for Advanced topics like Implicit Differentiation and JVP/HVP (this is for a later lecture)\nIn-Class example of Forward and Reverse AD on a simple function\nIn-Class example of AD through a Verlet Integrator\n\n\n\nB.1.4 Lecture 4: Advanced Differentiation\n\nRead the remaining parts of Taking Derivatives\nIn-Class example of Implicit Differentiation\nIn-Class example with JVPs for computing some example properties of Linear Models, for example, the Fisher Information Matrix.\n(Time Permitting) Examples of Projected Gradient or Proximal Gradient methods.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-1-advanced-neural-network-models",
    "href": "appendices/course_progression.html#part-1-advanced-neural-network-models",
    "title": "Appendix B — Course Lecture Progression",
    "section": "B.2 Part 1: Advanced Neural Network Models",
    "text": "B.2 Part 1: Advanced Neural Network Models\n\nB.2.1 Lecture 5: Review of Neural Network Models\n\nRead Review of Neural Networks\nIn-Class example of Visualizing NN Layers using ConvNetJS\nIn-Class examples of Auto-Encoders on Simple Data, Auto-Encoders on Airfoil Data, and Least Volume Auto-Encoders (including Spectral Normalization)\n\n\n\nB.2.2 Lecture 6: Push-Forward Generative Models\n\nRead Push-Forward Generative Models\nIn-Class examples of GANs, VAEs, and Normalizing Flows on Synthetic Data and the Airfoil Problem\n\n\n\nB.2.3 Lecture 7: Interlude – Techniques for Building and Debugging ML Models\n\nRead Helpful Tooling for Working with and Debugging Machine Learning Models\nIn-Class Before and After example implementing the changes\nIn-Class example of improving/correcting an LLM-provided code solution\n\n\n\nB.2.4 Lecture 8: Optimal Transport\n\nRead Measuring Differences Between Distributions\nIn-Class example of an Optimal Transport Generative Model\n\n\n\nB.2.5 Lecture 9: Stochastic Generative Models\n\nRead Stochastic Generative Models\nIn-Class examples of Diffusion Models on Synthetic Data and the Airfoil Problem\nIn-Class examples of Sequence Data generation (e.g., the Tangrams Example)\n\n\n\nB.2.6 Lecture 10: Latent Generative Neural Models\n\nIn-Class example of Latent Diffusion Models via Least Volume AEs.\nIn-Class example of continuous+discrete models\nPractical Demonstrations of complex models on EngiBench\n\n\n\nB.2.7 Lecture 11:\n\nRead Introduction to Transformers\nIn-Class experiment with the Attention Mechanism\nIn-Class demonstrations of Latent Transformer models (e.g., VQGAN)\n\n\n\nB.2.8 Lecture 12: Review of Reinforcement Learning\n\nRead Reinforcement Learning\nIn-Class example of Linear Functional Q-Learning\n\n\n\nB.2.9 Lecture 13: Actor-Critic and Policy Gradient Methods\n\nPolicy Gradients\n\n\n\nB.2.10 Lecture 14: Advanced Policy Methods\n\nDiffusion Policies\nTransformer Policies\nLatent Generative Policies (e.g., Dreamer and variants)\n\n\n\nB.2.11 Lecture 15: Message Passing, Graph Neural Networks, and other Structured Data\n\nRead Graph Neural Networks\nLearning with Meshes and Point Clouds\nLearning with Signed Distance Fields",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-2-probabilistic-models-and-kernels",
    "href": "appendices/course_progression.html#part-2-probabilistic-models-and-kernels",
    "title": "Appendix B — Course Lecture Progression",
    "section": "B.3 Part 2: Probabilistic Models and Kernels",
    "text": "B.3 Part 2: Probabilistic Models and Kernels\n\nB.3.1 Lecture 16: Review of Probabilistic Models\n\nRead ?sec-reviewprobability of Probabilistic Models, since this reviews the information from the prior course. We will revisit the later sections in a later lecture.\nIn-Class exercise deriving the MLE for various common distributions\n\n\n\nB.3.2 Lecture 17: Introduction to Probablistic Programming\n\nBayesian Linear Regression in a Probabilistic Programming Language\nIntroduction to Approximate Inference Methods, such as MCMC and Variational Inference\n\n\n\nB.3.3 Lecture 18: Large-Scale Bayesian Models and Debugging Probabilistic Inference\n\nStochastic Variational Inference and Stein Variational Gradient Descent\nPitfalls of Probabilistic Models and Debugging Inference\n\n\n\nB.3.4 Lecture 19: Kernel Basics\n\nRepresenter Theorem\nKernelized Ridge Regression\nRegularization of Kernels and effects of Fourier Spectra\n\n\n\nB.3.5 Lecture 20: Gaussian Processes and Bayesian Optimization\n\nBasics of Gaussian Processes\nBayesian Optimization\nEffect of Dimension on GPs and related variants (AdditiveGPs, etc.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/course_progression.html#part-3-engineering-relevant-machine-learning-topics",
    "href": "appendices/course_progression.html#part-3-engineering-relevant-machine-learning-topics",
    "title": "Appendix B — Course Lecture Progression",
    "section": "B.4 Part 3: Engineering-relevant Machine Learning Topics",
    "text": "B.4 Part 3: Engineering-relevant Machine Learning Topics\n\nB.4.1 Lecture 21: Active Learning and Semi-Supervised Learning\n\nNotebook on Active Learning and Semi-Supervised Learning\n\n\n\nB.4.2 Lecture 22: Transfer Learning and Foundation Models\n\nFine-Tuning a Pre-trained model\nJointly embedded models\n\n\n\nB.4.3 Lecture 23: Integrating Everything Together\n\nIn-Class exercise integrating all prior topics using EngiBench/EngiOpt\n\n\n\nB.4.4 Lecture 24: Ethical and Legal Implications of ML within Engineering\n\nAdversarial Attacks and Defenses\nPrivacy-Preserving and Federated ML\nInterpretability Methods\nCase Study: National Algorithms Safety Board",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Lecture Progression</span>"
    ]
  },
  {
    "objectID": "appendices/review_of_singular_value_decomposition.html",
    "href": "appendices/review_of_singular_value_decomposition.html",
    "title": "Appendix C — Review of Matrices and the Singular Value Decomposition",
    "section": "",
    "text": "For more info after class on fundamental matrix properties, see the Matrix Cookbook\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom ipywidgets import interact\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nsns.set_palette(\"Set1\", 8, .75)\nsns.set_color_codes()\n\n# Below are just to tell NumPy to print things nicely\nnp.set_printoptions(precision=3)\nnp.set_printoptions(suppress=True)\nnp.set_printoptions(threshold=5)\n\nLet’s generate a simple circle of points, so that we can see what Matrices do to objects:\n\ndef circle_points(a=1,b=1):\n    ''' Generates points on a ellipsoid on axes length a,b'''\n    t = np.linspace(0,2*np.pi,25)   # Define a line\n    X = np.matrix([a*np.cos(t),b*np.sin(t)]) # Create circle using polar coords\n    return X\n\ndef plot_circle(X,title=None):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(X[0].flat, X[1].flat,c='g')\n    if(title):\n        plt.title(title)\n    plt.axis('equal')\n    plt.show()\n\n\nX = circle_points()\nprint('X:', X.shape)\nprint(X)\nplot_circle(X,'A Unit Circle')\n\nX: (2, 25)\n[[ 1.     0.966  0.866 ...  0.866  0.966  1.   ]\n [ 0.     0.259  0.5   ... -0.5   -0.259 -0.   ]]\n\n\n\n\n\n\n\n\n\n\nM = np.matrix([[2, 0],[0, 1]])\nprint('M'); print(M)\n\nM\n[[2 0]\n [0 1]]\n\n\n\ndef plot_transformed_circle(X,NewX, title=None):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plt.scatter(X[0].flat, X[1].flat, c='g')\n    plt.scatter(NewX[0].flat, NewX[1].flat, c='b')\n    plt.plot(X[0].flat, X[1].flat, color='g')\n    plt.plot(NewX[0].flat, NewX[1].flat, color='b')\n    if(title):\n        plt.title(title)\n    plt.axis('equal')\n    plt.show() \n    \nplot_transformed_circle(X,    # First show the original circle\n                        M*X)  # Then show the transformed circle\n\n\n\n\n\n\n\n\n\nM = np.matrix([[2, 0],[0, 2]])\nprint('M'); print(M)\nplot_transformed_circle(X,M*X)\n\nM\n[[2 0]\n [0 2]]\n\n\n\n\n\n\n\n\n\n\nM = np.matrix([[1,2],[0, 1]])\nprint('M'); print(M)\nplot_transformed_circle(X,M*X)\n\nM\n[[1 2]\n [0 1]]\n\n\n\n\n\n\n\n\n\n\nnp.random.seed(100); np.set_printoptions(precision=1)\n# Now just create a random 2x2 matrix\nR = np.matrix(np.random.rand(2,2))\nprint(R)\n\n# Then transform points by that matrix\nNX = R*X\nplot_transformed_circle(X,NX)\n\n[[0.5 0.3]\n [0.4 0.8]]\n\n\n\n\n\n\n\n\n\nYou can do this same thing to different dimensions of inputs, such as 1-D (lines):\n\nX_line = np.linspace(-1,1,10)  # Just create a 1-D set of points\nprint(\"X_line:\\n\",X_line)\nR[:,0]*X_line   # Convert it into \nprint(\"Transformed:\\n\",R[:,0]*X_line)\nplot_transformed_circle(np.vstack([X_line,np.zeros_like(X_line)]),\n                        R[:,0]*X_line)\n\nX_line:\n [-1.  -0.8 -0.6 ...  0.6  0.8  1. ]\nTransformed:\n [[-0.5 -0.4 -0.3 ...  0.3  0.4  0.5]\n [-0.4 -0.3 -0.2 ...  0.2  0.3  0.4]]\n\n\n\n\n\n\n\n\n\n\nD The Singular Value Decomposition\nFor \\(N\\) data points of \\(d\\) dimensions: \\[\nX_{d\\times N} = U_{d\\times d}  \\Sigma_{d\\times N} V^*_{N\\times N}\n\\] Where \\(U\\), \\(V\\) are orthogonal (\\(UU^T=I\\)), and \\(\\Sigma\\) is diagonal.\n\nNX\n\nmatrix([[ 0.5,  0.6,  0.6, ...,  0.3,  0.5,  0.5],\n        [ 0.4,  0.6,  0.8, ..., -0.1,  0.2,  0.4]])\n\n\n\nU,s,V = np.linalg.svd(NX,full_matrices=False) # Why is this useful?\nS = np.diag(s)\nprint('U:', U.shape)\nprint('S:', S.shape) # Why is this only 2x2, rather than 2x25?\nprint('V:', V.shape) # Why is this only 2x25, rather than 25x25?\nprint('S ='); print(S)\nprint('U*U.T ='); print(U*U.T)\n\nU: (2, 2)\nS: (2, 2)\nV: (2, 25)\nS =\n[[3.8 0. ]\n [0.  1.1]]\nU*U.T =\n[[1. 0.]\n [0. 1.]]\n\n\n\nV\n\nmatrix([[-0.2, -0.2, -0.3, ..., -0. , -0.1, -0.2],\n        [ 0.2,  0.2,  0.1, ...,  0.3,  0.3,  0.2]])\n\n\n\nprint('S ='); print(S)\n\n# Allow me to plot multiple circles on one figure\ndef plot_ax(ax,X,title=None):\n    ax.scatter(X[0].flat, X[1].flat,c='g')\n    ax.set_xlim([-1.5, 1.5])\n    ax.set_ylim([-1.5, 1.5])\n    if(title):\n        plt.title(title)   \n\nfig = plt.figure()\nplot_ax(fig.add_subplot(221, aspect='equal'),    NX, 'Original Data')\nplot_ax(fig.add_subplot(223, aspect='equal'),     V, 'V')\nplot_ax(fig.add_subplot(224, aspect='equal'),   S*V, 'S*V')\nplot_ax(fig.add_subplot(222, aspect='equal'), U*S*V, 'U*S*V')\nplt.show() # Essentially a \"Change of Basis\"\n# U and V are orthogonal matrices, so they just represent Rotations/Reflections\n\nS =\n[[3.8 0. ]\n [0.  1.1]]\n\n\n\n\n\n\n\n\n\n\nY = circle_points(2,2/5) # Create points on a different ellipse\nNY = R*Y  # Transform those points with a matrix\nplot_transformed_circle(Y,NY)\n\n\n\n\n\n\n\n\n\n# Do the SVD\nU,s,V = np.linalg.svd(NY,full_matrices=False)\nS = np.diag(s); print('S ='); print(S)\n\n# Plot the data and the various SVD transformations\nfig = plt.figure()\nplot_ax(fig.add_subplot(221, aspect='equal'),    NY, 'Original Data')\nplot_ax(fig.add_subplot(223, aspect='equal'),     V, 'V')\nplot_ax(fig.add_subplot(224, aspect='equal'),   S*V, 'S*V')\nplot_ax(fig.add_subplot(222, aspect='equal'), U*S*V, 'U*S*V')\nplt.show()\n\nS =\n[[5.1 0. ]\n [0.  0.7]]\n\n\n\n\n\n\n\n\n\n\n\nE What about different dimensions?\n\nM = np.matrix([[1,0,],[0,1],[2,0.5]])\nr = np.matrix([[1],[1]])\nprint(f\"M = {M}\")\nprint(f\"r = {r}\")    # What are the dimensions of r?\nprint(f\"M*r = {M*r}\") # What are the dimensions of M*r?\n\nM = [[1.  0. ]\n [0.  1. ]\n [2.  0.5]]\nr = [[1]\n [1]]\nM*r = [[1. ]\n [1. ]\n [2.5]]\n\n\n\n# Let's plot the points in 3D \ndef plot_3D_circle(X, elev=10., azim=50, title=None, c='b'):\n    \"\"\"\n    Plots 3D points from a (3, N) matrix X using matplotlib.\n\n    Parameters\n    ----------\n    X : np.ndarray or np.matrix\n        3xN array of points to plot.\n    elev : float, optional\n        Elevation angle for the 3D plot.\n    azim : float, optional\n        Azimuth angle for the 3D plot.\n    title : str, optional\n        Title for the plot.\n    c : str, optional\n        Color for the points.\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    x = np.array(X[0]).flatten()\n    y = np.array(X[1]).flatten()\n    z = np.array(X[2]).flatten()\n    ax.scatter(x, y, z, c=c)\n    # Create cubic bounding box to simulate equal aspect ratio\n    max_range = np.array([x.max()-x.min(), y.max()-y.min(), z.max()-z.min()]).max()\n    Xb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][0].flatten() + 0.5*(x.max()+x.min())\n    Yb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][1].flatten() + 0.5*(y.max()+y.min())\n    Zb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][2].flatten() + 0.5*(z.max()+z.min())\n    for xb, yb, zb in zip(Xb, Yb, Zb):\n        ax.plot([xb], [yb], [zb], 'w')\n    ax.view_init(elev=elev, azim=azim)\n    if title:\n        plt.title(title)\n    plt.show()\n    \ndef plot_3D_ax(ax, X, elev=10., azim=50, title=None,c='b'):\n    x = np.array(X[0].flat)\n    y = np.array(X[1].flat)\n    z = np.array(X[2].flat)\n    ax.scatter(x,y,z,c=c)\n    # Create cubic bounding box to simulate equal aspect ratio\n    max_range = np.array([x.max()-x.min(), y.max()-y.min(), z.max()-z.min()]).max()\n    Xb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][0].flatten() + 0.5*(x.max()+x.min())\n    Yb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][1].flatten() + 0.5*(y.max()+y.min())\n    Zb = 0.5*max_range*np.mgrid[-1:2:2,-1:2:2,-1:2:2][2].flatten() + 0.5*(z.max()+z.min())\n    # Comment or uncomment following both lines to test the fake bounding box:\n    for xb, yb, zb in zip(Xb, Yb, Zb):\n       ax.plot([xb], [yb], [zb], 'w')\n\n\nM = np.matrix([[1,0,],[0,1],[2,0.5]])\ninteractive_3D = lambda e,a: plot_3D_circle(M*X,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a=(0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\n\nM*X\n\nmatrix([[ 1. ,  1. ,  0.9, ...,  0.9,  1. ,  1. ],\n        [ 0. ,  0.3,  0.5, ..., -0.5, -0.3, -0. ],\n        [ 2. ,  2.1,  2. , ...,  1.5,  1.8,  2. ]])\n\n\n\nM = np.matrix([[1,2],[-2,2],[2,0.5]])\nprint(M)\n\n[[ 1.   2. ]\n [-2.   2. ]\n [ 2.   0.5]]\n\n\n\ninteractive_3D = lambda e,a: plot_3D_circle(M*Y,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\n\n# M*Y is a 3D set of points\nU,s,V = np.linalg.svd(M*Y,full_matrices=False)\nS = np.diag(s)\nprint('U:', U.shape)\nprint('S:', S.shape)\nprint('V:', V.shape)\nprint('S =')\nprint(S)\n\nU: (3, 3)\nS: (3, 3)\nV: (3, 25)\nS =\n[[21.6  0.   0. ]\n [ 0.   4.   0. ]\n [ 0.   0.   0. ]]\n\n\n\nU*S\n\nmatrix([[ -7.1,   2.9,  -0. ],\n        [ 14.5,   2.5,   0. ],\n        [-14.4,   1. ,   0. ]])\n\n\n\nV\n\nmatrix([[-0.3, -0.3, -0.2, ..., -0.2, -0.3, -0.3],\n        [ 0. ,  0.1,  0.1, ..., -0.1, -0.1,  0. ],\n        [-0.8,  0.5,  0.1, ..., -0. ,  0.1,  0. ]])\n\n\n\nfig = plt.figure()\nplt.plot(np.diag(S),'o-')\nplt.title(\"Magnitude of Singular Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\ninteractive_3D = lambda e,a: plot_3D_circle(V,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\n\ninteractive_3D = lambda e,a: plot_3D_circle(S*V,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\nplt.show()\n\n\n\n\n\ninteractive_3D = lambda e,a: plot_3D_circle(U*S*V,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\n\nprint('S ='); print(S)\nprint('V ='); print(V)\nprint('S*V ='); print(S*V)\n\nS =\n[[21.6  0.   0. ]\n [ 0.   4.   0. ]\n [ 0.   0.   0. ]]\nV =\n[[-0.3 -0.3 -0.2 ... -0.2 -0.3 -0.3]\n [ 0.   0.1  0.1 ... -0.1 -0.1  0. ]\n [-0.8  0.5  0.1 ... -0.   0.1  0. ]]\nS*V =\n[[-6.  -5.8 -5.1 ... -5.3 -5.8 -6. ]\n [ 0.   0.3  0.6 ... -0.5 -0.3  0. ]\n [-0.   0.   0.  ... -0.   0.   0. ]]\n\n\n\n# So if V's 3rd row doesn't matter, why don't we just get rid of it?\nVt = V[0:2,:]\nSt = S[:,0:2]\nprint('St ='); print(St)\nprint('Vt ='); print(Vt)\nprint('St*Vt ='); print(St*Vt)\n\nSt =\n[[21.6  0. ]\n [ 0.   4. ]\n [ 0.   0. ]]\nVt =\n[[-0.3 -0.3 -0.2 ... -0.2 -0.3 -0.3]\n [ 0.   0.1  0.1 ... -0.1 -0.1  0. ]]\nSt*Vt =\n[[-6.  -5.8 -5.1 ... -5.3 -5.8 -6. ]\n [ 0.   0.3  0.6 ... -0.5 -0.3  0. ]\n [ 0.   0.   0.  ...  0.   0.   0. ]]\n\n\n\nprint('U ='); print(U)\n\nU =\n[[-0.3  0.7 -0.6]\n [ 0.7  0.6  0.4]\n [-0.7  0.3  0.7]]\n\n\n\n# Truncate U. Now we have the \"Truncated SVD\"\nUt = U[:,0:2]\nSt = St[0:2,:]\nprint('Ut ='); print(Ut)\nprint('St ='); print(St)\nprint('Vt ='); print(Vt)\n\nUt =\n[[-0.3  0.7]\n [ 0.7  0.6]\n [-0.7  0.3]]\nSt =\n[[21.6  0. ]\n [ 0.   4. ]]\nVt =\n[[-0.3 -0.3 -0.2 ... -0.2 -0.3 -0.3]\n [ 0.   0.1  0.1 ... -0.1 -0.1  0. ]]\n\n\n\nprint('Ut*St*Vt ='); print(Ut*St*Vt) # Even though we threw away info...\nprint('M*Y = '); print(M*Y)\nprint(\"Are they equal?: \", np.allclose(M*Y,Ut*St*Vt))\n\nUt*St*Vt =\n[[ 2.   2.1  2.1 ...  1.3  1.7  2. ]\n [-4.  -3.7 -3.1 ... -3.9 -4.1 -4. ]\n [ 4.   3.9  3.6 ...  3.4  3.8  4. ]]\nM*Y = \n[[ 2.   2.1  2.1 ...  1.3  1.7  2. ]\n [-4.  -3.7 -3.1 ... -3.9 -4.1 -4. ]\n [ 4.   3.9  3.6 ...  3.4  3.8  4. ]]\nAre they equal?:  True\n\n\n\nprint(Vt)\nplot_circle(Vt) # The actual basis which preserves data variability\n\n[[-0.3 -0.3 -0.2 ... -0.2 -0.3 -0.3]\n [ 0.   0.1  0.1 ... -0.1 -0.1  0. ]]\n\n\n\n\n\n\n\n\n\n\n# Let's make things more difficult - add some noise:\nZ = M*X + np.random.normal(0,0.5,size=(M*X).shape)\ninteractive_3D = lambda e,a: plot_3D_circle(Z,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\n\nUe,se,Ve = np.linalg.svd(Z,full_matrices=False)\nSe = np.diag(se)\nprint('S ='); print(Se) # What is different, compared to no-noise?\n\nS =\n[[11.5  0.   0. ]\n [ 0.   9.7  0. ]\n [ 0.   0.   2.8]]\n\n\n\n# Truncate:\nUet = Ue[:,0:2]\nSet = Se[0:2,0:2]\nVet = Ve[0:2,:]\nprint(Z); print(); print(Uet*Set*Vet)\n\n[[ 1.5  1.7  2.  ...  0.2  1.1  0.8]\n [-2.  -1.3 -1.5 ... -3.  -2.9 -2.4]\n [ 2.1  2.3  1.6 ...  1.9  1.1  2.3]]\n\n[[ 1.3  1.7  1.5 ...  0.2  0.5  0.9]\n [-1.9 -1.3 -1.2 ... -3.  -2.5 -2.5]\n [ 2.3  2.3  2.  ...  1.9  1.9  2.2]]\n\n\n\nprint(\"Are they equal?: \", np.allclose(Z,Uet*Set*Vet)) # We lost info\n\nAre they equal?:  False\n\n\n\nx = np.arange(-.2,.3,.05)\ny = np.arange(-.6,.6,.1)\nvep = np.matrix(np.transpose([np.tile(x, len(y)), np.repeat(y, len(x))]))\nZt = Uet*Set*vep.T\n#Zt = Uet*Set*Vet\ndef compare_3D(Z, Zt, elev=10., azim=50):\n    \"\"\"\n    Plots two sets of 3D points for visual comparison.\n\n    Parameters\n    ----------\n    Z : np.ndarray or np.matrix\n        Original 3D data (shape: 3 x N).\n    Zt : np.ndarray or np.matrix\n        Transformed 3D data (shape: 3 x N).\n    elev : float, optional\n        Elevation angle for the 3D plot.\n    azim : float, optional\n        Azimuth angle for the 3D plot.\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    # Ensure shapes are (3, N)\n    Z = np.asarray(Z)\n    Zt = np.asarray(Zt)\n    # If Zt is 2D, pad with zeros for 3D visualization\n    if Zt.shape[0] == 2:\n        Zt = np.vstack([Zt, np.zeros(Zt.shape[1])])\n    ax.scatter(Z[0], Z[1], Z[2], c='b', label='Original')\n    ax.scatter(Zt[0], Zt[1], Zt[2], c='g', label='Transformed')\n    ax.view_init(elev=elev, azim=azim)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n\ninteractive_3D = lambda e,a: compare_3D(Z,Zt,elev=e,azim=a)\ninteract(interactive_3D, e=(0,90,30), a = (0,360,30))\n\n\n\n\n&lt;function __main__.&lt;lambda&gt;(e, a)&gt;\n\n\nThe idea of uncovering structure, or reducing data-dimensions is one key goal of Unsupervised Learning. In particular, the SVD (among other methods) can be used for Principal Component Analysis: reducing the number of dimensions of a data-set, by finding a linear transformation to a smaller orthogonal basis which minimizes reconstruction error to the original space.\n\nfrom sklearn.decomposition import PCA\npcaY = PCA(n_components=2).fit_transform(np.asarray(Z).T)\npcaY = np.array([[-.1,0],[0,-.1]])@pcaY.T # Some scaling/flipping\nfig = plt.figure()\nplot_ax(fig.add_subplot(121, aspect='equal'), 4*pcaY, 'PCA')\nplot_ax(fig.add_subplot(122, aspect='equal'),  4*Vet, 'SVD')\nplt.show()\n# Note: result is (essentially) identical to PCA (up to scale/flipped axes)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Review of Matrices and the Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "appendices/review_of_math_and_computing_foundations.html",
    "href": "appendices/review_of_math_and_computing_foundations.html",
    "title": "Appendix D — Reviewing Mathematical and Computational Foundations for Machine Learning",
    "section": "",
    "text": "D.1 Random Variables, Monte Carlo, and Concentration Inequalities\nThis notebook is designed to help you build (or refresh) the conceptual and computational foundations needed for understanding probabilistic generative modeling methods and other important topics in Machine Learning. You can look at each sub-section independently and in any order to refresh your knowledge of some foundations, though we may build upon certain common examples throughout.\nLearning Objectives\nBy the end of this notebook, you will be able to:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Reviewing Mathematical and Computational Foundations for Machine Learning</span>"
    ]
  },
  {
    "objectID": "appendices/review_of_math_and_computing_foundations.html#random-variables-monte-carlo-and-concentration-inequalities",
    "href": "appendices/review_of_math_and_computing_foundations.html#random-variables-monte-carlo-and-concentration-inequalities",
    "title": "Appendix D — Reviewing Mathematical and Computational Foundations for Machine Learning",
    "section": "",
    "text": "D.1.1 Concepts:\n\nRandom variables are functions that map outcomes to numerical values\nExpectations represent the average value of a function over a probability distribution\nMonte Carlo methods use random sampling to approximate expectations and integrals\nThe Law of Large Numbers ensures that sample averages converge to the true expectation as sample size increases\nConvergence rate is typically \\(O(1/\\sqrt{N})\\), meaning we need 4x more samples to halve the error\nFinite samples introduce uncertainty that decreases with more data and Concentration Inequalities (Chebyshev, Hoeffding) bound the probability of large deviations, which can help us understand real-world machine learning behavior\n\n\n\nD.1.2 Key Equations:\nExpectation via Monte Carlo: \\[\\mathbb{E}_{p(x)}[f(X)] = \\int f(x) p(x) dx \\approx \\frac{1}{N} \\sum_{i=1}^{N} f(x_i), \\quad x_i \\sim p(x)\\]\nIntuition: If we can sample from a distribution, we can approximate any expectation by averaging function values over those samples. The approximation improves as we collect more samples.\nLaw of Large Numbers: \\[\\bar{X}_N = \\frac{1}{N} \\sum_{i=1}^{N} X_i \\xrightarrow{N \\to \\infty} \\mathbb{E}[X]\\]\nChebyshev’s Inequality: \\[P(|\\bar{X}_N - \\mu| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{N\\epsilon^2}\\]\nHoeffding’s Inequality (for bounded r.v. in \\([a,b]\\)): \\[P(|\\bar{X}_N - \\mu| \\geq \\epsilon) \\leq 2\\exp\\left(-\\frac{2N\\epsilon^2}{(b-a)^2}\\right)\\]\nUnderstanding convergence of statistical estimates of various functions will end up being foundational to many aspects of Machine Learning, so it is useful for us to build intuition about it ahead of time.\nIntuition: These inequalities tell us how confident we can be that our sample mean is close to the true mean. They guarantee exponential or polynomial concentration as \\(N\\) grows.\n\n\nD.1.3 Monte Carlo Mean Estimation\nWe’ll sample from a standard normal distribution \\(\\mathcal{N}(0, 1)\\) and watch how the sample mean converges to the true mean (which is 0) as we increase the number of samples.\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# True parameters\ntrue_mean = 0.0\ntrue_std = 1.0\n\n# Sample sizes to test\nsample_sizes = np.logspace(1, 4, 50).astype(int)  # From 10 to 10,000 samples\n\n# Compute sample means for increasing sample sizes\nsample_means = []\nconfidence_intervals = []\n\nfor N in sample_sizes:\n    # Draw N samples from standard normal\n    samples = np.random.normal(true_mean, true_std, N)\n    \n    # Compute sample mean\n    sample_mean = np.mean(samples)\n    sample_means.append(sample_mean)\n    \n    # Compute 95% confidence interval (±1.96 * standard error)\n    standard_error = np.std(samples) / np.sqrt(N)\n    ci = 1.96 * standard_error\n    confidence_intervals.append(ci)\n\nsample_means = np.array(sample_means)\nconfidence_intervals = np.array(confidence_intervals)\n\n\n\nShow Code\n# Create visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left plot: Sample mean convergence with confidence bands\nax1.plot(sample_sizes, sample_means, 'b-', linewidth=2, label='Sample Mean')\nax1.fill_between(sample_sizes, \n                  sample_means - confidence_intervals,\n                  sample_means + confidence_intervals,\n                  alpha=0.3, color='blue', label='95% Confidence Interval')\nax1.axhline(true_mean, color='r', linestyle='--', linewidth=2, label='True Mean')\nax1.set_xlabel('Number of Samples (N)')\nax1.set_ylabel('Estimated Mean')\nax1.set_xscale('log')\nax1.set_title('Convergence of Sample Mean')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Right plot: Confidence interval width vs sample size\nax2.plot(sample_sizes, confidence_intervals, 'g-', linewidth=2)\nax2.set_xlabel('Number of Samples (N)')\nax2.set_ylabel('95% CI Width (±)')\nax2.set_xscale('log')\nax2.set_yscale('log')\nax2.set_title('Uncertainty Shrinks as $1/\\\\sqrt{N}$')\nax2.grid(True, alpha=0.3)\n\n# Add reference line showing 1/sqrt(N) scaling\nreference_line = confidence_intervals[0] * np.sqrt(sample_sizes[0]) / np.sqrt(sample_sizes)\nax2.plot(sample_sizes, reference_line, 'r--', linewidth=1.5, \n         label='$1/\\\\sqrt{N}$ scaling', alpha=0.7)\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the sample mean oscillates around the true value but gradually settles down. The confidence interval shrinks, but not linearly—it follows a \\(1/\\sqrt{N}\\) pattern. This means:\n\nTo halve your error, you need 4x more samples\nTo reduce error by 10x, you need 100x more samples\n\nWhy do you think the convergence is so slow? What does this tell you about the computational cost of Monte Carlo methods, in general?\n\n\nD.1.4 Expectation of a Nonlinear Function\nSo far we’ve estimated the mean of a simple identity function. But Monte Carlo really shines when computing expectations of complex, nonlinear functions. Let’s estimate \\(\\mathbb{E}[X^2]\\) where \\(X \\sim \\mathcal{N}(0, 1)\\).\n\n# For a standard normal, E[X^2] = Var(X) + E[X]^2 = 1 + 0 = 1\ntrue_expectation = 1.0\n\n# Generate samples and compute E[X^2]\nnp.random.seed(42)\nsample_sizes = np.logspace(1, 6, 200).astype(int)\n\n# Function we want to compute the expectation of\ndef f(x):\n    return x**2\n\n# Monte Carlo estimates\nmc_estimates = []\nfor N in sample_sizes:\n    samples = np.random.normal(0, 1, N)\n    estimate = np.mean(f(samples))\n    mc_estimates.append(estimate)\n\nmc_estimates = np.array(mc_estimates)\n\n\n\n\n\n\n\n\n\n\nShow Code\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left plot: Convergence to true value\nax1.plot(sample_sizes, mc_estimates, 'b-', linewidth=2, label='Monte Carlo Estimate')\nax1.axhline(true_expectation, color='r', linestyle='--', linewidth=2, \n            label=f'True Value = {true_expectation}')\nax1.set_xlabel('Number of Samples (N)')\nax1.set_ylabel('Estimated $\\\\mathbb{E}[X^2]$')\nax1.set_xscale('log')\nax1.set_title('Monte Carlo Estimation of $\\\\mathbb{E}[X^2]$')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_ylim(0.8, 1.2)\n\n# Right plot: Absolute error\nabsolute_error = np.abs(mc_estimates - true_expectation)\nax2.plot(sample_sizes, absolute_error, 'g-', linewidth=2)\nax2.set_xlabel('Number of Samples (N)')\nax2.set_ylabel('Absolute Error')\nax2.set_xscale('log')\nax2.set_yscale('log')\nax2.set_title('Error Decreases as $1/\\\\sqrt{N}$')\nax2.grid(True, alpha=0.3)\n\n# Add reference line\nreference = absolute_error[0] * np.sqrt(sample_sizes[0]) / np.sqrt(sample_sizes)\nax2.plot(sample_sizes, reference, 'r--', linewidth=1.5, \n         label='$1/\\\\sqrt{N}$ scaling', alpha=0.7)\nax2.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Reviewing Mathematical and Computational Foundations for Machine Learning</span>"
    ]
  },
  {
    "objectID": "appendices/review_of_math_and_computing_foundations.html#change-of-variables-jacobians",
    "href": "appendices/review_of_math_and_computing_foundations.html#change-of-variables-jacobians",
    "title": "Appendix D — Reviewing Mathematical and Computational Foundations for Machine Learning",
    "section": "D.2 Change of Variables & Jacobians",
    "text": "D.2 Change of Variables & Jacobians\n\nD.2.1 Concepts:\n\nWhen you transform a random variable, the probability density changes\nThe Jacobian matrix captures how a transformation stretches or compresses space\nThe determinant of the Jacobian computes how the volume scales in the new space operated on by the Jacobian\nThis principle underlies normalizing flows and other generative models\nIn 1D: \\(|\\frac{dy}{dx}|\\) tells us how density scales; in higher dimensions we use \\(|\\det J|\\)\n\n\n\nD.2.2 Key Equations:\nChange of Variables Formula (1D): \\[p_Y(y) = p_X(x) \\left| \\frac{dx}{dy} \\right| = p_X(g^{-1}(y)) \\left| \\frac{dg^{-1}}{dy} \\right|\\]\nwhere \\(y = g(x)\\) is a transformation.\nChange of Variables Formula (Multidimensional): \\[p_Y(y) = p_X(x) \\left| \\det \\frac{\\partial x}{\\partial y} \\right| = p_X(g^{-1}(y)) \\left| \\det J_{g^{-1}}(y) \\right|\\]\nIntuition: When you transform a random variable, the probability density must be adjusted by how much the transformation stretches or compresses space. Think of it like this: if a transformation spreads points apart (stretches), the density must decrease to keep total probability = 1.\n\n\nD.2.3 Example: Linear Transformation (1D)\nLet’s start with a simple linear transformation: \\(y = 2x + 1\\). We’ll sample from a standard normal and see how the distribution changes. Recall that for a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\), a linear transformation \\(y = ax + b\\) results in \\(y \\sim \\mathcal{N}(a\\mu + b, (a\\sigma)^2)\\).\nIn this example, we should expect the expected mean of Y to be 1 and the standard deviation to be 2. Moreover, we should expect the density scaling factor from the Jacobian to be:\n\\[\n\\left| \\frac{dx}{dy} \\right| = \\frac{1}{a} = 0.5\n\\]\nwhich should be multiplied into the original density in the plots below.\n\n\nShow Code\n# Sample from standard normal\nnp.random.seed(42)\nn_samples = 10000\nx_samples = np.random.normal(0, 1, n_samples)\n\n# Apply linear transformation: y = 2x + 1\na, b = 2, 1\ny_samples = a * x_samples + b\n\n# For plotting, create a grid\nx_grid = np.linspace(-4, 4, 1000)\ny_grid = a * x_grid + b\n\n# Original density: N(0, 1)\npx = stats.norm.pdf(x_grid, 0, 1)\n\n# Transformed density (analytical): N(a*0 + b, (a*1)^2)\npy_analytical = stats.norm.pdf(y_grid, b, np.abs(a))\n\n# Using change of variables: p_Y(y) = p_X((y-b)/a) * |1/a|\n# For y = ax + b, we have x = (y-b)/a, so dx/dy = 1/a\npy_cov = stats.norm.pdf((y_grid - b) / a, 0, 1) * np.abs(1/a)\n\n# Create visualization\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Top left: Original samples histogram\naxes[0, 0].hist(x_samples, bins=50, density=True, alpha=0.6, color='blue', label='Samples')\naxes[0, 0].plot(x_grid, px, 'r-', linewidth=2, label='True PDF')\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('Density')\naxes[0, 0].set_title('Original Distribution: $X \\\\sim \\\\mathcal{N}(0, 1)$')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Top right: Transformed samples histogram\naxes[0, 1].hist(y_samples, bins=50, density=True, alpha=0.6, color='green', label='Samples')\naxes[0, 1].plot(y_grid, py_analytical, 'r-', linewidth=2, label='Analytical PDF')\naxes[0, 1].plot(y_grid, py_cov, 'k--', linewidth=2, label='Change of Variables', alpha=0.7)\naxes[0, 1].set_xlabel('y')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].set_title('Transformed Distribution: $Y = 2X + 1$')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Bottom left: Side-by-side comparison\naxes[1, 0].plot(x_grid, px, 'b-', linewidth=2, label='$p_X(x)$', alpha=0.7)\n# Plot transformed density on same scale (shift y_grid back to x scale for comparison)\naxes[1, 0].plot(y_grid, py_analytical, 'g-', linewidth=2, label='$p_Y(y)$', alpha=0.7)\naxes[1, 0].set_xlabel('Value')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].set_title('Comparing Densities (notice the scaling)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Bottom right: Derivative visualization\naxes[1, 1].plot(y_grid, np.ones_like(y_grid) * np.abs(1/a), 'r-', linewidth=3)\naxes[1, 1].axhline(np.abs(1/a), color='r', linestyle='--', alpha=0.3)\naxes[1, 1].set_xlabel('y')\naxes[1, 1].set_ylabel('$|dx/dy|$')\naxes[1, 1].set_title(f'Jacobian Factor: $|dx/dy| = |1/{a}| = {np.abs(1/a):.2f}$')\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].set_ylim(0, 1)\naxes[1, 1].text(0, 0.6, f'Density is scaled by {np.abs(1/a):.2f}\\n' + \n                f'(stretched by factor of {np.abs(a)})', \n                fontsize=11, ha='center',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Sample mean of Y: {np.mean(y_samples):.4f} (expected: {b})\")\nprint(f\"Sample std of Y: {np.std(y_samples):.4f} (expected: {np.abs(a):.4f})\")\n\n\n\n\n\n\n\n\n\nSample mean of Y: 0.9957 (expected: 1)\nSample std of Y: 2.0068 (expected: 2.0000)\n\nJacobian factor |dx/dy| = 0.5000\nDensity is compressed by factor of 2\n\n\nNotice how the transformation \\(y = 2x + 1\\) shifts the mean (the \\(+1\\) part) and stretches the distribution (the \\(2\\) x part). The density gets compressed because the same probability mass is now spread over a wider range.\nThe Jacobian factor \\(|dx/dy| = 1/2\\) tells us exactly how much to scale the density.\n\n\nD.2.4 Example: Nonlinear Transformation\nLinear transformations are straightforward, but what about nonlinear ones? Let’s try \\(y = \\tanh(x)\\), which squashes values into the range \\((-1, 1)\\).\n\n\nShow Code\n# Sample from standard normal\nnp.random.seed(42)\nn_samples = 10000\nz_samples = np.random.normal(0, 1, n_samples)\n\n# Apply nonlinear transformation: y = tanh(z)\ny_samples = np.tanh(z_samples)\n\n# Create grids for visualization\nz_grid = np.linspace(-3, 3, 1000)\ny_grid = np.linspace(-0.99, 0.99, 1000)\n\n# Original density\npz = stats.norm.pdf(z_grid, 0, 1)\n\n# For transformed density, we need the inverse: z = arctanh(y)\n# and the derivative: dz/dy = 1/(1-y^2)\nz_from_y = np.arctanh(y_grid)\ndz_dy = 1 / (1 - y_grid**2)\n\n# Apply change of variables\npy_analytical = stats.norm.pdf(z_from_y, 0, 1) * np.abs(dz_dy)\n\n# Create visualization\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Top left: Original samples\naxes[0, 0].hist(z_samples, bins=50, density=True, alpha=0.6, color='blue', label='Samples')\naxes[0, 0].plot(z_grid, pz, 'r-', linewidth=2, label='True PDF')\naxes[0, 0].set_xlabel('z')\naxes[0, 0].set_ylabel('Density')\naxes[0, 0].set_title('Original: $Z \\\\sim \\\\mathcal{N}(0, 1)$')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Top right: Transformed samples\naxes[0, 1].hist(y_samples, bins=50, density=True, alpha=0.6, color='green', \n                label='Empirical (samples)')\naxes[0, 1].plot(y_grid, py_analytical, 'r-', linewidth=2, label='Analytical (CoV)')\naxes[0, 1].set_xlabel('y')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].set_title('Transformed: $Y = \\\\tanh(Z)$')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Bottom left: Transformation function\naxes[1, 0].plot(z_grid, np.tanh(z_grid), 'purple', linewidth=2.5)\naxes[1, 0].plot(z_grid, z_grid, 'k--', alpha=0.3, label='Identity')\naxes[1, 0].set_xlabel('z')\naxes[1, 0].set_ylabel('y = tanh(z)')\naxes[1, 0].set_title('The Transformation Function')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].set_xlim(-3, 3)\naxes[1, 0].set_ylim(-1.5, 1.5)\n\n# Bottom right: Jacobian (derivative)\naxes[1, 1].plot(y_grid, dz_dy, 'orange', linewidth=2.5)\naxes[1, 1].set_xlabel('y')\naxes[1, 1].set_ylabel('$|dz/dy| = 1/(1-y^2)$')\naxes[1, 1].set_title('Jacobian Factor (notice the blow-up near ±1)')\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].set_ylim(0, 20)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe \\(\\tanh\\) function compresses the tails of the Gaussian distribution into a bounded range. Notice how the Jacobian factor \\(|dz/dy| = 1/(1-y^2)\\) grows very large near \\(y = \\pm 1\\). This creates the characteristic “U-shaped” density—probability piles up where the transformation compresses most severely.\nThis type of transformation is common in neural networks (e.g., in activation functions). What do you think would happen if we used a transformation that wasn’t invertible?\n\n\nD.2.5 Example: 2D Transformation and Grid Deformation\nIn higher dimensions, the Jacobian is a matrix, and its determinant tells us about volume changes. Let’s visualize this with a 2D linear transformation.\n\n\nShow Code\n# Define a 2D linear transformation matrix\n# We'll use a matrix that scales by 2 in x-direction and rotates\ntheta = np.pi / 6  # 30 degrees\nscale_x, scale_y = 2.0, 1.0\n\n# Transformation matrix: scale then rotate\nR = np.array([[np.cos(theta), -np.sin(theta)],\n              [np.sin(theta), np.cos(theta)]])\nS = np.array([[scale_x, 0],\n              [0, scale_y]])\nA = R @ S\n\nprint(\"Transformation matrix A:\")\nprint(A)\nprint(f\"\\nDeterminant (area scaling factor): {np.linalg.det(A):.4f}\")\n\n# Create a regular grid\nn_grid = 10\nx_range = np.linspace(-2, 2, n_grid)\ny_range = np.linspace(-2, 2, n_grid)\nX, Y = np.meshgrid(x_range, y_range)\n\n# Stack into points\npoints = np.stack([X.ravel(), Y.ravel()], axis=1)\n\n# Transform the grid\ntransformed_points = points @ A.T\nX_transformed = transformed_points[:, 0].reshape(X.shape)\nY_transformed = transformed_points[:, 1].reshape(Y.shape)\n\n# Sample from 2D Gaussian\nnp.random.seed(42)\nn_samples = 1000\nsamples = np.random.multivariate_normal([0, 0], np.eye(2), n_samples)\ntransformed_samples = samples @ A.T\n\n# Create visualization\nfig = plt.figure(figsize=(16, 7))\n\n# Left plot: Original grid and samples\nax1 = fig.add_subplot(121)\n# Draw grid lines\nfor i in range(n_grid):\n    ax1.plot(X[i, :], Y[i, :], 'b-', alpha=0.3, linewidth=0.8)\n    ax1.plot(X[:, i], Y[:, i], 'b-', alpha=0.3, linewidth=0.8)\n\n# Draw a unit square to highlight\nsquare = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\nax1.plot(square[:, 0], square[:, 1], 'r-', linewidth=2.5, label='Unit Square')\nax1.fill(square[:, 0], square[:, 1], 'red', alpha=0.2)\n\n# Plot samples\nax1.scatter(samples[:, 0], samples[:, 1], c='green', alpha=0.3, s=10, label='Samples')\n\nax1.set_xlabel('$x_1$')\nax1.set_ylabel('$x_2$')\nax1.set_title('Original Space')\nax1.legend()\nax1.grid(True, alpha=0.2)\nax1.set_aspect('equal')\nax1.set_xlim(-3, 3)\nax1.set_ylim(-3, 3)\n\n# Right plot: Transformed grid and samples\nax2 = fig.add_subplot(122)\n# Draw transformed grid lines\nfor i in range(n_grid):\n    ax2.plot(X_transformed[i, :], Y_transformed[i, :], 'b-', alpha=0.3, linewidth=0.8)\n    ax2.plot(X_transformed[:, i], Y_transformed[:, i], 'b-', alpha=0.3, linewidth=0.8)\n\n# Draw transformed unit square\ntransformed_square = square @ A.T\nax2.plot(transformed_square[:, 0], transformed_square[:, 1], 'r-', \n         linewidth=2.5, label=f'Transformed Square (area x {np.linalg.det(A):.2f})')\nax2.fill(transformed_square[:, 0], transformed_square[:, 1], 'red', alpha=0.2)\n\n# Plot transformed samples\nax2.scatter(transformed_samples[:, 0], transformed_samples[:, 1], \n            c='green', alpha=0.3, s=10, label='Samples')\n\nax2.set_xlabel('$y_1$')\nax2.set_ylabel('$y_2$')\nax2.set_title('Transformed Space: $y = Ax$')\nax2.legend()\nax2.grid(True, alpha=0.2)\nax2.set_aspect('equal')\nax2.set_xlim(-5, 5)\nax2.set_ylim(-5, 5)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate areas\noriginal_area = 1.0  # Unit square\ntransformed_area = np.linalg.det(A) * original_area\n\nprint(f\"\\nOriginal unit square area: {original_area:.4f}\")\nprint(f\"Transformed square area: {transformed_area:.4f}\")\nprint(f\"Area scaling factor: {np.linalg.det(A):.4f}\")\n\n\nTransformation matrix A:\n[[ 1.73205081 -0.5       ]\n [ 1.          0.8660254 ]]\n\nDeterminant (area scaling factor): 2.0000\n\n\n\n\n\n\n\n\n\n\nOriginal unit square area: 1.0000\nTransformed square area: 2.0000\nArea scaling factor: 2.0000\n\nThis means density must be scaled by 0.5000 to preserve probability mass\n\n\n\n\nD.2.6 Example: Computing Jacobians with PyTorch\nLet’s use PyTorch’s automatic differentiation to compute Jacobians for us. This is especially useful for complex non-linear functions for which writing out the analytical gradients might not be practical.\n\n\nShow Code\n# Define a transformation function\ndef transform_2d(x):\n    \"\"\"\n    A nonlinear transformation: \n    y1 = x1^2 + 0.5*x2\n    y2 = x1 + sin(x2)\n    \"\"\"\n    y1 = x[:, 0]**2 + 0.5 * x[:, 1]\n    y2 = x[:, 0] + torch.sin(x[:, 1])\n    return torch.stack([y1, y2], dim=1)\n\n# Function to compute Jacobian determinant using PyTorch\ndef jacobian_determinant(func, x):\n    \"\"\"\n    Compute the Jacobian determinant for a batch of inputs.\n    \n    Args:\n        func: transformation function\n        x: input tensor of shape (batch_size, input_dim)\n    \n    Returns:\n        det_J: Jacobian determinants of shape (batch_size,)\n    \"\"\"\n    batch_size, input_dim = x.shape\n    x_var = x.clone().detach().requires_grad_(True)\n    \n    # Compute Jacobian for each sample\n    jacobians = []\n    for i in range(batch_size):\n        # Compute Jacobian for single sample\n        jac = torch.autograd.functional.jacobian(func, x_var[i:i+1])\n        jac = jac.squeeze()  # Remove batch dimensions\n        jacobians.append(jac)\n    \n    jacobians = torch.stack(jacobians)\n    \n    # Compute determinants\n    det_J = torch.det(jacobians)\n    return det_J, jacobians\n\n# Visualize how Jacobian determinant varies across space\nx1_range = torch.linspace(-2, 2, 30)\nx2_range = torch.linspace(-2, 2, 30)\nX1, X2 = torch.meshgrid(x1_range, x2_range, indexing='ij')\n\ngrid_points = torch.stack([X1.flatten(), X2.flatten()], dim=1)\ndet_J_grid, _ = jacobian_determinant(transform_2d, grid_points)\ndet_J_grid = det_J_grid.abs().reshape(X1.shape)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 8))\ncontour = ax.contourf(X1.numpy(), X2.numpy(), det_J_grid.detach().numpy(), \n                       levels=20, cmap='viridis')\nplt.colorbar(contour, ax=ax, label='$|\\\\det(J)|$')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Jacobian Determinant Varies Across Space\\n(shows how much stretching/compression at each point)')\nax.grid(True, alpha=0.3, color='white', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nUnderstanding how to compute and interpret Jacobians is useful in many modern machine learning models. The Jacobian determinant tells us the local volume scaling at each point. For nonlinear transformations, this varies across space—some regions expand while others compress.\nThis is the core principle used in normalizing flows: by chaining multiple invertible transformations (each with computable Jacobians), we can transform a simple distribution (like a Gaussian) into arbitrarily complex distributions. The key requirement is that we can compute \\(|\\det(J)|\\) efficiently.\nCan you think of why we need the transformation to be invertible for normalizing flows? How would you chain multiple transformations together? What happens to the overall Jacobian determinant? (We will revisit these in the relevant generative models chapter.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Reviewing Mathematical and Computational Foundations for Machine Learning</span>"
    ]
  },
  {
    "objectID": "appendices/review_of_math_and_computing_foundations.html#linear-algebra-spectral-norms",
    "href": "appendices/review_of_math_and_computing_foundations.html#linear-algebra-spectral-norms",
    "title": "Appendix D — Reviewing Mathematical and Computational Foundations for Machine Learning",
    "section": "D.3 Linear Algebra & Spectral Norms",
    "text": "D.3 Linear Algebra & Spectral Norms\n\nD.3.1 Concepts:\n\nMatrices represent linear transformations that stretch, rotate, and compress space\nSingular values measure how much a matrix stretches along different directions\nThe spectral norm (largest singular value) gives the maximum stretching factor\nMatrix conditioning (ratio of largest to smallest singular value) indicates numerical stability\nUnderstanding these geometric properties is essential for analyzing neural networks and optimization\n\n\n\nD.3.2 Key Equations:\nSingular Value Decomposition: \\[A = U \\Sigma V^T\\] where \\(U, V\\) are orthogonal and \\(\\Sigma\\) contains singular values \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq 0\\)\nSpectral Norm (Matrix 2-norm): \\[\\|A\\|_2 = \\sigma_{\\max}(A) = \\max_{\\|x\\|=1} \\|Ax\\|\\]\nCondition Number: \\[\\kappa(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}\\]\nIntuition: The spectral norm tells you “how much can this transformation amplify a vector?” Large spectral norms mean the transformation can greatly magnify small changes in input—this is crucial for understanding gradient flow in deep networks.\n\n\nD.3.3 Example: 2D Linear Transform Visualization\nLet’s see how a matrix transforms space by watching what it does to a regular grid. This gives us intuition for singular values and the spectral norm.\n\n\nShow Code\n# Create an interesting transformation matrix\n# Mix of rotation and stretching\nnp.random.seed(42)\nA = np.array([[2.0, 0.5],\n              [0.3, 1.0]])\n\nprint(\"Transformation matrix A:\")\nprint(A)\n\n# Compute SVD\nU, singular_values, Vt = np.linalg.svd(A)\nprint(f\"\\nSingular values: {singular_values}\")\nprint(f\"Spectral norm (max singular value): {singular_values[0]:.4f}\")\nprint(f\"Condition number: {singular_values[0]/singular_values[1]:.4f}\")\n\n# Create a grid of points\nn_grid = 15\nx = np.linspace(-1, 1, n_grid)\ny = np.linspace(-1, 1, n_grid)\nX, Y = np.meshgrid(x, y)\n\n# Create points on grid\npoints = np.stack([X.ravel(), Y.ravel()], axis=0)\n\n# Apply transformation\ntransformed_points = A @ points\n\n# Reshape back to grid\nX_trans = transformed_points[0, :].reshape(X.shape)\nY_trans = transformed_points[1, :].reshape(Y.shape)\n\n# Create visualization\nfig = plt.figure(figsize=(16, 7))\n\n# Left: Original grid\nax1 = fig.add_subplot(121, aspect='equal')\nfor i in range(n_grid):\n    ax1.plot(X[i, :], Y[i, :], 'b-', alpha=0.5, linewidth=1)\n    ax1.plot(X[:, i], Y[:, i], 'b-', alpha=0.5, linewidth=1)\n\n# Draw unit circle\ntheta = np.linspace(0, 2*np.pi, 100)\nunit_circle_x = np.cos(theta)\nunit_circle_y = np.sin(theta)\nax1.plot(unit_circle_x, unit_circle_y, 'r-', linewidth=2.5, label='Unit Circle')\nax1.fill(unit_circle_x, unit_circle_y, 'red', alpha=0.1)\n\n# Draw coordinate axes\nax1.arrow(0, 0, 0.9, 0, head_width=0.05, head_length=0.05, fc='green', ec='green', linewidth=2)\nax1.arrow(0, 0, 0, 0.9, head_width=0.05, head_length=0.05, fc='purple', ec='purple', linewidth=2)\nax1.text(1.0, 0, '$e_1$', fontsize=14, color='green', fontweight='bold')\nax1.text(0, 1.0, '$e_2$', fontsize=14, color='purple', fontweight='bold')\n\nax1.set_xlim(-1.5, 1.5)\nax1.set_ylim(-1.5, 1.5)\nax1.set_xlabel('$x_1$')\nax1.set_ylabel('$x_2$')\nax1.set_title('Original Space')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Right: Transformed grid\nax2 = fig.add_subplot(122, aspect='equal')\nfor i in range(n_grid):\n    ax2.plot(X_trans[i, :], Y_trans[i, :], 'b-', alpha=0.5, linewidth=1)\n    ax2.plot(X_trans[:, i], Y_trans[:, i], 'b-', alpha=0.5, linewidth=1)\n\n# Transform unit circle -&gt; ellipse\nellipse_points = A @ np.stack([unit_circle_x, unit_circle_y])\nax2.plot(ellipse_points[0, :], ellipse_points[1, :], 'r-', linewidth=2.5, \n         label='Transformed Circle')\nax2.fill(ellipse_points[0, :], ellipse_points[1, :], 'red', alpha=0.1)\n\n# Draw transformed coordinate axes\ne1_trans = A @ np.array([1, 0])\ne2_trans = A @ np.array([0, 1])\nax2.arrow(0, 0, e1_trans[0]*0.9, e1_trans[1]*0.9, \n          head_width=0.1, head_length=0.1, fc='green', ec='green', linewidth=2)\nax2.arrow(0, 0, e2_trans[0]*0.9, e2_trans[1]*0.9, \n          head_width=0.1, head_length=0.1, fc='purple', ec='purple', linewidth=2)\nax2.text(e1_trans[0]*1.1, e1_trans[1]*1.1, '$Ae_1$', \n         fontsize=14, color='green', fontweight='bold')\nax2.text(e2_trans[0]*1.1, e2_trans[1]*1.1, '$Ae_2$', \n         fontsize=14, color='purple', fontweight='bold')\n\n# Draw singular vectors\n# Right singular vectors (V) point in direction of maximum stretch\n# Left singular vectors (U) point in direction of output\nv1 = Vt[0, :]  # Direction of max stretch in input\nv2 = Vt[1, :]  # Direction of min stretch in input\nu1 = U[:, 0]   # Direction in output\nu2 = U[:, 1]\n\n# Draw principal directions in transformed space\nsigma1_vec = singular_values[0] * u1\nsigma2_vec = singular_values[1] * u2\nax2.arrow(0, 0, sigma1_vec[0]*0.9, sigma1_vec[1]*0.9, \n          head_width=0.15, head_length=0.15, fc='orange', ec='orange', \n          linewidth=3, alpha=0.7, linestyle='--')\nax2.arrow(0, 0, sigma2_vec[0]*0.9, sigma2_vec[1]*0.9, \n          head_width=0.15, head_length=0.15, fc='cyan', ec='cyan', \n          linewidth=3, alpha=0.7, linestyle='--')\n\nax2.set_xlim(-3, 3)\nax2.set_ylim(-3, 3)\nax2.set_xlabel('$y_1$')\nax2.set_ylabel('$y_2$')\nax2.set_title('Transformed Space: $y = Ax$')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nTransformation matrix A:\n[[2.  0.5]\n [0.3 1. ]]\n\nSingular values: [2.14364206 0.86301721]\nSpectral norm (max singular value): 2.1436\nCondition number: 2.4839\n\n\n\n\n\n\n\n\n\nThe transformation turns a circle into an ellipse. The singular values tell us exactly how much the matrix stretches in its principal directions. The spectral norm is the maximum stretching factor, which in the above example is 2.1436 and the same as the ellipse’s semi-major axis length, with 0.863 being the semi-minor axis length. This is crucial for understanding how errors propagate through linear transformations. For example, in deep learning, if weight matrices have large spectral norms, small input perturbations can get amplified dramatically. What problems do you think this might cause?\n\n\nD.3.4 Example: Spectral Norms and Condition Numbers of Different Matrices\nLet’s explore how different matrices have different spectral properties and what that means for numerical stability.\n\n\nShow Code\n# code-fold: false\n# Create different types of matrices\nmatrices = {\n    'Well-conditioned': np.array([[1.0, 0.2], [0.2, 1.0]]),\n    'Stretched': np.array([[5.0, 0.0], [0.0, 1.0]]),\n    'Ill-conditioned': np.array([[10.0, 9.99], [9.99, 10.0]]),\n    'Nearly singular': np.array([[1.0, 1.0], [1.0, 1.001]])\n}\n\n# Analyze each matrix\nresults = {}\nfor name, A in matrices.items():\n    U, s, Vt = np.linalg.svd(A)\n    spectral_norm = s[0]\n    condition_number = s[0] / s[1] if s[1] &gt; 1e-10 else np.inf\n    det_A = np.linalg.det(A)\n    results[name] = {\n        'A': A,\n        'singular_values': s,\n        'spectral_norm': spectral_norm,\n        'condition_number': condition_number,\n        'determinant': det_A,\n        'U': U,\n        'Vt': Vt\n    }\n\n\n\n\nShow Code\n\n# Create visualization\nfig, axes = plt.subplots(2, 4, figsize=(18, 10))\n\nfor idx, (name, info) in enumerate(results.items()):\n    # Top row: transformed unit circle\n    ax_circle = axes[0, idx]\n    \n    # Draw unit circle\n    theta = np.linspace(0, 2*np.pi, 100)\n    circle = np.stack([np.cos(theta), np.sin(theta)])\n    \n    # Transform circle\n    ellipse = info['A'] @ circle\n    \n    ax_circle.plot(circle[0, :], circle[1, :], 'b--', linewidth=1.5, \n                   alpha=0.5, label='Unit circle')\n    ax_circle.plot(ellipse[0, :], ellipse[1, :], 'r-', linewidth=2.5, \n                   label='Transformed')\n    ax_circle.fill(ellipse[0, :], ellipse[1, :], 'red', alpha=0.2)\n    \n    # Draw singular value directions\n    s = info['singular_values']\n    U = info['U']\n    ax_circle.arrow(0, 0, s[0]*U[0,0]*0.9, s[0]*U[1,0]*0.9,\n                    head_width=0.3, head_length=0.2, fc='orange', ec='orange',\n                    linewidth=2, alpha=0.8)\n    ax_circle.arrow(0, 0, s[1]*U[0,1]*0.9, s[1]*U[1,1]*0.9,\n                    head_width=0.3, head_length=0.2, fc='cyan', ec='cyan',\n                    linewidth=2, alpha=0.8)\n    \n    ax_circle.set_aspect('equal')\n    ax_circle.grid(True, alpha=0.3)\n    ax_circle.set_title(name)\n    ax_circle.legend(fontsize=8)\n    ax_circle.set_xlim(-12, 12)\n    ax_circle.set_ylim(-12, 12)\n    \n    # Bottom row: Singular value bar chart\n    ax_sv = axes[1, idx]\n    ax_sv.bar(['$\\\\sigma_1$', '$\\\\sigma_2$'], s, color=['orange', 'cyan'], alpha=0.7)\n    ax_sv.set_ylabel('Singular Value')\n    ax_sv.set_title(f'$\\\\|A\\\\|_2$={info[\"spectral_norm\"]:.2f}, $\\\\kappa$={info[\"condition_number\"]:.1f}')\n    ax_sv.grid(True, alpha=0.3, axis='y')\n    ax_sv.set_ylim(0, max(s[0], 10))\n\nplt.tight_layout()\nplt.show()\n\n# Print detailed comparison\nprint(\"Matrix Analysis Summary\")\nprint(\"=\" * 80)\nprint(f\"{'Name':&lt;20} {'‖A‖₂':&lt;12} {'κ(A)':&lt;15} {'det(A)':&lt;12}\")\nprint(\"-\" * 80)\n\nfor name, info in results.items():\n    spec_norm = info['spectral_norm']\n    cond_num = info['condition_number']\n    det_val = info['determinant']\n    print(f\"{name:&lt;20} {spec_norm:&lt;12.4f} {cond_num:&lt;15.2f} {det_val:&lt;12.6f}\")\n\n\n\n\n\n\n\n\n\nMatrix Analysis Summary\n================================================================================\nName                 ‖A‖₂         κ(A)            det(A)      \n--------------------------------------------------------------------------------\nWell-conditioned     1.2000       1.50            0.960000    \nStretched            5.0000       5.00            5.000000    \nIll-conditioned      19.9900      1999.00         0.199900    \nNearly singular      2.0005       4002.00         0.001000    \n\n\nWe see that for well-conditioned matrices, they have a small condition number that barely amplifies or stretches the origina data. As we increase the condition number, small perturbations in the input can cause large changes in the output matrix. However, even if the spectral norm is small, the matrix can still have a large condition number, which can make the matrix difficult to invert or compute stable gradients with respect to.\n\n\nD.3.5 Example: Neural Network Jacobian and Local Geometry\nNeural networks are compositions of nonlinear transformations. At any point, the local behavior is captured by the Jacobian matrix. Let’s visualize how a simple neural network transforms space locally.\n\n# Define a simple 2-layer neural network: R^2 -&gt; R^2\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(2, 8, bias=True)\n        self.fc2 = nn.Linear(8, 2, bias=True)\n    \n    def forward(self, x):\n        x = torch.tanh(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Initialize network with specific seed\ntorch.manual_seed(42)\nnet = SimpleNet()\n\n# Function to compute Jacobian at a point\ndef compute_jacobian(model, x):\n    \"\"\"Compute Jacobian matrix of model output w.r.t. input at point x\"\"\"\n    x_tensor = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n    y = model(x_tensor.unsqueeze(0))\n    \n    jacobian = []\n    for i in range(y.shape[1]):\n        grad = torch.autograd.grad(y[0, i], x_tensor, \n                                   create_graph=True, retain_graph=True)[0]\n        jacobian.append(grad.detach().numpy())\n    \n    return np.array(jacobian)\n\n# Visualize transformation at different points in space\ntest_points = [\n    np.array([0.0, 0.0]),\n    np.array([1.0, 1.0]),\n    np.array([-1.0, 0.5]),\n]\n\n\n\nShow Code\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\nfor idx, point in enumerate(test_points):\n    ax = axes[idx]\n    \n    # Compute Jacobian at this point\n    J = compute_jacobian(net, point)\n    \n    # Compute singular values\n    U, s, Vt = np.linalg.svd(J)\n    spectral_norm = s[0]\n    \n    # Create a small grid around the point\n    delta = 0.2\n    n_grid = 10\n    x_local = np.linspace(-delta, delta, n_grid)\n    y_local = np.linspace(-delta, delta, n_grid)\n    X_local, Y_local = np.meshgrid(x_local, y_local)\n    \n    # Transform grid through network\n    grid_points = np.stack([X_local.ravel() + point[0], \n                           Y_local.ravel() + point[1]], axis=0)\n    \n    with torch.no_grad():\n        grid_tensor = torch.tensor(grid_points.T, dtype=torch.float32)\n        transformed = net(grid_tensor).numpy()\n    \n    X_trans = transformed[:, 0].reshape(X_local.shape)\n    Y_trans = transformed[:, 1].reshape(Y_local.shape)\n    \n    # Plot transformed grid\n    for i in range(n_grid):\n        ax.plot(X_trans[i, :], Y_trans[i, :], 'b-', alpha=0.5, linewidth=0.8)\n        ax.plot(X_trans[:, i], Y_trans[:, i], 'b-', alpha=0.5, linewidth=0.8)\n    \n    # Draw a small circle around the point and its transformation\n    theta = np.linspace(0, 2*np.pi, 50)\n    radius = 0.1\n    circle = radius * np.stack([np.cos(theta), np.sin(theta)])\n    circle_centered = circle + point[:, np.newaxis]\n    \n    with torch.no_grad():\n        circle_tensor = torch.tensor(circle_centered.T, dtype=torch.float32)\n        circle_transformed = net(circle_tensor).numpy()\n    \n    ax.plot(circle_transformed[:, 0], circle_transformed[:, 1], 'r-', \n            linewidth=2.5, label='Transformed circle')\n    \n    # Draw ellipse predicted by linear approximation (Jacobian)\n    # Transform centered circle with Jacobian, then translate\n    with torch.no_grad():\n        point_tensor = torch.tensor(point, dtype=torch.float32).unsqueeze(0)\n        center_transformed = net(point_tensor).numpy()[0]\n    \n    ellipse_linear = J @ circle + center_transformed[:, np.newaxis]\n    ax.plot(ellipse_linear[0, :], ellipse_linear[1, :], 'g--', \n            linewidth=2, label='Linear approximation', alpha=0.7)\n    \n    ax.set_aspect('equal')\n    ax.set_title(f'At point {point}\\n$\\\\|J\\\\|_2$ = {spectral_norm:.3f}')\n    ax.legend(fontsize=9)\n    ax.grid(True, alpha=0.3)\n    ax.set_xlabel('Output dim 1')\n    ax.set_ylabel('Output dim 2')\n\nplt.tight_layout()\nplt.show()\n\n# Print Jacobian analysis\nfor point in test_points:\n    J = compute_jacobian(net, point)\n    U, s, Vt = np.linalg.svd(J)\n    \n    print(f\"\\nAt point {point}:\")\n    print(\"Jacobian matrix:\")\n    print(J)\n    print(f\"Singular values: {s}\")\n    print(f\"Spectral norm: {s[0]:.4f}\")\n    print(f\"Condition number: {s[0]/s[1]:.4f}\")\n\n\n\n\n\n\n\n\n\n\nAt point [0. 0.]:\nJacobian matrix:\n[[-0.12561867 -0.3354176 ]\n [ 0.23783804  0.2557892 ]]\nSingular values: [0.4907707  0.09707827]\nSpectral norm: 0.4908\nCondition number: 5.0554\n\nAt point [1. 1.]:\nJacobian matrix:\n[[ 0.04538195 -0.19429947]\n [ 0.10090272  0.12735607]]\nSingular values: [0.23314807 0.10887937]\nSpectral norm: 0.2331\nCondition number: 2.1413\n\nAt point [-1.   0.5]:\nJacobian matrix:\n[[-0.20401104 -0.28298327]\n [ 0.20504908  0.24390957]]\nSingular values: [0.47215527 0.01750529]\nSpectral norm: 0.4722\nCondition number: 26.9721\n\n\nWe can see that the neural network’s Jacobian is changing across the input space. The Jacobian tells us how the neural network behaves locally—like a linear approximation at each point. The red curve shows the actual transformation of a small circle, while the green dashed line shows what the Jacobian predicts. When they match well, the network is locally linear; when they diverge, the nonlinearity is strong.\n\n\nD.3.6 Example: Spectral Normalization\nSpectral normalization is a technique to control the Lipschitz constant of neural networks by constraining weight matrices to have spectral norm \\(\\leq 1\\). Let’s see how this affects the transformation.\n\n# Function to apply spectral normalization to a weight matrix\ndef spectral_normalize(W, n_iterations=1):\n    \"\"\"\n    Normalize a weight matrix so its spectral norm is 1.\n    Uses power iteration to estimate the largest singular value.\n    \"\"\"\n    U, s, Vt = np.linalg.svd(W, full_matrices=False)\n    sigma_max = s[0]\n    return W / sigma_max, sigma_max\n\n# Create two versions of a weight matrix\nnp.random.seed(42)\nW_original = np.random.randn(2, 2) * 2  # Scaled up for effect\n\n# Apply spectral normalization\nW_normalized, original_spectral_norm = spectral_normalize(W_original)\n\n\n\nShow Code\nprint(\"Original weight matrix:\")\nprint(W_original)\nprint(f\"Spectral norm: {original_spectral_norm:.4f}\")\n\nprint(\"\\nSpectral normalized weight matrix:\")\nprint(W_normalized)\nprint(f\"Spectral norm: {np.linalg.svd(W_normalized, compute_uv=False)[0]:.4f}\")\n\n# Visualize the effect on unit circle\ntheta = np.linspace(0, 2*np.pi, 100)\nunit_circle = np.stack([np.cos(theta), np.sin(theta)])\n\ntransformed_original = W_original @ unit_circle\ntransformed_normalized = W_normalized @ unit_circle\n\n# Create visualization\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Left: Original transformation\nax = axes[0]\nax.plot(unit_circle[0, :], unit_circle[1, :], 'b-', linewidth=2, \n        label='Unit circle', alpha=0.5)\nax.plot(transformed_original[0, :], transformed_original[1, :], 'r-', \n        linewidth=2.5, label='Transformed')\nax.fill(transformed_original[0, :], transformed_original[1, :], 'red', alpha=0.2)\nax.set_aspect('equal')\nax.set_title(f'Original Weights\\nSpectral Norm = {original_spectral_norm:.3f}')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_xlim(-5, 5)\nax.set_ylim(-5, 5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n\n# Middle: Normalized transformation\nax = axes[1]\nax.plot(unit_circle[0, :], unit_circle[1, :], 'b-', linewidth=2, \n        label='Unit circle', alpha=0.5)\nax.plot(transformed_normalized[0, :], transformed_normalized[1, :], 'g-', \n        linewidth=2.5, label='Transformed')\nax.fill(transformed_normalized[0, :], transformed_normalized[1, :], 'green', alpha=0.2)\n\n# Draw unit circle as reference\ncircle_ref = plt.Circle((0, 0), 1, fill=False, color='black', \n                        linestyle='--', linewidth=1.5, label='Unit circle reference')\nax.add_patch(circle_ref)\n\nax.set_aspect('equal')\nax.set_title('Spectral Normalized\\nSpectral Norm = 1.000')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n\n# Right: Comparison\nax = axes[2]\nax.plot(unit_circle[0, :], unit_circle[1, :], 'b-', linewidth=2, \n        label='Input', alpha=0.7)\nax.plot(transformed_original[0, :], transformed_original[1, :], 'r-', \n        linewidth=2, label='Original', alpha=0.7)\nax.plot(transformed_normalized[0, :], transformed_normalized[1, :], 'g-', \n        linewidth=2, label='Normalized', alpha=0.7)\nax.set_aspect('equal')\nax.set_title('Comparison')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_xlim(-5, 5)\nax.set_ylim(-5, 5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n\nplt.tight_layout()\nplt.show()\n\n\nOriginal weight matrix:\n[[ 0.99342831 -0.2765286 ]\n [ 1.29537708  3.04605971]]\nSpectral norm: 3.3131\n\nSpectral normalized weight matrix:\n[[ 0.29985151 -0.08346603]\n [ 0.39099024  0.91940767]]\nSpectral norm: 1.0000\n\n\n\n\n\n\n\n\n\nSpectral normalization ensures that no input vector gets amplified by more than a factor of 1. This is useful for training stability in deep networks since repeated multiplications by weight matrices with large spectral norms can cause gradients to explode. It can also have benefits in cases where we might wish to bound the amount that a given input perturbation can affect a network (e.g., for adversarial robustness).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Reviewing Mathematical and Computational Foundations for Machine Learning</span>"
    ]
  },
  {
    "objectID": "appendices/review_of_math_and_computing_foundations.html#differentiation-curvature",
    "href": "appendices/review_of_math_and_computing_foundations.html#differentiation-curvature",
    "title": "Appendix D — Reviewing Mathematical and Computational Foundations for Machine Learning",
    "section": "D.4 Differentiation & Curvature",
    "text": "D.4 Differentiation & Curvature\n\nD.4.1 Concepts:\n\nGradients point in the direction of steepest ascent and guide first-order optimization\nHessians capture curvature (second-order information) about the function’s shape\nJacobian-vector products (JVPs) and Hessian-vector products (HVPs) allow efficient computation without forming full matrices\nCurvature determines optimization difficulty: flat regions are slow, steep valleys cause oscillation\nUnderstanding higher-order structure is useful for advanced optimization and model analysis\n\n\n\nD.4.2 Key Equations:\nGradient (First derivative): \\[\\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\\]\nHessian (Second derivative matrix): \\[H(f)(x) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\end{bmatrix}\\]\nIntuition: The gradient tells you which way is “downhill,” but the Hessian tells you how quickly the slope is changing. Various optimizers like Newton’s method or L-BFGS can use curvature to take smarter steps than simple gradient descent.\n\n\nD.4.3 Example: Gradient and Hessian Visualization\nLet’s visualize how the gradient (slope) and Hessian (curvature) change at different points along a function. We’ll use \\(f(x) = x^3 - 3x\\) and detect different gradient and curvature regions.\n\n\nShow Code\n# Define function and its derivatives\ndef f(x):\n    \"\"\"f(x) = x^3 - 3x\"\"\"\n    return x**3 - 3*x\n\ndef f_prime(x):\n    \"\"\"First derivative: f'(x) = 3x^2 - 3\"\"\"\n    return 3*x**2 - 3\n\ndef f_double_prime(x):\n    \"\"\"Second derivative: f''(x) = 6x\"\"\"\n    return 6*x\n\n# Points to analyze\ntest_points = [-1.5, -1.0, 0.0, 1.0, 1.5]\n\n# Create grid for plotting\nx_grid = np.linspace(-2, 2, 500)\ny_grid = f(x_grid)\n\n# Create visualization\nfig, axes = plt.subplots(2, len(test_points), figsize=(18, 8))\n\nfor idx, x0 in enumerate(test_points):\n    # Compute derivatives at x0\n    y0 = f(x0)\n    gradient = f_prime(x0)\n    hessian = f_double_prime(x0)\n    \n    # Top row: Function with tangent line\n    ax_top = axes[0, idx]\n    ax_top.plot(x_grid, y_grid, 'b-', linewidth=2, label='$f(x)$')\n    ax_top.plot(x0, y0, 'ro', markersize=10, label=f'$x_0={x0}$')\n    \n    # Draw tangent line\n    tangent_x = np.linspace(x0 - 0.5, x0 + 0.5, 100)\n    tangent_y = y0 + gradient * (tangent_x - x0)\n    ax_top.plot(tangent_x, tangent_y, 'r--', linewidth=2, label='Tangent')\n    \n    # Draw gradient arrow\n    arrow_scale = 0.3\n    ax_top.arrow(x0, y0, arrow_scale, gradient * arrow_scale,\n                 head_width=0.3, head_length=0.15, fc='orange', ec='orange',\n                 linewidth=2, alpha=0.7)\n    \n    ax_top.set_xlabel('$x$')\n    ax_top.set_ylabel('$f(x)$')\n    ax_top.set_title(f'$x_0 = {x0}$\\n$f\\'(x_0) = {gradient:.2f}$')\n    ax_top.legend(fontsize=8)\n    ax_top.grid(True, alpha=0.3)\n    ax_top.set_xlim(-2, 2)\n    ax_top.set_ylim(-3, 3)\n    \n    # Bottom row: Second derivative (curvature)\n    ax_bottom = axes[1, idx]\n    ax_bottom.plot(x_grid, f_double_prime(x_grid), 'g-', linewidth=2, label='$f\\'\\'(x)$')\n    ax_bottom.plot(x0, hessian, 'ro', markersize=10, label=f'$f\\'\\'(x_0)={hessian:.2f}$')\n    ax_bottom.axhline(0, color='black', linestyle=':', linewidth=1, alpha=0.5)\n    \n    # Shade regions\n    if hessian &gt; 0:\n        ax_bottom.axvspan(x0 - 0.3, x0 + 0.3, alpha=0.2, color='blue', \n                          label='Convex (positive curvature)')\n    elif hessian &lt; 0:\n        ax_bottom.axvspan(x0 - 0.3, x0 + 0.3, alpha=0.2, color='red',\n                          label='Concave (negative curvature)')\n    else:\n        ax_bottom.axvspan(x0 - 0.3, x0 + 0.3, alpha=0.2, color='gray',\n                          label='Inflection point')\n    \n    ax_bottom.set_xlabel('$x$')\n    ax_bottom.set_ylabel('$f\\'\\'(x)$')\n    ax_bottom.set_title(f'Curvature at $x_0 = {x0}$')\n    ax_bottom.legend(fontsize=7)\n    ax_bottom.grid(True, alpha=0.3)\n    ax_bottom.set_xlim(-2, 2)\n    ax_bottom.set_ylim(-10, 10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe gradient tells you the instantaneous rate of change (the slope of the tangent line), while the Hessian tells you how fast that slope is changing (the curvature). When the Hessian is positive, the function curves upward like a bowl—this is where gradient descent naturally converges. When it’s negative, the function curves downward—a local maximum. At the inflection point (x=0), the curvature is zero.\n\n\nD.4.4 Example: Reducing Computational Cost using JVPs and HVPs\nJacobian-vector products (JVPs) and Hessian-vector products (HVPs) let us compute directional derivatives efficiently without forming the full Jacobian or Hessian matrix. This is crucial for large-scale problems. We will first see how this works on a simple 2D function, and then later move to a more realistic case of a Neural Network.\n\n# A simple 2D function f(x, y)\ndef f(xy):\n    x, y = xy[..., 0], xy[..., 1]\n    return (x**2 + y**2) + 0.5 * x * y**3\n\nxy = torch.tensor([1.0, 2.0], requires_grad=True)\nf_val = f(xy)\n\n# Gradient (Jacobian of f wrt xy)\n# create_graph=True so we can differentiate the gradient later\ngrad = torch.autograd.grad(f_val, xy, create_graph=True)[0]\nprint(\"Gradient:\", grad)\n\n# Choose a direction vector v (match dtype/device of xy)\nv = torch.tensor([1.0, -0.5], dtype=xy.dtype, device=xy.device)\n\n# Directional derivative (scalar): ∇f(x) · v\njvp_scalar = (grad * v).sum()\nprint(\"Directional derivative (∇f · v):\", jvp_scalar.item())\n\n# Hessian-vector product: H·v = ∇_x (∇f · v)\nhvp = torch.autograd.grad(jvp_scalar, xy)[0]\nprint(\"Hessian-Vector Product (H·v):\", hvp)\n\nGradient: tensor([ 6., 10.], grad_fn=&lt;AddBackward0&gt;)\nDirectional derivative (∇f · v): 1.0\nHessian-Vector Product (H·v): tensor([-1.,  2.])\n\n\n\n\nShow Code\n# Sweep along direction v\nalphas = torch.linspace(-1, 1, 100)\npoints = xy + alphas[:, None] * v\nf_vals = torch.stack([f(p) for p in points]).detach()\n\n# First and second order Taylor approximations\nf0 = f_val.detach()\ngrad_v = (grad @ v).detach()\ncurv_v = (v @ hvp).detach()\n\nf_linear = f0 + alphas * grad_v\nf_quadratic = f0 + alphas * grad_v + 0.5 * (alphas**2) * curv_v\n\nplt.figure(figsize=(6,4))\nplt.plot(alphas, f_vals, label=\"True f(x + αv)\")\nplt.plot(alphas, f_linear, '--', label=\"1st order (JVP)\")\nplt.plot(alphas, f_quadratic, ':', label=\"2nd order (HVP)\")\nplt.xlabel(\"α (step along v)\")\nplt.ylabel(\"f(x + αv)\")\nplt.legend()\nplt.title(\"Directional derivatives and curvature via JVP/HVP\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNow let’s show how this might be useful in a Neural Network context:\n\n# Small network: 2 inputs → 4 hidden → 1 output\nnet = nn.Sequential(\n    nn.Linear(2, 4),\n    nn.Tanh(),\n    nn.Linear(4, 1)\n)\n\n# Make output scalar to ensure gradients are well-defined for higher-order derivatives\nx = torch.randn(1, 2)\ny = net(x).sum()  # sum to scalar\n\n# Collect parameters\nparams = [p for p in net.parameters() if p.requires_grad]\n\n# First-order gradients (create_graph=True so we can compute higher-order derivatives)\ngrads = torch.autograd.grad(y, params, create_graph=True)\n\n# Random direction in parameter space\nv = [torch.randn_like(p) for p in params]\n\n# Compute JVP contributions per-parameter (these are tensors shaped like each parameter)\njvp_per_param = [g * vi for g, vi in zip(grads, v)]\n\n# Scalar directional derivative (sum over all parameters) = (∂y/∂θ) · v\njvp_scalar = sum((jp).sum() for jp in jvp_per_param)\n\n# Hessian-vector product H·v computed as gradient of the scalar jvp_scalar w.r.t. params\n# allow_unused=True may return None for parameters that did not contribute to jvp_scalar\nhvp = torch.autograd.grad(jvp_scalar, params, retain_graph=True, allow_unused=True)\n\n\n\nShow Code\n# Report norms (handle possible None entries safely)\nfor i, (p, j_contrib, h) in enumerate(zip(params, jvp_per_param, hvp)):\n    # j_contrib is the per-parameter contribution to the JVP; take its norm if available\n    try:\n        j_norm = j_contrib.norm().item() if j_contrib is not None else float('nan')\n    except Exception:\n        j_norm = float('nan')\n    # h may be None if the corresponding parameter did not affect jvp_scalar\n    try:\n        h_norm = h.norm().item() if (h is not None and isinstance(h, torch.Tensor)) else float('nan')\n    except Exception:\n        h_norm = float('nan')\n    print(f\"Layer {i} | Param shape {tuple(p.shape)} | JVP contrib norm={j_norm:.3f} | HVP norm={h_norm:.3f}\")\n\n\nLayer 0 | Param shape (4, 2) | JVP contrib norm=0.246 | HVP norm=2.030\nLayer 1 | Param shape (4,) | JVP contrib norm=0.176 | HVP norm=1.066\nLayer 2 | Param shape (1, 4) | JVP contrib norm=1.220 | HVP norm=0.983\nLayer 3 | Param shape (1,) | JVP contrib norm=0.027 | HVP norm=nan\n\n\n\n\nShow Code\n# Visualize how the output changes along an input direction\nmodel = net\nx = torch.randn(1, 2, requires_grad=True)\ny = model(x)\n\nv = torch.tensor([[1.0, 0.0]])\ny_val, jvp = torch.autograd.functional.jvp(model, x, v)\n\nalphas = torch.linspace(-1, 1, 100).unsqueeze(1)\noutputs = [model(x + α*v).item() for α in alphas]\nplt.plot(alphas, outputs, label=\"True model output\")\nplt.plot(alphas, y_val.item() + alphas*jvp.item(), '--', label=\"Linearized (JVP)\")\nplt.xlabel(\"Step along v\")\nplt.ylabel(\"Model output\")\nplt.title(\"Local linear approximation using JVP\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe full Hessian of a neural network with \\(p\\) parameters is a \\(p \\times p\\) matrix, and for modern networks with millions of parameters, this is completely infeasible. JVPs and HVPs give us access to gradient and curvature information, respectively. along specific directions at a fraction of the cost.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Reviewing Mathematical and Computational Foundations for Machine Learning</span>"
    ]
  },
  {
    "objectID": "appendices/review_of_math_and_computing_foundations.html#marginalization-of-probability-distributions",
    "href": "appendices/review_of_math_and_computing_foundations.html#marginalization-of-probability-distributions",
    "title": "Appendix D — Reviewing Mathematical and Computational Foundations for Machine Learning",
    "section": "D.5 Marginalization of Probability Distributions",
    "text": "D.5 Marginalization of Probability Distributions\n\nD.5.1 Concepts:\n\nJoint distributions \\(p(x, z)\\) describe the probability of multiple variables together\nMarginal distributions \\(p(x)\\) are obtained by integrating (or summing) out other variables\nMarginalization is fundamental to probabilistic inference and latent variable models\nFor continuous variables: \\(p(x) = \\int p(x, z) dz\\)\nFor discrete variables: \\(p(x) = \\sum_z p(x, z)\\)\nThis operation removes nuisance variables and reveals the distribution of the variable we care about\n\n\n\nD.5.2 Key Equations:\nMarginalization (continuous): \\[p(x) = \\int p(x, z) \\, dz\\]\nLaw of Total Probability: \\[p(x) = \\int p(x | z) p(z) \\, dz = \\mathbb{E}_Z[p(x|Z)]\\]\nConditional Distribution: \\[p(x | z) = \\frac{p(x, z)}{p(z)}\\]\nIntuition: Marginalization asks “what’s the probability of \\(x\\), averaging over all possible values of \\(z\\)?” It’s like projecting a 2D distribution onto a 1D axis: you’re summing up all the probability mass along one direction.\n\n\nD.5.3 Example: 2D Gaussian Marginalization\nLet’s visualize a 2D Gaussian distribution and see what happens when we marginalize out one variable. This is the foundation for understanding latent variable models.\n\n\nShow Code\n# Define a 2D Gaussian with correlation\nmean = np.array([1.0, 2.0])\ncov = np.array([[1.0, 0.7],\n                [0.7, 1.5]])\n\n# Create a grid\nx_range = np.linspace(-2, 4, 100)\nz_range = np.linspace(-2, 6, 100)\nX, Z = np.meshgrid(x_range, z_range)\n\n# Evaluate joint density p(x, z)\npos = np.dstack((X, Z))\njoint_dist = multivariate_normal(mean, cov)\njoint_pdf = joint_dist.pdf(pos)\n\n# Compute marginal p(x) by integrating over z\n# For a Gaussian, this is analytically known: p(x) ~ N(μ_x, Σ_xx)\nmarginal_x_mean = mean[0]\nmarginal_x_var = cov[0, 0]\nmarginal_x = stats.norm(marginal_x_mean, np.sqrt(marginal_x_var))\nmarginal_x_pdf = marginal_x.pdf(x_range)\n\n# Compute marginal p(z) by integrating over x\nmarginal_z_mean = mean[1]\nmarginal_z_var = cov[1, 1]\nmarginal_z = stats.norm(marginal_z_mean, np.sqrt(marginal_z_var))\nmarginal_z_pdf = marginal_z.pdf(z_range)\n\n# Numerical integration to verify\nmarginal_x_numerical = np.trapezoid(joint_pdf, z_range, axis=0)\nmarginal_z_numerical = np.trapezoid(joint_pdf, x_range, axis=1)\n\n# Normalize numerical marginals\nmarginal_x_numerical = marginal_x_numerical / np.trapezoid(marginal_x_numerical, x_range)\nmarginal_z_numerical = marginal_z_numerical / np.trapezoid(marginal_z_numerical, z_range)\n\n# Create visualization\nfig = plt.figure(figsize=(16, 12))\n\n# Top-left: 2D joint distribution\nax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2, rowspan=2)\ncontour = ax1.contourf(X, Z, joint_pdf, levels=20, cmap='viridis')\nax1.contour(X, Z, joint_pdf, levels=10, colors='white', alpha=0.3, linewidths=0.5)\n#plt.colorbar(contour, ax=ax1, label='$p(x, z)$')\nax1.set_xlabel('$x$', fontsize=12)\nax1.set_ylabel('$z$', fontsize=12)\nax1.set_title('Joint Distribution $p(x, z)$', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.2)\n\n# Top-right: Marginal p(z)\nax2 = plt.subplot2grid((3, 3), (0, 2), rowspan=2)\nax2.plot(marginal_z_pdf, z_range, 'r-', linewidth=3, label='Analytical $p(z)$')\nax2.plot(marginal_z_numerical, z_range, 'b--', linewidth=2, \n        label='Numerical', alpha=0.7)\nax2.fill_betweenx(z_range, 0, marginal_z_pdf, alpha=0.3, color='red')\nax2.set_ylabel('$z$', fontsize=12)\nax2.set_xlabel('$p(z)$', fontsize=12)\nax2.set_title('Marginal $p(z)$', fontsize=12, fontweight='bold')\nax2.legend(fontsize=9)\nax2.grid(True, alpha=0.3)\nax2.set_ylim(z_range[0], z_range[-1])\n\n# Bottom-left: Marginal p(x)\nax3 = plt.subplot2grid((3, 3), (2, 0), colspan=2)\nax3.plot(x_range, marginal_x_pdf, 'g-', linewidth=3, label='Analytical $p(x)$')\nax3.plot(x_range, marginal_x_numerical, 'b--', linewidth=2,\n        label='Numerical', alpha=0.7)\nax3.fill_between(x_range, 0, marginal_x_pdf, alpha=0.3, color='green')\nax3.set_xlabel('$x$', fontsize=12)\nax3.set_ylabel('$p(x)$', fontsize=12)\nax3.set_title('Marginal $p(x)$', fontsize=12, fontweight='bold')\nax3.legend(fontsize=9)\nax3.grid(True, alpha=0.3)\nax3.set_xlim(x_range[0], x_range[-1])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nMarginalization “collapses” the joint distribution onto one axis by integrating out the other variable. Think of it as viewing the shadow of a 3D object—you lose information about one dimension but get a simpler view.\nThe marginal distributions don’t tell you anything about the correlation between \\(x\\) and \\(z\\). If you only observe \\(p(x)\\) and \\(p(z)\\) separately, could you reconstruct \\(p(x, z)\\)?\n\n\nD.5.4 Example: Conditional Sampling and the Joint Distribution\nThe joint distribution can be decomposed as \\(p(x, z) = p(x|z) p(z)\\). Let’s visualize how conditionals \\(p(x|z)\\) for different values of \\(z\\) shape the joint distribution.\n\n\nShow Code\n# Use same joint distribution as before\nmean = np.array([1.0, 2.0])\ncov = np.array([[1.0, 0.7],\n                [0.7, 1.5]])\n\njoint_dist = multivariate_normal(mean, cov)\n\n# Select several z values to condition on\nz_values = np.linspace(-1, 5, 7)\n\n# For each z, compute the conditional p(x | z)\n# For a 2D Gaussian: p(x|z) ~ N(μ_x + (Σ_xz/Σ_zz)(z - μ_z), Σ_xx - Σ_xz²/Σ_zz)\ndef conditional_params(z_val):\n    \"\"\"Compute parameters of p(x|z) for a 2D Gaussian\"\"\"\n    mu_x, mu_z = mean[0], mean[1]\n    sigma_xx = cov[0, 0]\n    sigma_zz = cov[1, 1]\n    sigma_xz = cov[0, 1]\n    \n    # Conditional mean and variance\n    cond_mean = mu_x + (sigma_xz / sigma_zz) * (z_val - mu_z)\n    cond_var = sigma_xx - (sigma_xz ** 2) / sigma_zz\n    \n    return cond_mean, cond_var\n\n# Create visualization\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Left plot: Joint distribution with conditional slices\nax1 = axes[0]\nX_grid = np.linspace(-2, 4, 100)\nZ_grid = np.linspace(-2, 6, 100)\nX_mesh, Z_mesh = np.meshgrid(X_grid, Z_grid)\npos_mesh = np.dstack((X_mesh, Z_mesh))\njoint_pdf_mesh = joint_dist.pdf(pos_mesh)\n\ncontour = ax1.contourf(X_mesh, Z_mesh, joint_pdf_mesh, levels=15, cmap='viridis', alpha=0.6)\nax1.contour(X_mesh, Z_mesh, joint_pdf_mesh, levels=10, colors='white', alpha=0.3, linewidths=0.5)\n\n# Draw conditional distributions as curves\ncolors = plt.cm.rainbow(np.linspace(0, 1, len(z_values)))\n\nfor z_val, color in zip(z_values, colors):\n    # Compute conditional p(x | z)\n    cond_mean, cond_var = conditional_params(z_val)\n    cond_dist = stats.norm(cond_mean, np.sqrt(cond_var))\n    \n    # Scale pdf for visualization\n    scale_factor = 0.5\n    x_cond = X_grid\n    pdf_cond = cond_dist.pdf(x_cond) * scale_factor\n    \n    # Plot as curve at height z\n    ax1.plot(x_cond, z_val + pdf_cond, color=color, linewidth=2.5, label=f'$z={z_val:.1f}$')\n    ax1.axhline(z_val, color=color, linestyle=':', alpha=0.3, linewidth=1)\n\nax1.set_xlabel('$x$', fontsize=12)\nax1.set_ylabel('$z$', fontsize=12)\nax1.set_title('Joint $p(x,z)$ with Conditional Slices $p(x|z)$', fontsize=13, fontweight='bold')\nax1.legend(fontsize=8, loc='upper right')\nax1.grid(True, alpha=0.2)\n\n# Right plot: Individual conditionals\nax2 = axes[1]\n\nfor z_val, color in zip(z_values, colors):\n    cond_mean, cond_var = conditional_params(z_val)\n    cond_dist = stats.norm(cond_mean, np.sqrt(cond_var))\n    \n    x_plot = np.linspace(-2, 4, 200)\n    pdf_plot = cond_dist.pdf(x_plot)\n    \n    ax2.plot(x_plot, pdf_plot, color=color, linewidth=2.5, label=f'$p(x|z={z_val:.1f})$')\n\nax2.set_xlabel('$x$', fontsize=12)\nax2.set_ylabel('$p(x|z)$', fontsize=12)\nax2.set_title('Conditional Distributions $p(x|z)$ for Different $z$', fontsize=13, fontweight='bold')\nax2.legend(fontsize=9)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Sample from joint and visualize\nnp.random.seed(42)\nn_samples = 500\nsamples = joint_dist.rvs(n_samples)\n\n# Create scatter plot with conditional structure\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Plot samples colored by z-value\nscatter = ax.scatter(samples[:, 0], samples[:, 1], c=samples[:, 1], \n                    cmap='rainbow', alpha=0.6, s=30, edgecolors='black', linewidths=0.5)\nplt.colorbar(scatter, ax=ax, label='$z$ value')\n\n# Overlay conditional means\nz_plot = np.linspace(-2, 6, 50)\nconditional_means = [conditional_params(z)[0] for z in z_plot]\nax.plot(conditional_means, z_plot, 'r--', linewidth=3, label='Conditional mean $E[x|z]$')\n\nax.set_xlabel('$x$', fontsize=12)\nax.set_ylabel('$z$', fontsize=12)\nax.set_title('Samples from $p(x,z)$ showing Conditional Structure', fontsize=13, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe conditional distributions \\(p(x|z)\\) show how knowing \\(z\\) affects our beliefs about \\(x\\). When variables are correlated, observing one variable shifts the expected value of the other. The “ridge” of the joint distribution follows the conditional mean. In VAEs and other latent variable models, we often work with \\(p(x|z)\\) where the decoder generates \\(x\\) given a latent code \\(z\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Reviewing Mathematical and Computational Foundations for Machine Learning</span>"
    ]
  }
]